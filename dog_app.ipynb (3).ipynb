{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5749 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob('lfw/lfw/*/'))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:9748: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-d76d09ca5fb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhuman_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# convert BGR image to grayscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# find faces in image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:9748: error: (-215) scn == 3 || scn == 4 in function cv::cvtColor\n"
     ]
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "import os\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "cwd = os.getcwd()\n",
    "img = cv2.imread(\"{}/{}\".format(cwd, human_files[3]))\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 99 images in the human_files.\n",
      "There are 11 images in the dog_files.\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "human_face_count = 0\n",
    "for image_y in human_files_short:\n",
    "    if(face_detector(image_y)) == True:\n",
    "        human_face_count += 1\n",
    "        \n",
    "dog_face_count = 0\n",
    "for image_x in dog_files_short:\n",
    "    if(face_detector(image_x)) == True:\n",
    "        dog_face_count += 1\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "\n",
    "\n",
    "print('There are %d images in the human_files.' % human_face_count)\n",
    "print('There are %d images in the dog_files.' % dog_face_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "## dont now have to make this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "humen_rase_count = 0\n",
    "for i in human_files_short:\n",
    "    if(dog_detector(i)) == True:\n",
    "        humen_rase_count += 1\n",
    "\n",
    "dog_rase_count = 0\n",
    "for im in dog_files_short:\n",
    "    if(dog_detector(im)) == True:\n",
    "        dog_rase_count += 1\n",
    "        \n",
    "print(humen_rase_count)\n",
    "print(dog_rase_count)\n",
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6680/6680 [01:51<00:00, 60.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 835/835 [00:13<00:00, 63.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 836/836 [00:13<00:00, 60.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 223, 223, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 111, 111, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 110, 110, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 55, 55, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 54, 54, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 27, 27, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               8645      \n",
      "=================================================================\n",
      "Total params: 19,189\n",
      "Trainable params: 19,189\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, activation='relu', input_shape=(224, 224, 3))) # get the images in th first layer\n",
    "model.add(MaxPooling2D(pool_size=2))   # make them smaller\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))  #\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "4040/6680 [=================>............] - ETA: 13:12 - loss: 4.9224 - acc: 0.0000e+ - ETA: 9:18 - loss: 4.9098 - acc: 0.0000e+00 - ETA: 7:56 - loss: 4.9019 - acc: 0.0000e+0 - ETA: 7:17 - loss: 4.8987 - acc: 0.0000e+0 - ETA: 6:53 - loss: 4.8973 - acc: 0.0000e+0 - ETA: 6:36 - loss: 4.8960 - acc: 0.0000e+0 - ETA: 6:24 - loss: 4.8951 - acc: 0.0000e+0 - ETA: 6:15 - loss: 4.8943 - acc: 0.0000e+0 - ETA: 6:08 - loss: 4.8928 - acc: 0.0000e+0 - ETA: 6:02 - loss: 4.8912 - acc: 0.0000e+0 - ETA: 5:57 - loss: 4.8896 - acc: 0.0045    - ETA: 5:53 - loss: 4.8887 - acc: 0.012 - ETA: 5:49 - loss: 4.8885 - acc: 0.011 - ETA: 5:45 - loss: 4.8899 - acc: 0.010 - ETA: 5:42 - loss: 4.8893 - acc: 0.010 - ETA: 5:39 - loss: 4.8923 - acc: 0.009 - ETA: 5:37 - loss: 4.8930 - acc: 0.008 - ETA: 5:34 - loss: 4.8923 - acc: 0.008 - ETA: 5:32 - loss: 4.8917 - acc: 0.007 - ETA: 5:30 - loss: 4.8915 - acc: 0.007 - ETA: 5:28 - loss: 4.8924 - acc: 0.009 - ETA: 5:26 - loss: 4.8917 - acc: 0.009 - ETA: 5:24 - loss: 4.8901 - acc: 0.010 - ETA: 5:23 - loss: 4.8889 - acc: 0.010 - ETA: 5:21 - loss: 4.8879 - acc: 0.010 - ETA: 5:19 - loss: 4.8860 - acc: 0.011 - ETA: 5:17 - loss: 4.8856 - acc: 0.013 - ETA: 5:16 - loss: 4.8877 - acc: 0.012 - ETA: 5:15 - loss: 4.8880 - acc: 0.012 - ETA: 5:13 - loss: 4.8881 - acc: 0.011 - ETA: 5:11 - loss: 4.8879 - acc: 0.011 - ETA: 5:10 - loss: 4.8882 - acc: 0.012 - ETA: 5:09 - loss: 4.8890 - acc: 0.012 - ETA: 5:07 - loss: 4.8892 - acc: 0.013 - ETA: 5:06 - loss: 4.8887 - acc: 0.012 - ETA: 5:05 - loss: 4.8878 - acc: 0.012 - ETA: 5:04 - loss: 4.8869 - acc: 0.012 - ETA: 5:03 - loss: 4.8876 - acc: 0.011 - ETA: 5:01 - loss: 4.8885 - acc: 0.011 - ETA: 5:00 - loss: 4.8891 - acc: 0.011 - ETA: 4:59 - loss: 4.8895 - acc: 0.011 - ETA: 4:57 - loss: 4.8890 - acc: 0.010 - ETA: 4:56 - loss: 4.8885 - acc: 0.010 - ETA: 4:55 - loss: 4.8894 - acc: 0.010 - ETA: 4:54 - loss: 4.8898 - acc: 0.010 - ETA: 4:53 - loss: 4.8902 - acc: 0.009 - ETA: 4:51 - loss: 4.8896 - acc: 0.010 - ETA: 4:50 - loss: 4.8897 - acc: 0.010 - ETA: 4:49 - loss: 4.8896 - acc: 0.011 - ETA: 4:48 - loss: 4.8897 - acc: 0.011 - ETA: 4:47 - loss: 4.8900 - acc: 0.010 - ETA: 4:46 - loss: 4.8902 - acc: 0.010 - ETA: 4:45 - loss: 4.8900 - acc: 0.010 - ETA: 4:43 - loss: 4.8902 - acc: 0.010 - ETA: 4:42 - loss: 4.8903 - acc: 0.010 - ETA: 4:41 - loss: 4.8900 - acc: 0.009 - ETA: 4:40 - loss: 4.8898 - acc: 0.009 - ETA: 4:39 - loss: 4.8893 - acc: 0.009 - ETA: 4:38 - loss: 4.8892 - acc: 0.009 - ETA: 4:36 - loss: 4.8888 - acc: 0.009 - ETA: 4:35 - loss: 4.8889 - acc: 0.009 - ETA: 4:34 - loss: 4.8887 - acc: 0.009 - ETA: 4:33 - loss: 4.8884 - acc: 0.009 - ETA: 4:32 - loss: 4.8885 - acc: 0.009 - ETA: 4:31 - loss: 4.8880 - acc: 0.010 - ETA: 4:30 - loss: 4.8877 - acc: 0.009 - ETA: 4:29 - loss: 4.8875 - acc: 0.009 - ETA: 4:28 - loss: 4.8881 - acc: 0.009 - ETA: 4:27 - loss: 4.8875 - acc: 0.009 - ETA: 4:26 - loss: 4.8878 - acc: 0.009 - ETA: 4:25 - loss: 4.8881 - acc: 0.009 - ETA: 4:24 - loss: 4.8887 - acc: 0.009 - ETA: 4:23 - loss: 4.8887 - acc: 0.008 - ETA: 4:22 - loss: 4.8886 - acc: 0.008 - ETA: 4:21 - loss: 4.8885 - acc: 0.009 - ETA: 4:20 - loss: 4.8882 - acc: 0.009 - ETA: 4:18 - loss: 4.8883 - acc: 0.009 - ETA: 4:17 - loss: 4.8884 - acc: 0.009 - ETA: 4:16 - loss: 4.8884 - acc: 0.008 - ETA: 4:15 - loss: 4.8890 - acc: 0.008 - ETA: 4:14 - loss: 4.8890 - acc: 0.009 - ETA: 4:13 - loss: 4.8893 - acc: 0.009 - ETA: 4:12 - loss: 4.8893 - acc: 0.009 - ETA: 4:11 - loss: 4.8895 - acc: 0.008 - ETA: 4:10 - loss: 4.8898 - acc: 0.008 - ETA: 4:09 - loss: 4.8897 - acc: 0.008 - ETA: 4:08 - loss: 4.8895 - acc: 0.008 - ETA: 4:07 - loss: 4.8897 - acc: 0.008 - ETA: 4:06 - loss: 4.8897 - acc: 0.009 - ETA: 4:05 - loss: 4.8896 - acc: 0.008 - ETA: 4:04 - loss: 4.8893 - acc: 0.008 - ETA: 4:03 - loss: 4.8893 - acc: 0.008 - ETA: 4:02 - loss: 4.8894 - acc: 0.008 - ETA: 4:01 - loss: 4.8893 - acc: 0.009 - ETA: 4:00 - loss: 4.8892 - acc: 0.009 - ETA: 3:59 - loss: 4.8889 - acc: 0.009 - ETA: 3:58 - loss: 4.8887 - acc: 0.009 - ETA: 3:57 - loss: 4.8888 - acc: 0.009 - ETA: 3:56 - loss: 4.8888 - acc: 0.009 - ETA: 3:55 - loss: 4.8888 - acc: 0.010 - ETA: 3:54 - loss: 4.8889 - acc: 0.009 - ETA: 3:53 - loss: 4.8891 - acc: 0.010 - ETA: 3:52 - loss: 4.8888 - acc: 0.010 - ETA: 3:51 - loss: 4.8888 - acc: 0.010 - ETA: 3:50 - loss: 4.8890 - acc: 0.010 - ETA: 3:49 - loss: 4.8890 - acc: 0.010 - ETA: 3:48 - loss: 4.8887 - acc: 0.010 - ETA: 3:47 - loss: 4.8886 - acc: 0.010 - ETA: 3:46 - loss: 4.8887 - acc: 0.010 - ETA: 3:45 - loss: 4.8887 - acc: 0.010 - ETA: 3:44 - loss: 4.8888 - acc: 0.010 - ETA: 3:43 - loss: 4.8885 - acc: 0.010 - ETA: 3:42 - loss: 4.8885 - acc: 0.010 - ETA: 3:41 - loss: 4.8884 - acc: 0.010 - ETA: 3:40 - loss: 4.8884 - acc: 0.010 - ETA: 3:39 - loss: 4.8883 - acc: 0.010 - ETA: 3:38 - loss: 4.8885 - acc: 0.010 - ETA: 3:37 - loss: 4.8885 - acc: 0.010 - ETA: 3:36 - loss: 4.8885 - acc: 0.010 - ETA: 3:35 - loss: 4.8881 - acc: 0.010 - ETA: 3:34 - loss: 4.8880 - acc: 0.010 - ETA: 3:33 - loss: 4.8882 - acc: 0.010 - ETA: 3:32 - loss: 4.8881 - acc: 0.011 - ETA: 3:31 - loss: 4.8884 - acc: 0.010 - ETA: 3:30 - loss: 4.8884 - acc: 0.011 - ETA: 3:29 - loss: 4.8885 - acc: 0.011 - ETA: 3:28 - loss: 4.8882 - acc: 0.011 - ETA: 3:27 - loss: 4.8885 - acc: 0.011 - ETA: 3:26 - loss: 4.8886 - acc: 0.011 - ETA: 3:25 - loss: 4.8885 - acc: 0.011 - ETA: 3:24 - loss: 4.8882 - acc: 0.011 - ETA: 3:23 - loss: 4.8884 - acc: 0.011 - ETA: 3:21 - loss: 4.8885 - acc: 0.010 - ETA: 3:20 - loss: 4.8885 - acc: 0.010 - ETA: 3:19 - loss: 4.8882 - acc: 0.010 - ETA: 3:18 - loss: 4.8884 - acc: 0.010 - ETA: 3:17 - loss: 4.8884 - acc: 0.010 - ETA: 3:16 - loss: 4.8884 - acc: 0.010 - ETA: 3:15 - loss: 4.8883 - acc: 0.010 - ETA: 3:14 - loss: 4.8881 - acc: 0.010 - ETA: 3:13 - loss: 4.8883 - acc: 0.010 - ETA: 3:12 - loss: 4.8887 - acc: 0.010 - ETA: 3:11 - loss: 4.8887 - acc: 0.010 - ETA: 3:10 - loss: 4.8888 - acc: 0.010 - ETA: 3:09 - loss: 4.8887 - acc: 0.010 - ETA: 3:08 - loss: 4.8887 - acc: 0.009 - ETA: 3:07 - loss: 4.8887 - acc: 0.009 - ETA: 3:06 - loss: 4.8887 - acc: 0.009 - ETA: 3:05 - loss: 4.8886 - acc: 0.010 - ETA: 3:04 - loss: 4.8886 - acc: 0.010 - ETA: 3:03 - loss: 4.8885 - acc: 0.009 - ETA: 3:02 - loss: 4.8887 - acc: 0.009 - ETA: 3:01 - loss: 4.8887 - acc: 0.009 - ETA: 3:00 - loss: 4.8887 - acc: 0.009 - ETA: 2:59 - loss: 4.8885 - acc: 0.010 - ETA: 2:58 - loss: 4.8882 - acc: 0.010 - ETA: 2:57 - loss: 4.8881 - acc: 0.010 - ETA: 2:56 - loss: 4.8881 - acc: 0.010 - ETA: 2:55 - loss: 4.8883 - acc: 0.010 - ETA: 2:54 - loss: 4.8882 - acc: 0.010 - ETA: 2:53 - loss: 4.8882 - acc: 0.009 - ETA: 2:52 - loss: 4.8883 - acc: 0.010 - ETA: 2:51 - loss: 4.8882 - acc: 0.010 - ETA: 2:50 - loss: 4.8883 - acc: 0.010 - ETA: 2:49 - loss: 4.8882 - acc: 0.010 - ETA: 2:48 - loss: 4.8883 - acc: 0.010 - ETA: 2:47 - loss: 4.8882 - acc: 0.010 - ETA: 2:46 - loss: 4.8881 - acc: 0.010 - ETA: 2:45 - loss: 4.8880 - acc: 0.010 - ETA: 2:44 - loss: 4.8880 - acc: 0.010 - ETA: 2:43 - loss: 4.8880 - acc: 0.010 - ETA: 2:42 - loss: 4.8880 - acc: 0.010 - ETA: 2:41 - loss: 4.8880 - acc: 0.010 - ETA: 2:40 - loss: 4.8877 - acc: 0.010 - ETA: 2:39 - loss: 4.8878 - acc: 0.010 - ETA: 2:38 - loss: 4.8879 - acc: 0.009 - ETA: 2:37 - loss: 4.8878 - acc: 0.010 - ETA: 2:36 - loss: 4.8876 - acc: 0.010 - ETA: 2:35 - loss: 4.8874 - acc: 0.010 - ETA: 2:34 - loss: 4.8873 - acc: 0.010 - ETA: 2:33 - loss: 4.8871 - acc: 0.010 - ETA: 2:32 - loss: 4.8870 - acc: 0.010 - ETA: 2:31 - loss: 4.8872 - acc: 0.010 - ETA: 2:30 - loss: 4.8871 - acc: 0.010 - ETA: 2:29 - loss: 4.8871 - acc: 0.010 - ETA: 2:28 - loss: 4.8871 - acc: 0.010 - ETA: 2:27 - loss: 4.8871 - acc: 0.010 - ETA: 2:26 - loss: 4.8869 - acc: 0.010 - ETA: 2:25 - loss: 4.8869 - acc: 0.010 - ETA: 2:24 - loss: 4.8867 - acc: 0.010 - ETA: 2:23 - loss: 4.8865 - acc: 0.010 - ETA: 2:22 - loss: 4.8865 - acc: 0.010 - ETA: 2:21 - loss: 4.8867 - acc: 0.010 - ETA: 2:20 - loss: 4.8869 - acc: 0.010 - ETA: 2:19 - loss: 4.8868 - acc: 0.010 - ETA: 2:18 - loss: 4.8868 - acc: 0.010 - ETA: 2:17 - loss: 4.8869 - acc: 0.010 - ETA: 2:16 - loss: 4.8871 - acc: 0.010 - ETA: 2:15 - loss: 4.8873 - acc: 0.010 - ETA: 2:14 - loss: 4.8874 - acc: 0.010 - ETA: 2:13 - loss: 4.8872 - acc: 0.010 - ETA: 2:12 - loss: 4.8874 - acc: 0.0106680/6680 [==============================] - ETA: 2:11 - loss: 4.8872 - acc: 0.011 - ETA: 2:10 - loss: 4.8871 - acc: 0.011 - ETA: 2:09 - loss: 4.8872 - acc: 0.011 - ETA: 2:08 - loss: 4.8871 - acc: 0.010 - ETA: 2:07 - loss: 4.8870 - acc: 0.010 - ETA: 2:06 - loss: 4.8869 - acc: 0.010 - ETA: 2:05 - loss: 4.8870 - acc: 0.010 - ETA: 2:04 - loss: 4.8869 - acc: 0.010 - ETA: 2:03 - loss: 4.8866 - acc: 0.010 - ETA: 2:02 - loss: 4.8866 - acc: 0.010 - ETA: 2:01 - loss: 4.8865 - acc: 0.010 - ETA: 2:00 - loss: 4.8865 - acc: 0.010 - ETA: 1:59 - loss: 4.8865 - acc: 0.010 - ETA: 1:58 - loss: 4.8864 - acc: 0.010 - ETA: 1:57 - loss: 4.8864 - acc: 0.010 - ETA: 1:56 - loss: 4.8862 - acc: 0.010 - ETA: 1:55 - loss: 4.8863 - acc: 0.010 - ETA: 1:54 - loss: 4.8862 - acc: 0.010 - ETA: 1:53 - loss: 4.8859 - acc: 0.010 - ETA: 1:52 - loss: 4.8857 - acc: 0.011 - ETA: 1:51 - loss: 4.8858 - acc: 0.011 - ETA: 1:50 - loss: 4.8858 - acc: 0.010 - ETA: 1:49 - loss: 4.8859 - acc: 0.011 - ETA: 1:48 - loss: 4.8862 - acc: 0.011 - ETA: 1:47 - loss: 4.8861 - acc: 0.011 - ETA: 1:46 - loss: 4.8863 - acc: 0.011 - ETA: 1:45 - loss: 4.8863 - acc: 0.011 - ETA: 1:44 - loss: 4.8862 - acc: 0.011 - ETA: 1:43 - loss: 4.8862 - acc: 0.011 - ETA: 1:42 - loss: 4.8861 - acc: 0.011 - ETA: 1:41 - loss: 4.8860 - acc: 0.010 - ETA: 1:40 - loss: 4.8859 - acc: 0.010 - ETA: 1:39 - loss: 4.8857 - acc: 0.010 - ETA: 1:38 - loss: 4.8854 - acc: 0.011 - ETA: 1:37 - loss: 4.8853 - acc: 0.011 - ETA: 1:36 - loss: 4.8849 - acc: 0.011 - ETA: 1:35 - loss: 4.8849 - acc: 0.011 - ETA: 1:34 - loss: 4.8847 - acc: 0.011 - ETA: 1:33 - loss: 4.8848 - acc: 0.011 - ETA: 1:32 - loss: 4.8849 - acc: 0.011 - ETA: 1:31 - loss: 4.8850 - acc: 0.011 - ETA: 1:30 - loss: 4.8851 - acc: 0.011 - ETA: 1:29 - loss: 4.8851 - acc: 0.011 - ETA: 1:28 - loss: 4.8850 - acc: 0.011 - ETA: 1:27 - loss: 4.8847 - acc: 0.011 - ETA: 1:26 - loss: 4.8849 - acc: 0.011 - ETA: 1:25 - loss: 4.8848 - acc: 0.011 - ETA: 1:24 - loss: 4.8845 - acc: 0.011 - ETA: 1:23 - loss: 4.8847 - acc: 0.011 - ETA: 1:22 - loss: 4.8844 - acc: 0.011 - ETA: 1:21 - loss: 4.8846 - acc: 0.011 - ETA: 1:20 - loss: 4.8846 - acc: 0.011 - ETA: 1:19 - loss: 4.8845 - acc: 0.011 - ETA: 1:18 - loss: 4.8845 - acc: 0.011 - ETA: 1:17 - loss: 4.8847 - acc: 0.011 - ETA: 1:16 - loss: 4.8847 - acc: 0.011 - ETA: 1:15 - loss: 4.8847 - acc: 0.011 - ETA: 1:14 - loss: 4.8848 - acc: 0.011 - ETA: 1:13 - loss: 4.8848 - acc: 0.011 - ETA: 1:12 - loss: 4.8850 - acc: 0.011 - ETA: 1:11 - loss: 4.8849 - acc: 0.011 - ETA: 1:10 - loss: 4.8849 - acc: 0.011 - ETA: 1:09 - loss: 4.8848 - acc: 0.011 - ETA: 1:08 - loss: 4.8848 - acc: 0.011 - ETA: 1:07 - loss: 4.8849 - acc: 0.011 - ETA: 1:06 - loss: 4.8850 - acc: 0.011 - ETA: 1:05 - loss: 4.8852 - acc: 0.011 - ETA: 1:04 - loss: 4.8851 - acc: 0.011 - ETA: 1:03 - loss: 4.8850 - acc: 0.011 - ETA: 1:02 - loss: 4.8849 - acc: 0.011 - ETA: 1:01 - loss: 4.8850 - acc: 0.011 - ETA: 1:00 - loss: 4.8850 - acc: 0.010 - ETA: 59s - loss: 4.8853 - acc: 0.010 - ETA: 58s - loss: 4.8853 - acc: 0.01 - ETA: 57s - loss: 4.8853 - acc: 0.01 - ETA: 56s - loss: 4.8855 - acc: 0.01 - ETA: 55s - loss: 4.8854 - acc: 0.01 - ETA: 54s - loss: 4.8853 - acc: 0.01 - ETA: 53s - loss: 4.8853 - acc: 0.01 - ETA: 52s - loss: 4.8852 - acc: 0.01 - ETA: 51s - loss: 4.8852 - acc: 0.01 - ETA: 50s - loss: 4.8852 - acc: 0.01 - ETA: 49s - loss: 4.8849 - acc: 0.01 - ETA: 48s - loss: 4.8849 - acc: 0.01 - ETA: 47s - loss: 4.8848 - acc: 0.01 - ETA: 46s - loss: 4.8847 - acc: 0.01 - ETA: 45s - loss: 4.8847 - acc: 0.01 - ETA: 44s - loss: 4.8847 - acc: 0.01 - ETA: 43s - loss: 4.8846 - acc: 0.01 - ETA: 42s - loss: 4.8845 - acc: 0.01 - ETA: 41s - loss: 4.8843 - acc: 0.01 - ETA: 40s - loss: 4.8844 - acc: 0.01 - ETA: 39s - loss: 4.8842 - acc: 0.01 - ETA: 38s - loss: 4.8845 - acc: 0.01 - ETA: 37s - loss: 4.8846 - acc: 0.01 - ETA: 36s - loss: 4.8847 - acc: 0.01 - ETA: 35s - loss: 4.8847 - acc: 0.01 - ETA: 34s - loss: 4.8848 - acc: 0.01 - ETA: 33s - loss: 4.8846 - acc: 0.01 - ETA: 32s - loss: 4.8848 - acc: 0.01 - ETA: 31s - loss: 4.8848 - acc: 0.01 - ETA: 30s - loss: 4.8849 - acc: 0.01 - ETA: 29s - loss: 4.8848 - acc: 0.01 - ETA: 28s - loss: 4.8849 - acc: 0.01 - ETA: 27s - loss: 4.8848 - acc: 0.01 - ETA: 26s - loss: 4.8849 - acc: 0.01 - ETA: 25s - loss: 4.8848 - acc: 0.01 - ETA: 24s - loss: 4.8847 - acc: 0.01 - ETA: 23s - loss: 4.8846 - acc: 0.01 - ETA: 22s - loss: 4.8846 - acc: 0.01 - ETA: 21s - loss: 4.8845 - acc: 0.01 - ETA: 20s - loss: 4.8845 - acc: 0.01 - ETA: 19s - loss: 4.8846 - acc: 0.01 - ETA: 18s - loss: 4.8845 - acc: 0.01 - ETA: 17s - loss: 4.8846 - acc: 0.01 - ETA: 16s - loss: 4.8844 - acc: 0.01 - ETA: 15s - loss: 4.8843 - acc: 0.01 - ETA: 14s - loss: 4.8843 - acc: 0.01 - ETA: 13s - loss: 4.8844 - acc: 0.01 - ETA: 12s - loss: 4.8844 - acc: 0.01 - ETA: 11s - loss: 4.8843 - acc: 0.01 - ETA: 10s - loss: 4.8842 - acc: 0.01 - ETA: 9s - loss: 4.8843 - acc: 0.0106 - ETA: 8s - loss: 4.8842 - acc: 0.010 - ETA: 7s - loss: 4.8844 - acc: 0.010 - ETA: 6s - loss: 4.8843 - acc: 0.010 - ETA: 5s - loss: 4.8845 - acc: 0.010 - ETA: 4s - loss: 4.8846 - acc: 0.010 - ETA: 3s - loss: 4.8846 - acc: 0.010 - ETA: 2s - loss: 4.8845 - acc: 0.010 - ETA: 1s - loss: 4.8845 - acc: 0.010 - 357s 53ms/step - loss: 4.8845 - acc: 0.0105 - val_loss: 4.8699 - val_acc: 0.0120\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.86985, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 7:20 - loss: 4.9048 - acc: 0.0000e+0 - ETA: 6:13 - loss: 4.9136 - acc: 0.0000e+0 - ETA: 5:49 - loss: 4.9061 - acc: 0.0000e+0 - ETA: 5:47 - loss: 4.8898 - acc: 0.0125    - ETA: 5:43 - loss: 4.8931 - acc: 0.010 - ETA: 5:39 - loss: 4.8801 - acc: 0.008 - ETA: 5:49 - loss: 4.8748 - acc: 0.007 - ETA: 6:54 - loss: 4.8680 - acc: 0.012 - ETA: 7:05 - loss: 4.8650 - acc: 0.016 - ETA: 7:28 - loss: 4.8661 - acc: 0.015 - ETA: 7:22 - loss: 4.8678 - acc: 0.013 - ETA: 7:19 - loss: 4.8642 - acc: 0.016 - ETA: 7:15 - loss: 4.8659 - acc: 0.015 - ETA: 7:03 - loss: 4.8669 - acc: 0.017 - ETA: 6:53 - loss: 4.8660 - acc: 0.016 - ETA: 6:44 - loss: 4.8639 - acc: 0.015 - ETA: 6:37 - loss: 4.8616 - acc: 0.014 - ETA: 6:29 - loss: 4.8631 - acc: 0.013 - ETA: 6:23 - loss: 4.8636 - acc: 0.013 - ETA: 6:19 - loss: 4.8615 - acc: 0.012 - ETA: 6:14 - loss: 4.8644 - acc: 0.011 - ETA: 6:09 - loss: 4.8647 - acc: 0.011 - ETA: 6:04 - loss: 4.8684 - acc: 0.010 - ETA: 6:00 - loss: 4.8679 - acc: 0.010 - ETA: 5:56 - loss: 4.8701 - acc: 0.012 - ETA: 5:52 - loss: 4.8691 - acc: 0.011 - ETA: 5:49 - loss: 4.8707 - acc: 0.011 - ETA: 5:46 - loss: 4.8690 - acc: 0.010 - ETA: 5:44 - loss: 4.8698 - acc: 0.010 - ETA: 5:41 - loss: 4.8696 - acc: 0.010 - ETA: 5:38 - loss: 4.8687 - acc: 0.009 - ETA: 5:35 - loss: 4.8684 - acc: 0.010 - ETA: 5:32 - loss: 4.8681 - acc: 0.010 - ETA: 5:30 - loss: 4.8711 - acc: 0.010 - ETA: 5:28 - loss: 4.8715 - acc: 0.010 - ETA: 5:26 - loss: 4.8716 - acc: 0.009 - ETA: 5:24 - loss: 4.8721 - acc: 0.009 - ETA: 5:23 - loss: 4.8724 - acc: 0.009 - ETA: 5:21 - loss: 4.8708 - acc: 0.009 - ETA: 5:19 - loss: 4.8712 - acc: 0.010 - ETA: 5:17 - loss: 4.8701 - acc: 0.011 - ETA: 5:15 - loss: 4.8703 - acc: 0.013 - ETA: 5:14 - loss: 4.8729 - acc: 0.012 - ETA: 5:13 - loss: 4.8707 - acc: 0.013 - ETA: 5:11 - loss: 4.8714 - acc: 0.013 - ETA: 5:09 - loss: 4.8712 - acc: 0.013 - ETA: 5:07 - loss: 4.8716 - acc: 0.012 - ETA: 5:05 - loss: 4.8709 - acc: 0.012 - ETA: 5:03 - loss: 4.8706 - acc: 0.012 - ETA: 5:01 - loss: 4.8703 - acc: 0.012 - ETA: 5:00 - loss: 4.8699 - acc: 0.011 - ETA: 4:58 - loss: 4.8697 - acc: 0.011 - ETA: 4:56 - loss: 4.8698 - acc: 0.011 - ETA: 4:54 - loss: 4.8694 - acc: 0.011 - ETA: 4:53 - loss: 4.8676 - acc: 0.011 - ETA: 4:51 - loss: 4.8665 - acc: 0.011 - ETA: 4:50 - loss: 4.8660 - acc: 0.011 - ETA: 4:49 - loss: 4.8640 - acc: 0.012 - ETA: 4:47 - loss: 4.8632 - acc: 0.012 - ETA: 4:46 - loss: 4.8640 - acc: 0.012 - ETA: 4:45 - loss: 4.8623 - acc: 0.013 - ETA: 4:43 - loss: 4.8595 - acc: 0.014 - ETA: 4:42 - loss: 4.8593 - acc: 0.014 - ETA: 4:41 - loss: 4.8571 - acc: 0.014 - ETA: 4:41 - loss: 4.8584 - acc: 0.013 - ETA: 4:40 - loss: 4.8596 - acc: 0.013 - ETA: 4:39 - loss: 4.8584 - acc: 0.013 - ETA: 4:38 - loss: 4.8591 - acc: 0.014 - ETA: 4:37 - loss: 4.8604 - acc: 0.013 - ETA: 4:36 - loss: 4.8612 - acc: 0.013 - ETA: 4:35 - loss: 4.8611 - acc: 0.014 - ETA: 4:34 - loss: 4.8619 - acc: 0.013 - ETA: 4:33 - loss: 4.8626 - acc: 0.013 - ETA: 4:32 - loss: 4.8621 - acc: 0.013 - ETA: 4:31 - loss: 4.8620 - acc: 0.013 - ETA: 4:30 - loss: 4.8626 - acc: 0.013 - ETA: 4:29 - loss: 4.8629 - acc: 0.013 - ETA: 4:28 - loss: 4.8622 - acc: 0.014 - ETA: 4:27 - loss: 4.8623 - acc: 0.013 - ETA: 4:26 - loss: 4.8628 - acc: 0.013 - ETA: 4:25 - loss: 4.8626 - acc: 0.013 - ETA: 4:24 - loss: 4.8627 - acc: 0.013 - ETA: 4:23 - loss: 4.8626 - acc: 0.013 - ETA: 4:22 - loss: 4.8628 - acc: 0.013 - ETA: 4:20 - loss: 4.8627 - acc: 0.012 - ETA: 4:19 - loss: 4.8630 - acc: 0.012 - ETA: 4:18 - loss: 4.8627 - acc: 0.013 - ETA: 4:17 - loss: 4.8626 - acc: 0.013 - ETA: 4:16 - loss: 4.8631 - acc: 0.014 - ETA: 4:14 - loss: 4.8629 - acc: 0.013 - ETA: 4:13 - loss: 4.8635 - acc: 0.013 - ETA: 4:12 - loss: 4.8635 - acc: 0.013 - ETA: 4:11 - loss: 4.8643 - acc: 0.014 - ETA: 4:10 - loss: 4.8640 - acc: 0.013 - ETA: 4:08 - loss: 4.8633 - acc: 0.013 - ETA: 4:07 - loss: 4.8624 - acc: 0.014 - ETA: 4:06 - loss: 4.8621 - acc: 0.014 - ETA: 4:05 - loss: 4.8622 - acc: 0.014 - ETA: 4:04 - loss: 4.8616 - acc: 0.014 - ETA: 4:02 - loss: 4.8619 - acc: 0.015 - ETA: 4:01 - loss: 4.8623 - acc: 0.014 - ETA: 4:00 - loss: 4.8621 - acc: 0.014 - ETA: 3:58 - loss: 4.8625 - acc: 0.014 - ETA: 3:57 - loss: 4.8629 - acc: 0.014 - ETA: 3:56 - loss: 4.8626 - acc: 0.014 - ETA: 3:54 - loss: 4.8629 - acc: 0.014 - ETA: 3:53 - loss: 4.8621 - acc: 0.014 - ETA: 3:52 - loss: 4.8630 - acc: 0.014 - ETA: 3:51 - loss: 4.8623 - acc: 0.014 - ETA: 3:49 - loss: 4.8633 - acc: 0.014 - ETA: 3:48 - loss: 4.8633 - acc: 0.014 - ETA: 3:47 - loss: 4.8632 - acc: 0.013 - ETA: 3:46 - loss: 4.8633 - acc: 0.014 - ETA: 3:44 - loss: 4.8642 - acc: 0.014 - ETA: 3:43 - loss: 4.8646 - acc: 0.014 - ETA: 3:42 - loss: 4.8641 - acc: 0.014 - ETA: 3:41 - loss: 4.8642 - acc: 0.014 - ETA: 3:40 - loss: 4.8634 - acc: 0.014 - ETA: 3:38 - loss: 4.8637 - acc: 0.014 - ETA: 3:37 - loss: 4.8641 - acc: 0.014 - ETA: 3:36 - loss: 4.8638 - acc: 0.014 - ETA: 3:35 - loss: 4.8634 - acc: 0.014 - ETA: 3:34 - loss: 4.8636 - acc: 0.014 - ETA: 3:33 - loss: 4.8643 - acc: 0.014 - ETA: 3:31 - loss: 4.8644 - acc: 0.014 - ETA: 3:30 - loss: 4.8635 - acc: 0.014 - ETA: 3:29 - loss: 4.8634 - acc: 0.015 - ETA: 3:28 - loss: 4.8636 - acc: 0.014 - ETA: 3:27 - loss: 4.8643 - acc: 0.014 - ETA: 3:25 - loss: 4.8646 - acc: 0.014 - ETA: 3:24 - loss: 4.8642 - acc: 0.014 - ETA: 3:23 - loss: 4.8642 - acc: 0.014 - ETA: 3:22 - loss: 4.8642 - acc: 0.014 - ETA: 3:21 - loss: 4.8641 - acc: 0.014 - ETA: 3:20 - loss: 4.8637 - acc: 0.014 - ETA: 3:19 - loss: 4.8640 - acc: 0.014 - ETA: 3:19 - loss: 4.8640 - acc: 0.013 - ETA: 3:18 - loss: 4.8634 - acc: 0.014 - ETA: 3:17 - loss: 4.8638 - acc: 0.014 - ETA: 3:16 - loss: 4.8642 - acc: 0.013 - ETA: 3:15 - loss: 4.8635 - acc: 0.013 - ETA: 3:14 - loss: 4.8637 - acc: 0.013 - ETA: 3:13 - loss: 4.8647 - acc: 0.013 - ETA: 3:12 - loss: 4.8642 - acc: 0.013 - ETA: 3:11 - loss: 4.8647 - acc: 0.013 - ETA: 3:10 - loss: 4.8648 - acc: 0.013 - ETA: 3:10 - loss: 4.8649 - acc: 0.013 - ETA: 3:09 - loss: 4.8650 - acc: 0.013 - ETA: 3:08 - loss: 4.8642 - acc: 0.013 - ETA: 3:07 - loss: 4.8640 - acc: 0.013 - ETA: 3:06 - loss: 4.8637 - acc: 0.014 - ETA: 3:05 - loss: 4.8634 - acc: 0.014 - ETA: 3:05 - loss: 4.8636 - acc: 0.014 - ETA: 3:04 - loss: 4.8639 - acc: 0.014 - ETA: 3:03 - loss: 4.8639 - acc: 0.013 - ETA: 3:02 - loss: 4.8641 - acc: 0.013 - ETA: 3:01 - loss: 4.8643 - acc: 0.013 - ETA: 3:00 - loss: 4.8641 - acc: 0.014 - ETA: 2:59 - loss: 4.8641 - acc: 0.014 - ETA: 2:58 - loss: 4.8638 - acc: 0.014 - ETA: 2:57 - loss: 4.8641 - acc: 0.014 - ETA: 2:56 - loss: 4.8644 - acc: 0.013 - ETA: 2:55 - loss: 4.8644 - acc: 0.014 - ETA: 2:54 - loss: 4.8646 - acc: 0.014 - ETA: 2:53 - loss: 4.8641 - acc: 0.013 - ETA: 2:52 - loss: 4.8643 - acc: 0.013 - ETA: 2:51 - loss: 4.8645 - acc: 0.013 - ETA: 2:51 - loss: 4.8643 - acc: 0.013 - ETA: 2:50 - loss: 4.8647 - acc: 0.013 - ETA: 2:49 - loss: 4.8648 - acc: 0.013 - ETA: 2:47 - loss: 4.8646 - acc: 0.013 - ETA: 2:46 - loss: 4.8645 - acc: 0.013 - ETA: 2:45 - loss: 4.8649 - acc: 0.013 - ETA: 2:44 - loss: 4.8644 - acc: 0.013 - ETA: 2:43 - loss: 4.8639 - acc: 0.013 - ETA: 2:42 - loss: 4.8638 - acc: 0.013 - ETA: 2:41 - loss: 4.8642 - acc: 0.013 - ETA: 2:40 - loss: 4.8636 - acc: 0.013 - ETA: 2:39 - loss: 4.8637 - acc: 0.013 - ETA: 2:38 - loss: 4.8638 - acc: 0.013 - ETA: 2:37 - loss: 4.8641 - acc: 0.013 - ETA: 2:36 - loss: 4.8644 - acc: 0.013 - ETA: 2:35 - loss: 4.8647 - acc: 0.013 - ETA: 2:34 - loss: 4.8645 - acc: 0.013 - ETA: 2:33 - loss: 4.8644 - acc: 0.013 - ETA: 2:32 - loss: 4.8644 - acc: 0.013 - ETA: 2:31 - loss: 4.8643 - acc: 0.013 - ETA: 2:31 - loss: 4.8645 - acc: 0.013 - ETA: 2:30 - loss: 4.8650 - acc: 0.013 - ETA: 2:29 - loss: 4.8652 - acc: 0.013 - ETA: 2:28 - loss: 4.8654 - acc: 0.013 - ETA: 2:27 - loss: 4.8655 - acc: 0.013 - ETA: 2:26 - loss: 4.8655 - acc: 0.014 - ETA: 2:25 - loss: 4.8656 - acc: 0.013 - ETA: 2:24 - loss: 4.8659 - acc: 0.013 - ETA: 2:25 - loss: 4.8655 - acc: 0.013 - ETA: 2:24 - loss: 4.8655 - acc: 0.013 - ETA: 2:24 - loss: 4.8655 - acc: 0.013 - ETA: 2:23 - loss: 4.8653 - acc: 0.013 - ETA: 2:22 - loss: 4.8652 - acc: 0.013 - ETA: 2:22 - loss: 4.8652 - acc: 0.013 - ETA: 2:21 - loss: 4.8653 - acc: 0.014 - ETA: 2:20 - loss: 4.8651 - acc: 0.014 - ETA: 2:20 - loss: 4.8654 - acc: 0.0142"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2:19 - loss: 4.8654 - acc: 0.014 - ETA: 2:18 - loss: 4.8655 - acc: 0.014 - ETA: 2:17 - loss: 4.8653 - acc: 0.014 - ETA: 2:16 - loss: 4.8651 - acc: 0.014 - ETA: 2:15 - loss: 4.8648 - acc: 0.014 - ETA: 2:14 - loss: 4.8649 - acc: 0.014 - ETA: 2:13 - loss: 4.8654 - acc: 0.014 - ETA: 2:12 - loss: 4.8656 - acc: 0.014 - ETA: 2:11 - loss: 4.8660 - acc: 0.014 - ETA: 2:10 - loss: 4.8661 - acc: 0.014 - ETA: 2:09 - loss: 4.8662 - acc: 0.014 - ETA: 2:08 - loss: 4.8661 - acc: 0.014 - ETA: 2:07 - loss: 4.8660 - acc: 0.014 - ETA: 2:06 - loss: 4.8660 - acc: 0.014 - ETA: 2:05 - loss: 4.8663 - acc: 0.014 - ETA: 2:04 - loss: 4.8662 - acc: 0.014 - ETA: 2:03 - loss: 4.8663 - acc: 0.014 - ETA: 2:01 - loss: 4.8663 - acc: 0.014 - ETA: 2:00 - loss: 4.8661 - acc: 0.014 - ETA: 1:59 - loss: 4.8662 - acc: 0.014 - ETA: 1:58 - loss: 4.8660 - acc: 0.014 - ETA: 1:57 - loss: 4.8661 - acc: 0.014 - ETA: 1:56 - loss: 4.8662 - acc: 0.014 - ETA: 1:56 - loss: 4.8663 - acc: 0.014 - ETA: 1:54 - loss: 4.8664 - acc: 0.014 - ETA: 1:53 - loss: 4.8666 - acc: 0.013 - ETA: 1:52 - loss: 4.8666 - acc: 0.013 - ETA: 1:51 - loss: 4.8666 - acc: 0.013 - ETA: 1:50 - loss: 4.8667 - acc: 0.013 - ETA: 1:49 - loss: 4.8668 - acc: 0.013 - ETA: 1:48 - loss: 4.8665 - acc: 0.013 - ETA: 1:47 - loss: 4.8670 - acc: 0.013 - ETA: 1:46 - loss: 4.8670 - acc: 0.013 - ETA: 1:45 - loss: 4.8670 - acc: 0.013 - ETA: 1:44 - loss: 4.8671 - acc: 0.013 - ETA: 1:43 - loss: 4.8672 - acc: 0.013 - ETA: 1:42 - loss: 4.8675 - acc: 0.013 - ETA: 1:41 - loss: 4.8673 - acc: 0.013 - ETA: 1:40 - loss: 4.8672 - acc: 0.013 - ETA: 1:39 - loss: 4.8671 - acc: 0.013 - ETA: 1:38 - loss: 4.8673 - acc: 0.013 - ETA: 1:37 - loss: 4.8673 - acc: 0.013 - ETA: 1:36 - loss: 4.8675 - acc: 0.013 - ETA: 1:35 - loss: 4.8674 - acc: 0.013 - ETA: 1:33 - loss: 4.8673 - acc: 0.013 - ETA: 1:32 - loss: 4.8673 - acc: 0.013 - ETA: 1:31 - loss: 4.8672 - acc: 0.013 - ETA: 1:30 - loss: 4.8673 - acc: 0.013 - ETA: 1:29 - loss: 4.8675 - acc: 0.013 - ETA: 1:28 - loss: 4.8672 - acc: 0.013 - ETA: 1:27 - loss: 4.8671 - acc: 0.013 - ETA: 1:26 - loss: 4.8668 - acc: 0.013 - ETA: 1:25 - loss: 4.8669 - acc: 0.013 - ETA: 1:24 - loss: 4.8670 - acc: 0.013 - ETA: 1:23 - loss: 4.8669 - acc: 0.012 - ETA: 1:22 - loss: 4.8670 - acc: 0.012 - ETA: 1:21 - loss: 4.8670 - acc: 0.012 - ETA: 1:20 - loss: 4.8670 - acc: 0.012 - ETA: 1:18 - loss: 4.8671 - acc: 0.012 - ETA: 1:17 - loss: 4.8670 - acc: 0.012 - ETA: 1:16 - loss: 4.8667 - acc: 0.012 - ETA: 1:15 - loss: 4.8666 - acc: 0.012 - ETA: 1:14 - loss: 4.8665 - acc: 0.012 - ETA: 1:13 - loss: 4.8664 - acc: 0.012 - ETA: 1:12 - loss: 4.8663 - acc: 0.012 - ETA: 1:11 - loss: 4.8664 - acc: 0.012 - ETA: 1:10 - loss: 4.8664 - acc: 0.012 - ETA: 1:09 - loss: 4.8665 - acc: 0.012 - ETA: 1:08 - loss: 4.8665 - acc: 0.012 - ETA: 1:07 - loss: 4.8665 - acc: 0.012 - ETA: 1:06 - loss: 4.8667 - acc: 0.012 - ETA: 1:05 - loss: 4.8666 - acc: 0.012 - ETA: 1:04 - loss: 4.8667 - acc: 0.012 - ETA: 1:03 - loss: 4.8665 - acc: 0.012 - ETA: 1:02 - loss: 4.8666 - acc: 0.012 - ETA: 1:01 - loss: 4.8669 - acc: 0.012 - ETA: 1:00 - loss: 4.8669 - acc: 0.012 - ETA: 59s - loss: 4.8667 - acc: 0.012 - ETA: 58s - loss: 4.8665 - acc: 0.01 - ETA: 56s - loss: 4.8667 - acc: 0.01 - ETA: 55s - loss: 4.8668 - acc: 0.01 - ETA: 54s - loss: 4.8664 - acc: 0.01 - ETA: 53s - loss: 4.8665 - acc: 0.01 - ETA: 52s - loss: 4.8665 - acc: 0.01 - ETA: 51s - loss: 4.8664 - acc: 0.01 - ETA: 50s - loss: 4.8661 - acc: 0.01 - ETA: 49s - loss: 4.8659 - acc: 0.01 - ETA: 48s - loss: 4.8659 - acc: 0.01 - ETA: 47s - loss: 4.8659 - acc: 0.01 - ETA: 46s - loss: 4.8661 - acc: 0.01 - ETA: 44s - loss: 4.8658 - acc: 0.01 - ETA: 43s - loss: 4.8660 - acc: 0.01 - ETA: 42s - loss: 4.8659 - acc: 0.01 - ETA: 41s - loss: 4.8663 - acc: 0.01 - ETA: 40s - loss: 4.8663 - acc: 0.01 - ETA: 39s - loss: 4.8663 - acc: 0.01 - ETA: 38s - loss: 4.8665 - acc: 0.01 - ETA: 37s - loss: 4.8665 - acc: 0.01 - ETA: 35s - loss: 4.8666 - acc: 0.01 - ETA: 34s - loss: 4.8667 - acc: 0.01 - ETA: 33s - loss: 4.8668 - acc: 0.01 - ETA: 32s - loss: 4.8670 - acc: 0.01 - ETA: 31s - loss: 4.8670 - acc: 0.01 - ETA: 30s - loss: 4.8669 - acc: 0.01 - ETA: 29s - loss: 4.8668 - acc: 0.01 - ETA: 27s - loss: 4.8667 - acc: 0.01 - ETA: 26s - loss: 4.8668 - acc: 0.01 - ETA: 25s - loss: 4.8668 - acc: 0.01 - ETA: 24s - loss: 4.8665 - acc: 0.01 - ETA: 23s - loss: 4.8664 - acc: 0.01 - ETA: 22s - loss: 4.8667 - acc: 0.01 - ETA: 21s - loss: 4.8668 - acc: 0.01 - ETA: 19s - loss: 4.8671 - acc: 0.01 - ETA: 18s - loss: 4.8669 - acc: 0.01 - ETA: 17s - loss: 4.8668 - acc: 0.01 - ETA: 16s - loss: 4.8668 - acc: 0.01 - ETA: 15s - loss: 4.8672 - acc: 0.01 - ETA: 14s - loss: 4.8670 - acc: 0.01 - ETA: 12s - loss: 4.8671 - acc: 0.01 - ETA: 11s - loss: 4.8669 - acc: 0.01 - ETA: 10s - loss: 4.8669 - acc: 0.01 - ETA: 9s - loss: 4.8670 - acc: 0.0126 - ETA: 8s - loss: 4.8670 - acc: 0.012 - ETA: 7s - loss: 4.8672 - acc: 0.012 - ETA: 5s - loss: 4.8670 - acc: 0.012 - ETA: 4s - loss: 4.8669 - acc: 0.012 - ETA: 3s - loss: 4.8670 - acc: 0.012 - ETA: 2s - loss: 4.8670 - acc: 0.012 - ETA: 1s - loss: 4.8670 - acc: 0.012 - 419s 63ms/step - loss: 4.8669 - acc: 0.0124 - val_loss: 4.8570 - val_acc: 0.0216\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.86985 to 4.85703, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/10\n",
      "4080/6680 [=================>............] - ETA: 8:46 - loss: 4.8252 - acc: 0.050 - ETA: 8:40 - loss: 4.8125 - acc: 0.050 - ETA: 8:35 - loss: 4.8214 - acc: 0.033 - ETA: 8:31 - loss: 4.8182 - acc: 0.025 - ETA: 8:34 - loss: 4.8085 - acc: 0.020 - ETA: 8:31 - loss: 4.8209 - acc: 0.016 - ETA: 8:29 - loss: 4.8385 - acc: 0.014 - ETA: 8:24 - loss: 4.8376 - acc: 0.018 - ETA: 8:19 - loss: 4.8374 - acc: 0.016 - ETA: 8:16 - loss: 4.8385 - acc: 0.015 - ETA: 8:11 - loss: 4.8412 - acc: 0.013 - ETA: 8:09 - loss: 4.8336 - acc: 0.012 - ETA: 8:07 - loss: 4.8367 - acc: 0.011 - ETA: 8:08 - loss: 4.8368 - acc: 0.010 - ETA: 8:06 - loss: 4.8446 - acc: 0.010 - ETA: 8:05 - loss: 4.8445 - acc: 0.009 - ETA: 8:02 - loss: 4.8422 - acc: 0.011 - ETA: 7:59 - loss: 4.8399 - acc: 0.013 - ETA: 8:00 - loss: 4.8424 - acc: 0.013 - ETA: 7:59 - loss: 4.8402 - acc: 0.012 - ETA: 7:58 - loss: 4.8426 - acc: 0.011 - ETA: 7:57 - loss: 4.8428 - acc: 0.011 - ETA: 7:55 - loss: 4.8419 - acc: 0.010 - ETA: 7:53 - loss: 4.8416 - acc: 0.012 - ETA: 7:50 - loss: 4.8429 - acc: 0.012 - ETA: 7:48 - loss: 4.8412 - acc: 0.013 - ETA: 7:45 - loss: 4.8422 - acc: 0.014 - ETA: 7:44 - loss: 4.8391 - acc: 0.017 - ETA: 7:42 - loss: 4.8374 - acc: 0.017 - ETA: 7:39 - loss: 4.8380 - acc: 0.016 - ETA: 7:38 - loss: 4.8373 - acc: 0.017 - ETA: 7:35 - loss: 4.8377 - acc: 0.017 - ETA: 7:34 - loss: 4.8368 - acc: 0.016 - ETA: 7:32 - loss: 4.8358 - acc: 0.016 - ETA: 7:29 - loss: 4.8319 - acc: 0.015 - ETA: 7:28 - loss: 4.8328 - acc: 0.015 - ETA: 7:26 - loss: 4.8343 - acc: 0.014 - ETA: 7:24 - loss: 4.8361 - acc: 0.014 - ETA: 7:22 - loss: 4.8365 - acc: 0.014 - ETA: 7:21 - loss: 4.8381 - acc: 0.013 - ETA: 7:19 - loss: 4.8392 - acc: 0.013 - ETA: 7:17 - loss: 4.8378 - acc: 0.013 - ETA: 7:15 - loss: 4.8368 - acc: 0.015 - ETA: 7:13 - loss: 4.8387 - acc: 0.014 - ETA: 7:12 - loss: 4.8383 - acc: 0.014 - ETA: 7:09 - loss: 4.8385 - acc: 0.014 - ETA: 7:07 - loss: 4.8380 - acc: 0.013 - ETA: 7:05 - loss: 4.8377 - acc: 0.014 - ETA: 7:04 - loss: 4.8383 - acc: 0.014 - ETA: 7:02 - loss: 4.8382 - acc: 0.014 - ETA: 7:01 - loss: 4.8413 - acc: 0.013 - ETA: 7:00 - loss: 4.8415 - acc: 0.014 - ETA: 6:58 - loss: 4.8419 - acc: 0.014 - ETA: 6:56 - loss: 4.8415 - acc: 0.013 - ETA: 6:54 - loss: 4.8423 - acc: 0.013 - ETA: 6:53 - loss: 4.8418 - acc: 0.013 - ETA: 6:50 - loss: 4.8411 - acc: 0.013 - ETA: 6:49 - loss: 4.8408 - acc: 0.013 - ETA: 6:47 - loss: 4.8401 - acc: 0.013 - ETA: 6:46 - loss: 4.8415 - acc: 0.014 - ETA: 6:44 - loss: 4.8412 - acc: 0.014 - ETA: 6:43 - loss: 4.8413 - acc: 0.014 - ETA: 6:41 - loss: 4.8411 - acc: 0.014 - ETA: 6:40 - loss: 4.8407 - acc: 0.014 - ETA: 6:39 - loss: 4.8406 - acc: 0.014 - ETA: 6:37 - loss: 4.8415 - acc: 0.014 - ETA: 6:36 - loss: 4.8411 - acc: 0.014 - ETA: 6:34 - loss: 4.8419 - acc: 0.014 - ETA: 6:33 - loss: 4.8428 - acc: 0.013 - ETA: 6:32 - loss: 4.8426 - acc: 0.013 - ETA: 6:30 - loss: 4.8428 - acc: 0.013 - ETA: 6:29 - loss: 4.8427 - acc: 0.013 - ETA: 6:28 - loss: 4.8429 - acc: 0.013 - ETA: 6:28 - loss: 4.8428 - acc: 0.012 - ETA: 6:26 - loss: 4.8438 - acc: 0.012 - ETA: 6:25 - loss: 4.8448 - acc: 0.013 - ETA: 6:24 - loss: 4.8447 - acc: 0.013 - ETA: 6:22 - loss: 4.8453 - acc: 0.014 - ETA: 6:21 - loss: 4.8457 - acc: 0.013 - ETA: 6:20 - loss: 4.8459 - acc: 0.013 - ETA: 6:18 - loss: 4.8463 - acc: 0.013 - ETA: 6:17 - loss: 4.8467 - acc: 0.013 - ETA: 6:16 - loss: 4.8465 - acc: 0.013 - ETA: 6:14 - loss: 4.8466 - acc: 0.013 - ETA: 6:14 - loss: 4.8459 - acc: 0.012 - ETA: 6:12 - loss: 4.8453 - acc: 0.012 - ETA: 6:11 - loss: 4.8456 - acc: 0.013 - ETA: 6:09 - loss: 4.8442 - acc: 0.013 - ETA: 6:08 - loss: 4.8447 - acc: 0.014 - ETA: 6:08 - loss: 4.8457 - acc: 0.013 - ETA: 6:07 - loss: 4.8458 - acc: 0.013 - ETA: 6:06 - loss: 4.8460 - acc: 0.014 - ETA: 6:05 - loss: 4.8468 - acc: 0.014 - ETA: 6:04 - loss: 4.8465 - acc: 0.013 - ETA: 6:02 - loss: 4.8471 - acc: 0.013 - ETA: 6:01 - loss: 4.8461 - acc: 0.014 - ETA: 6:00 - loss: 4.8454 - acc: 0.013 - ETA: 5:58 - loss: 4.8445 - acc: 0.014 - ETA: 5:57 - loss: 4.8456 - acc: 0.014 - ETA: 5:54 - loss: 4.8458 - acc: 0.014 - ETA: 5:52 - loss: 4.8458 - acc: 0.014 - ETA: 5:49 - loss: 4.8473 - acc: 0.014 - ETA: 5:47 - loss: 4.8468 - acc: 0.014 - ETA: 5:44 - loss: 4.8472 - acc: 0.014 - ETA: 5:42 - loss: 4.8467 - acc: 0.014 - ETA: 5:40 - loss: 4.8464 - acc: 0.014 - ETA: 5:37 - loss: 4.8464 - acc: 0.014 - ETA: 5:35 - loss: 4.8457 - acc: 0.014 - ETA: 5:33 - loss: 4.8457 - acc: 0.015 - ETA: 5:30 - loss: 4.8451 - acc: 0.015 - ETA: 5:28 - loss: 4.8446 - acc: 0.015 - ETA: 5:26 - loss: 4.8436 - acc: 0.015 - ETA: 5:23 - loss: 4.8436 - acc: 0.015 - ETA: 5:21 - loss: 4.8435 - acc: 0.014 - ETA: 5:19 - loss: 4.8439 - acc: 0.014 - ETA: 5:17 - loss: 4.8439 - acc: 0.014 - ETA: 5:14 - loss: 4.8446 - acc: 0.014 - ETA: 5:12 - loss: 4.8435 - acc: 0.015 - ETA: 5:10 - loss: 4.8423 - acc: 0.015 - ETA: 5:08 - loss: 4.8428 - acc: 0.015 - ETA: 5:06 - loss: 4.8422 - acc: 0.014 - ETA: 5:03 - loss: 4.8421 - acc: 0.014 - ETA: 5:01 - loss: 4.8424 - acc: 0.014 - ETA: 4:59 - loss: 4.8428 - acc: 0.014 - ETA: 4:58 - loss: 4.8434 - acc: 0.014 - ETA: 4:56 - loss: 4.8432 - acc: 0.014 - ETA: 4:54 - loss: 4.8421 - acc: 0.015 - ETA: 4:53 - loss: 4.8422 - acc: 0.015 - ETA: 4:52 - loss: 4.8413 - acc: 0.015 - ETA: 4:50 - loss: 4.8412 - acc: 0.015 - ETA: 4:48 - loss: 4.8414 - acc: 0.015 - ETA: 4:46 - loss: 4.8422 - acc: 0.015 - ETA: 4:45 - loss: 4.8422 - acc: 0.015 - ETA: 4:43 - loss: 4.8422 - acc: 0.015 - ETA: 4:41 - loss: 4.8421 - acc: 0.015 - ETA: 4:39 - loss: 4.8420 - acc: 0.015 - ETA: 4:38 - loss: 4.8419 - acc: 0.015 - ETA: 4:36 - loss: 4.8415 - acc: 0.015 - ETA: 4:34 - loss: 4.8414 - acc: 0.015 - ETA: 4:33 - loss: 4.8419 - acc: 0.015 - ETA: 4:31 - loss: 4.8423 - acc: 0.015 - ETA: 4:29 - loss: 4.8423 - acc: 0.015 - ETA: 4:27 - loss: 4.8419 - acc: 0.015 - ETA: 4:26 - loss: 4.8418 - acc: 0.014 - ETA: 4:24 - loss: 4.8419 - acc: 0.014 - ETA: 4:22 - loss: 4.8418 - acc: 0.014 - ETA: 4:20 - loss: 4.8418 - acc: 0.014 - ETA: 4:19 - loss: 4.8422 - acc: 0.014 - ETA: 4:17 - loss: 4.8429 - acc: 0.014 - ETA: 4:15 - loss: 4.8428 - acc: 0.014 - ETA: 4:13 - loss: 4.8432 - acc: 0.014 - ETA: 4:11 - loss: 4.8436 - acc: 0.014 - ETA: 4:09 - loss: 4.8430 - acc: 0.014 - ETA: 4:07 - loss: 4.8432 - acc: 0.014 - ETA: 4:06 - loss: 4.8436 - acc: 0.014 - ETA: 4:04 - loss: 4.8430 - acc: 0.014 - ETA: 4:03 - loss: 4.8423 - acc: 0.014 - ETA: 4:01 - loss: 4.8423 - acc: 0.014 - ETA: 4:00 - loss: 4.8429 - acc: 0.014 - ETA: 3:58 - loss: 4.8429 - acc: 0.014 - ETA: 3:56 - loss: 4.8427 - acc: 0.014 - ETA: 3:55 - loss: 4.8432 - acc: 0.014 - ETA: 3:53 - loss: 4.8441 - acc: 0.014 - ETA: 3:52 - loss: 4.8441 - acc: 0.014 - ETA: 3:51 - loss: 4.8435 - acc: 0.014 - ETA: 3:49 - loss: 4.8436 - acc: 0.014 - ETA: 3:48 - loss: 4.8432 - acc: 0.014 - ETA: 3:46 - loss: 4.8429 - acc: 0.014 - ETA: 3:45 - loss: 4.8425 - acc: 0.013 - ETA: 3:43 - loss: 4.8419 - acc: 0.014 - ETA: 3:42 - loss: 4.8416 - acc: 0.014 - ETA: 3:40 - loss: 4.8410 - acc: 0.014 - ETA: 3:38 - loss: 4.8412 - acc: 0.014 - ETA: 3:37 - loss: 4.8413 - acc: 0.014 - ETA: 3:35 - loss: 4.8412 - acc: 0.014 - ETA: 3:34 - loss: 4.8410 - acc: 0.014 - ETA: 3:32 - loss: 4.8405 - acc: 0.014 - ETA: 3:30 - loss: 4.8404 - acc: 0.014 - ETA: 3:29 - loss: 4.8400 - acc: 0.014 - ETA: 3:27 - loss: 4.8401 - acc: 0.014 - ETA: 3:26 - loss: 4.8399 - acc: 0.014 - ETA: 3:24 - loss: 4.8399 - acc: 0.014 - ETA: 3:23 - loss: 4.8396 - acc: 0.013 - ETA: 3:21 - loss: 4.8392 - acc: 0.013 - ETA: 3:19 - loss: 4.8398 - acc: 0.013 - ETA: 3:18 - loss: 4.8395 - acc: 0.014 - ETA: 3:17 - loss: 4.8390 - acc: 0.014 - ETA: 3:15 - loss: 4.8386 - acc: 0.014 - ETA: 3:13 - loss: 4.8387 - acc: 0.014 - ETA: 3:12 - loss: 4.8381 - acc: 0.014 - ETA: 3:10 - loss: 4.8384 - acc: 0.014 - ETA: 3:09 - loss: 4.8379 - acc: 0.014 - ETA: 3:07 - loss: 4.8381 - acc: 0.014 - ETA: 3:06 - loss: 4.8380 - acc: 0.014 - ETA: 3:04 - loss: 4.8380 - acc: 0.014 - ETA: 3:03 - loss: 4.8376 - acc: 0.014 - ETA: 3:01 - loss: 4.8377 - acc: 0.014 - ETA: 3:00 - loss: 4.8375 - acc: 0.014 - ETA: 2:59 - loss: 4.8370 - acc: 0.015 - ETA: 2:57 - loss: 4.8370 - acc: 0.015 - ETA: 2:56 - loss: 4.8365 - acc: 0.014 - ETA: 2:54 - loss: 4.8365 - acc: 0.014 - ETA: 2:53 - loss: 4.8368 - acc: 0.014 - ETA: 2:51 - loss: 4.8371 - acc: 0.01476680/6680 [==============================] - ETA: 2:50 - loss: 4.8376 - acc: 0.014 - ETA: 2:48 - loss: 4.8371 - acc: 0.014 - ETA: 2:47 - loss: 4.8369 - acc: 0.014 - ETA: 2:45 - loss: 4.8374 - acc: 0.014 - ETA: 2:44 - loss: 4.8372 - acc: 0.014 - ETA: 2:42 - loss: 4.8371 - acc: 0.014 - ETA: 2:41 - loss: 4.8363 - acc: 0.014 - ETA: 2:40 - loss: 4.8362 - acc: 0.014 - ETA: 2:38 - loss: 4.8361 - acc: 0.014 - ETA: 2:37 - loss: 4.8360 - acc: 0.014 - ETA: 2:35 - loss: 4.8361 - acc: 0.014 - ETA: 2:34 - loss: 4.8363 - acc: 0.014 - ETA: 2:32 - loss: 4.8361 - acc: 0.014 - ETA: 2:31 - loss: 4.8362 - acc: 0.014 - ETA: 2:30 - loss: 4.8356 - acc: 0.014 - ETA: 2:28 - loss: 4.8358 - acc: 0.014 - ETA: 2:27 - loss: 4.8357 - acc: 0.014 - ETA: 2:25 - loss: 4.8361 - acc: 0.014 - ETA: 2:24 - loss: 4.8369 - acc: 0.014 - ETA: 2:22 - loss: 4.8367 - acc: 0.014 - ETA: 2:21 - loss: 4.8362 - acc: 0.014 - ETA: 2:20 - loss: 4.8363 - acc: 0.014 - ETA: 2:18 - loss: 4.8367 - acc: 0.014 - ETA: 2:17 - loss: 4.8372 - acc: 0.014 - ETA: 2:16 - loss: 4.8367 - acc: 0.014 - ETA: 2:14 - loss: 4.8370 - acc: 0.014 - ETA: 2:13 - loss: 4.8372 - acc: 0.014 - ETA: 2:12 - loss: 4.8367 - acc: 0.014 - ETA: 2:10 - loss: 4.8358 - acc: 0.015 - ETA: 2:09 - loss: 4.8364 - acc: 0.015 - ETA: 2:07 - loss: 4.8360 - acc: 0.015 - ETA: 2:06 - loss: 4.8361 - acc: 0.015 - ETA: 2:05 - loss: 4.8358 - acc: 0.015 - ETA: 2:03 - loss: 4.8363 - acc: 0.015 - ETA: 2:02 - loss: 4.8363 - acc: 0.015 - ETA: 2:01 - loss: 4.8357 - acc: 0.015 - ETA: 1:59 - loss: 4.8357 - acc: 0.015 - ETA: 1:58 - loss: 4.8360 - acc: 0.015 - ETA: 1:56 - loss: 4.8359 - acc: 0.015 - ETA: 1:55 - loss: 4.8359 - acc: 0.015 - ETA: 1:54 - loss: 4.8360 - acc: 0.015 - ETA: 1:52 - loss: 4.8359 - acc: 0.015 - ETA: 1:51 - loss: 4.8365 - acc: 0.015 - ETA: 1:50 - loss: 4.8365 - acc: 0.015 - ETA: 1:48 - loss: 4.8364 - acc: 0.015 - ETA: 1:47 - loss: 4.8366 - acc: 0.015 - ETA: 1:46 - loss: 4.8364 - acc: 0.015 - ETA: 1:44 - loss: 4.8366 - acc: 0.015 - ETA: 1:43 - loss: 4.8369 - acc: 0.016 - ETA: 1:42 - loss: 4.8365 - acc: 0.016 - ETA: 1:40 - loss: 4.8363 - acc: 0.016 - ETA: 1:39 - loss: 4.8359 - acc: 0.016 - ETA: 1:38 - loss: 4.8360 - acc: 0.016 - ETA: 1:36 - loss: 4.8362 - acc: 0.015 - ETA: 1:35 - loss: 4.8362 - acc: 0.015 - ETA: 1:34 - loss: 4.8362 - acc: 0.015 - ETA: 1:32 - loss: 4.8360 - acc: 0.015 - ETA: 1:31 - loss: 4.8361 - acc: 0.015 - ETA: 1:30 - loss: 4.8360 - acc: 0.015 - ETA: 1:28 - loss: 4.8360 - acc: 0.015 - ETA: 1:27 - loss: 4.8360 - acc: 0.015 - ETA: 1:26 - loss: 4.8359 - acc: 0.015 - ETA: 1:24 - loss: 4.8360 - acc: 0.015 - ETA: 1:23 - loss: 4.8358 - acc: 0.015 - ETA: 1:22 - loss: 4.8355 - acc: 0.015 - ETA: 1:21 - loss: 4.8352 - acc: 0.015 - ETA: 1:19 - loss: 4.8346 - acc: 0.015 - ETA: 1:18 - loss: 4.8342 - acc: 0.015 - ETA: 1:17 - loss: 4.8342 - acc: 0.015 - ETA: 1:15 - loss: 4.8339 - acc: 0.015 - ETA: 1:14 - loss: 4.8331 - acc: 0.015 - ETA: 1:13 - loss: 4.8330 - acc: 0.015 - ETA: 1:11 - loss: 4.8331 - acc: 0.015 - ETA: 1:10 - loss: 4.8331 - acc: 0.015 - ETA: 1:09 - loss: 4.8333 - acc: 0.015 - ETA: 1:08 - loss: 4.8334 - acc: 0.015 - ETA: 1:06 - loss: 4.8334 - acc: 0.015 - ETA: 1:05 - loss: 4.8329 - acc: 0.015 - ETA: 1:04 - loss: 4.8332 - acc: 0.015 - ETA: 1:02 - loss: 4.8332 - acc: 0.015 - ETA: 1:01 - loss: 4.8328 - acc: 0.015 - ETA: 1:00 - loss: 4.8328 - acc: 0.015 - ETA: 59s - loss: 4.8330 - acc: 0.015 - ETA: 57s - loss: 4.8322 - acc: 0.01 - ETA: 56s - loss: 4.8324 - acc: 0.01 - ETA: 55s - loss: 4.8324 - acc: 0.01 - ETA: 53s - loss: 4.8324 - acc: 0.01 - ETA: 52s - loss: 4.8325 - acc: 0.01 - ETA: 51s - loss: 4.8320 - acc: 0.01 - ETA: 49s - loss: 4.8322 - acc: 0.01 - ETA: 48s - loss: 4.8321 - acc: 0.01 - ETA: 47s - loss: 4.8318 - acc: 0.01 - ETA: 46s - loss: 4.8317 - acc: 0.01 - ETA: 44s - loss: 4.8314 - acc: 0.01 - ETA: 43s - loss: 4.8314 - acc: 0.01 - ETA: 42s - loss: 4.8318 - acc: 0.01 - ETA: 41s - loss: 4.8312 - acc: 0.01 - ETA: 39s - loss: 4.8315 - acc: 0.01 - ETA: 38s - loss: 4.8315 - acc: 0.01 - ETA: 37s - loss: 4.8311 - acc: 0.01 - ETA: 35s - loss: 4.8306 - acc: 0.01 - ETA: 34s - loss: 4.8306 - acc: 0.01 - ETA: 33s - loss: 4.8306 - acc: 0.01 - ETA: 32s - loss: 4.8311 - acc: 0.01 - ETA: 30s - loss: 4.8314 - acc: 0.01 - ETA: 29s - loss: 4.8312 - acc: 0.01 - ETA: 28s - loss: 4.8311 - acc: 0.01 - ETA: 27s - loss: 4.8308 - acc: 0.01 - ETA: 25s - loss: 4.8305 - acc: 0.01 - ETA: 24s - loss: 4.8308 - acc: 0.01 - ETA: 23s - loss: 4.8308 - acc: 0.01 - ETA: 22s - loss: 4.8303 - acc: 0.01 - ETA: 20s - loss: 4.8299 - acc: 0.01 - ETA: 19s - loss: 4.8296 - acc: 0.01 - ETA: 18s - loss: 4.8294 - acc: 0.01 - ETA: 17s - loss: 4.8297 - acc: 0.01 - ETA: 16s - loss: 4.8300 - acc: 0.01 - ETA: 14s - loss: 4.8303 - acc: 0.01 - ETA: 13s - loss: 4.8307 - acc: 0.01 - ETA: 12s - loss: 4.8306 - acc: 0.01 - ETA: 11s - loss: 4.8307 - acc: 0.01 - ETA: 9s - loss: 4.8311 - acc: 0.0164 - ETA: 8s - loss: 4.8307 - acc: 0.016 - ETA: 7s - loss: 4.8305 - acc: 0.016 - ETA: 6s - loss: 4.8307 - acc: 0.016 - ETA: 4s - loss: 4.8309 - acc: 0.016 - ETA: 3s - loss: 4.8310 - acc: 0.016 - ETA: 2s - loss: 4.8311 - acc: 0.016 - ETA: 1s - loss: 4.8311 - acc: 0.016 - 427s 64ms/step - loss: 4.8312 - acc: 0.0163 - val_loss: 4.8165 - val_acc: 0.0204\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.85703 to 4.81646, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 5:34 - loss: 4.7087 - acc: 0.050 - ETA: 5:37 - loss: 4.7049 - acc: 0.050 - ETA: 5:35 - loss: 4.7214 - acc: 0.033 - ETA: 5:38 - loss: 4.7302 - acc: 0.025 - ETA: 5:38 - loss: 4.7377 - acc: 0.020 - ETA: 5:39 - loss: 4.7433 - acc: 0.033 - ETA: 5:40 - loss: 4.7663 - acc: 0.028 - ETA: 5:45 - loss: 4.7702 - acc: 0.025 - ETA: 5:47 - loss: 4.7808 - acc: 0.022 - ETA: 5:45 - loss: 4.7745 - acc: 0.025 - ETA: 5:43 - loss: 4.7728 - acc: 0.022 - ETA: 5:42 - loss: 4.7817 - acc: 0.020 - ETA: 5:40 - loss: 4.7746 - acc: 0.019 - ETA: 5:39 - loss: 4.7654 - acc: 0.021 - ETA: 5:38 - loss: 4.7704 - acc: 0.020 - ETA: 5:36 - loss: 4.7618 - acc: 0.025 - ETA: 5:35 - loss: 4.7662 - acc: 0.023 - ETA: 5:34 - loss: 4.7638 - acc: 0.022 - ETA: 5:32 - loss: 4.7623 - acc: 0.023 - ETA: 5:31 - loss: 4.7623 - acc: 0.022 - ETA: 5:30 - loss: 4.7586 - acc: 0.023 - ETA: 5:28 - loss: 4.7661 - acc: 0.022 - ETA: 5:27 - loss: 4.7645 - acc: 0.021 - ETA: 5:26 - loss: 4.7740 - acc: 0.020 - ETA: 5:24 - loss: 4.7685 - acc: 0.020 - ETA: 5:23 - loss: 4.7679 - acc: 0.019 - ETA: 5:22 - loss: 4.7678 - acc: 0.018 - ETA: 5:20 - loss: 4.7736 - acc: 0.019 - ETA: 5:19 - loss: 4.7728 - acc: 0.019 - ETA: 5:18 - loss: 4.7746 - acc: 0.018 - ETA: 5:16 - loss: 4.7745 - acc: 0.017 - ETA: 5:15 - loss: 4.7738 - acc: 0.017 - ETA: 5:14 - loss: 4.7711 - acc: 0.016 - ETA: 5:13 - loss: 4.7738 - acc: 0.016 - ETA: 5:12 - loss: 4.7718 - acc: 0.017 - ETA: 5:10 - loss: 4.7670 - acc: 0.019 - ETA: 5:10 - loss: 4.7684 - acc: 0.020 - ETA: 5:09 - loss: 4.7642 - acc: 0.019 - ETA: 5:08 - loss: 4.7644 - acc: 0.019 - ETA: 5:06 - loss: 4.7636 - acc: 0.018 - ETA: 5:05 - loss: 4.7612 - acc: 0.018 - ETA: 5:04 - loss: 4.7635 - acc: 0.017 - ETA: 5:03 - loss: 4.7655 - acc: 0.018 - ETA: 5:02 - loss: 4.7651 - acc: 0.019 - ETA: 5:02 - loss: 4.7653 - acc: 0.018 - ETA: 5:01 - loss: 4.7668 - acc: 0.019 - ETA: 5:00 - loss: 4.7662 - acc: 0.019 - ETA: 4:59 - loss: 4.7646 - acc: 0.018 - ETA: 4:57 - loss: 4.7665 - acc: 0.018 - ETA: 4:56 - loss: 4.7676 - acc: 0.018 - ETA: 4:55 - loss: 4.7686 - acc: 0.017 - ETA: 4:54 - loss: 4.7721 - acc: 0.017 - ETA: 4:53 - loss: 4.7736 - acc: 0.017 - ETA: 4:52 - loss: 4.7721 - acc: 0.017 - ETA: 4:51 - loss: 4.7690 - acc: 0.017 - ETA: 4:50 - loss: 4.7706 - acc: 0.017 - ETA: 4:49 - loss: 4.7702 - acc: 0.017 - ETA: 4:48 - loss: 4.7698 - acc: 0.017 - ETA: 4:47 - loss: 4.7708 - acc: 0.016 - ETA: 4:46 - loss: 4.7707 - acc: 0.016 - ETA: 4:44 - loss: 4.7700 - acc: 0.016 - ETA: 4:44 - loss: 4.7728 - acc: 0.016 - ETA: 4:43 - loss: 4.7749 - acc: 0.015 - ETA: 4:42 - loss: 4.7766 - acc: 0.016 - ETA: 4:41 - loss: 4.7801 - acc: 0.016 - ETA: 4:40 - loss: 4.7829 - acc: 0.015 - ETA: 4:39 - loss: 4.7822 - acc: 0.015 - ETA: 4:38 - loss: 4.7837 - acc: 0.015 - ETA: 4:37 - loss: 4.7825 - acc: 0.015 - ETA: 4:36 - loss: 4.7828 - acc: 0.015 - ETA: 4:35 - loss: 4.7824 - acc: 0.015 - ETA: 4:34 - loss: 4.7821 - acc: 0.015 - ETA: 4:32 - loss: 4.7831 - acc: 0.016 - ETA: 4:31 - loss: 4.7816 - acc: 0.016 - ETA: 4:30 - loss: 4.7820 - acc: 0.016 - ETA: 4:29 - loss: 4.7812 - acc: 0.016 - ETA: 4:28 - loss: 4.7823 - acc: 0.016 - ETA: 4:27 - loss: 4.7829 - acc: 0.016 - ETA: 4:26 - loss: 4.7841 - acc: 0.015 - ETA: 4:25 - loss: 4.7825 - acc: 0.016 - ETA: 4:23 - loss: 4.7827 - acc: 0.016 - ETA: 4:22 - loss: 4.7820 - acc: 0.016 - ETA: 4:21 - loss: 4.7833 - acc: 0.016 - ETA: 4:20 - loss: 4.7823 - acc: 0.016 - ETA: 4:19 - loss: 4.7820 - acc: 0.015 - ETA: 4:18 - loss: 4.7821 - acc: 0.016 - ETA: 4:17 - loss: 4.7814 - acc: 0.016 - ETA: 4:16 - loss: 4.7817 - acc: 0.015 - ETA: 4:15 - loss: 4.7810 - acc: 0.016 - ETA: 4:14 - loss: 4.7819 - acc: 0.016 - ETA: 4:13 - loss: 4.7815 - acc: 0.015 - ETA: 4:12 - loss: 4.7805 - acc: 0.015 - ETA: 4:11 - loss: 4.7789 - acc: 0.015 - ETA: 4:09 - loss: 4.7799 - acc: 0.015 - ETA: 4:08 - loss: 4.7797 - acc: 0.015 - ETA: 4:07 - loss: 4.7811 - acc: 0.015 - ETA: 4:06 - loss: 4.7820 - acc: 0.015 - ETA: 4:05 - loss: 4.7831 - acc: 0.015 - ETA: 4:04 - loss: 4.7842 - acc: 0.015 - ETA: 4:03 - loss: 4.7841 - acc: 0.015 - ETA: 4:02 - loss: 4.7848 - acc: 0.015 - ETA: 4:01 - loss: 4.7864 - acc: 0.015 - ETA: 4:00 - loss: 4.7860 - acc: 0.015 - ETA: 3:59 - loss: 4.7841 - acc: 0.016 - ETA: 3:58 - loss: 4.7835 - acc: 0.016 - ETA: 3:57 - loss: 4.7827 - acc: 0.017 - ETA: 3:56 - loss: 4.7830 - acc: 0.016 - ETA: 3:55 - loss: 4.7839 - acc: 0.016 - ETA: 3:54 - loss: 4.7837 - acc: 0.016 - ETA: 3:53 - loss: 4.7842 - acc: 0.016 - ETA: 3:52 - loss: 4.7849 - acc: 0.016 - ETA: 3:51 - loss: 4.7850 - acc: 0.016 - ETA: 3:50 - loss: 4.7861 - acc: 0.015 - ETA: 3:49 - loss: 4.7858 - acc: 0.015 - ETA: 3:47 - loss: 4.7853 - acc: 0.015 - ETA: 3:46 - loss: 4.7845 - acc: 0.015 - ETA: 3:45 - loss: 4.7841 - acc: 0.015 - ETA: 3:44 - loss: 4.7830 - acc: 0.015 - ETA: 3:43 - loss: 4.7833 - acc: 0.015 - ETA: 3:42 - loss: 4.7831 - acc: 0.015 - ETA: 3:41 - loss: 4.7819 - acc: 0.015 - ETA: 3:40 - loss: 4.7830 - acc: 0.016 - ETA: 3:39 - loss: 4.7823 - acc: 0.015 - ETA: 3:38 - loss: 4.7828 - acc: 0.015 - ETA: 3:37 - loss: 4.7832 - acc: 0.015 - ETA: 3:36 - loss: 4.7851 - acc: 0.015 - ETA: 3:35 - loss: 4.7855 - acc: 0.015 - ETA: 3:34 - loss: 4.7866 - acc: 0.015 - ETA: 3:33 - loss: 4.7863 - acc: 0.015 - ETA: 3:32 - loss: 4.7858 - acc: 0.015 - ETA: 3:31 - loss: 4.7855 - acc: 0.014 - ETA: 3:30 - loss: 4.7857 - acc: 0.014 - ETA: 3:29 - loss: 4.7845 - acc: 0.015 - ETA: 3:27 - loss: 4.7857 - acc: 0.014 - ETA: 3:26 - loss: 4.7866 - acc: 0.015 - ETA: 3:25 - loss: 4.7860 - acc: 0.015 - ETA: 3:24 - loss: 4.7862 - acc: 0.015 - ETA: 3:23 - loss: 4.7862 - acc: 0.015 - ETA: 3:22 - loss: 4.7860 - acc: 0.015 - ETA: 3:21 - loss: 4.7864 - acc: 0.015 - ETA: 3:20 - loss: 4.7857 - acc: 0.014 - ETA: 3:19 - loss: 4.7850 - acc: 0.014 - ETA: 3:18 - loss: 4.7847 - acc: 0.014 - ETA: 3:17 - loss: 4.7844 - acc: 0.014 - ETA: 3:16 - loss: 4.7853 - acc: 0.014 - ETA: 3:15 - loss: 4.7856 - acc: 0.014 - ETA: 3:14 - loss: 4.7855 - acc: 0.014 - ETA: 3:13 - loss: 4.7859 - acc: 0.014 - ETA: 3:12 - loss: 4.7866 - acc: 0.014 - ETA: 3:11 - loss: 4.7883 - acc: 0.014 - ETA: 3:10 - loss: 4.7889 - acc: 0.013 - ETA: 3:09 - loss: 4.7885 - acc: 0.013 - ETA: 3:08 - loss: 4.7880 - acc: 0.013 - ETA: 3:07 - loss: 4.7873 - acc: 0.013 - ETA: 3:06 - loss: 4.7861 - acc: 0.013 - ETA: 3:05 - loss: 4.7851 - acc: 0.013 - ETA: 3:04 - loss: 4.7847 - acc: 0.013 - ETA: 3:02 - loss: 4.7841 - acc: 0.013 - ETA: 3:01 - loss: 4.7840 - acc: 0.013 - ETA: 3:00 - loss: 4.7836 - acc: 0.013 - ETA: 2:59 - loss: 4.7832 - acc: 0.013 - ETA: 2:58 - loss: 4.7826 - acc: 0.013 - ETA: 2:57 - loss: 4.7827 - acc: 0.013 - ETA: 2:56 - loss: 4.7830 - acc: 0.013 - ETA: 2:55 - loss: 4.7832 - acc: 0.013 - ETA: 2:54 - loss: 4.7823 - acc: 0.013 - ETA: 2:53 - loss: 4.7818 - acc: 0.014 - ETA: 2:52 - loss: 4.7815 - acc: 0.014 - ETA: 2:51 - loss: 4.7813 - acc: 0.013 - ETA: 2:50 - loss: 4.7812 - acc: 0.014 - ETA: 2:49 - loss: 4.7804 - acc: 0.014 - ETA: 2:48 - loss: 4.7798 - acc: 0.014 - ETA: 2:47 - loss: 4.7804 - acc: 0.013 - ETA: 2:46 - loss: 4.7803 - acc: 0.013 - ETA: 2:45 - loss: 4.7793 - acc: 0.013 - ETA: 2:44 - loss: 4.7798 - acc: 0.013 - ETA: 2:43 - loss: 4.7795 - acc: 0.013 - ETA: 2:42 - loss: 4.7799 - acc: 0.013 - ETA: 2:41 - loss: 4.7801 - acc: 0.013 - ETA: 2:40 - loss: 4.7809 - acc: 0.013 - ETA: 2:39 - loss: 4.7810 - acc: 0.013 - ETA: 2:37 - loss: 4.7815 - acc: 0.013 - ETA: 2:36 - loss: 4.7810 - acc: 0.013 - ETA: 2:35 - loss: 4.7797 - acc: 0.013 - ETA: 2:34 - loss: 4.7808 - acc: 0.013 - ETA: 2:33 - loss: 4.7809 - acc: 0.013 - ETA: 2:32 - loss: 4.7813 - acc: 0.013 - ETA: 2:31 - loss: 4.7809 - acc: 0.013 - ETA: 2:30 - loss: 4.7810 - acc: 0.013 - ETA: 2:29 - loss: 4.7811 - acc: 0.013 - ETA: 2:28 - loss: 4.7811 - acc: 0.013 - ETA: 2:27 - loss: 4.7814 - acc: 0.013 - ETA: 2:26 - loss: 4.7821 - acc: 0.013 - ETA: 2:25 - loss: 4.7810 - acc: 0.013 - ETA: 2:24 - loss: 4.7817 - acc: 0.013 - ETA: 2:23 - loss: 4.7817 - acc: 0.013 - ETA: 2:22 - loss: 4.7821 - acc: 0.013 - ETA: 2:21 - loss: 4.7811 - acc: 0.013 - ETA: 2:20 - loss: 4.7817 - acc: 0.013 - ETA: 2:19 - loss: 4.7820 - acc: 0.013 - ETA: 2:18 - loss: 4.7822 - acc: 0.013 - ETA: 2:17 - loss: 4.7823 - acc: 0.013 - ETA: 2:16 - loss: 4.7822 - acc: 0.013 - ETA: 2:15 - loss: 4.7821 - acc: 0.0135"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2:13 - loss: 4.7818 - acc: 0.013 - ETA: 2:12 - loss: 4.7824 - acc: 0.013 - ETA: 2:11 - loss: 4.7821 - acc: 0.013 - ETA: 2:10 - loss: 4.7814 - acc: 0.013 - ETA: 2:09 - loss: 4.7813 - acc: 0.013 - ETA: 2:08 - loss: 4.7817 - acc: 0.013 - ETA: 2:07 - loss: 4.7823 - acc: 0.013 - ETA: 2:06 - loss: 4.7818 - acc: 0.013 - ETA: 2:05 - loss: 4.7815 - acc: 0.013 - ETA: 2:04 - loss: 4.7821 - acc: 0.013 - ETA: 2:03 - loss: 4.7816 - acc: 0.013 - ETA: 2:02 - loss: 4.7817 - acc: 0.013 - ETA: 2:01 - loss: 4.7810 - acc: 0.013 - ETA: 2:00 - loss: 4.7811 - acc: 0.013 - ETA: 1:59 - loss: 4.7801 - acc: 0.013 - ETA: 1:58 - loss: 4.7799 - acc: 0.013 - ETA: 1:57 - loss: 4.7803 - acc: 0.013 - ETA: 1:56 - loss: 4.7805 - acc: 0.013 - ETA: 1:55 - loss: 4.7806 - acc: 0.013 - ETA: 1:54 - loss: 4.7809 - acc: 0.013 - ETA: 1:53 - loss: 4.7811 - acc: 0.013 - ETA: 1:52 - loss: 4.7810 - acc: 0.013 - ETA: 1:51 - loss: 4.7812 - acc: 0.013 - ETA: 1:50 - loss: 4.7813 - acc: 0.013 - ETA: 1:49 - loss: 4.7811 - acc: 0.013 - ETA: 1:48 - loss: 4.7816 - acc: 0.013 - ETA: 1:47 - loss: 4.7815 - acc: 0.013 - ETA: 1:46 - loss: 4.7819 - acc: 0.013 - ETA: 1:44 - loss: 4.7818 - acc: 0.013 - ETA: 1:43 - loss: 4.7821 - acc: 0.013 - ETA: 1:42 - loss: 4.7827 - acc: 0.013 - ETA: 1:41 - loss: 4.7828 - acc: 0.013 - ETA: 1:40 - loss: 4.7825 - acc: 0.013 - ETA: 1:39 - loss: 4.7825 - acc: 0.013 - ETA: 1:38 - loss: 4.7820 - acc: 0.013 - ETA: 1:37 - loss: 4.7814 - acc: 0.013 - ETA: 1:36 - loss: 4.7808 - acc: 0.013 - ETA: 1:35 - loss: 4.7811 - acc: 0.013 - ETA: 1:34 - loss: 4.7809 - acc: 0.013 - ETA: 1:33 - loss: 4.7809 - acc: 0.013 - ETA: 1:32 - loss: 4.7803 - acc: 0.013 - ETA: 1:31 - loss: 4.7796 - acc: 0.013 - ETA: 1:30 - loss: 4.7797 - acc: 0.013 - ETA: 1:29 - loss: 4.7799 - acc: 0.013 - ETA: 1:28 - loss: 4.7801 - acc: 0.013 - ETA: 1:27 - loss: 4.7803 - acc: 0.013 - ETA: 1:26 - loss: 4.7804 - acc: 0.013 - ETA: 1:25 - loss: 4.7800 - acc: 0.013 - ETA: 1:24 - loss: 4.7796 - acc: 0.013 - ETA: 1:23 - loss: 4.7797 - acc: 0.013 - ETA: 1:22 - loss: 4.7793 - acc: 0.013 - ETA: 1:21 - loss: 4.7789 - acc: 0.013 - ETA: 1:19 - loss: 4.7787 - acc: 0.013 - ETA: 1:18 - loss: 4.7789 - acc: 0.013 - ETA: 1:17 - loss: 4.7786 - acc: 0.013 - ETA: 1:16 - loss: 4.7791 - acc: 0.013 - ETA: 1:15 - loss: 4.7781 - acc: 0.013 - ETA: 1:14 - loss: 4.7784 - acc: 0.013 - ETA: 1:13 - loss: 4.7791 - acc: 0.013 - ETA: 1:12 - loss: 4.7792 - acc: 0.013 - ETA: 1:11 - loss: 4.7784 - acc: 0.013 - ETA: 1:10 - loss: 4.7784 - acc: 0.013 - ETA: 1:09 - loss: 4.7788 - acc: 0.013 - ETA: 1:08 - loss: 4.7788 - acc: 0.013 - ETA: 1:07 - loss: 4.7791 - acc: 0.013 - ETA: 1:06 - loss: 4.7786 - acc: 0.014 - ETA: 1:05 - loss: 4.7782 - acc: 0.014 - ETA: 1:04 - loss: 4.7790 - acc: 0.014 - ETA: 1:03 - loss: 4.7790 - acc: 0.014 - ETA: 1:02 - loss: 4.7790 - acc: 0.014 - ETA: 1:01 - loss: 4.7790 - acc: 0.014 - ETA: 1:00 - loss: 4.7790 - acc: 0.014 - ETA: 59s - loss: 4.7788 - acc: 0.014 - ETA: 58s - loss: 4.7793 - acc: 0.01 - ETA: 57s - loss: 4.7797 - acc: 0.01 - ETA: 56s - loss: 4.7801 - acc: 0.01 - ETA: 55s - loss: 4.7800 - acc: 0.01 - ETA: 54s - loss: 4.7803 - acc: 0.01 - ETA: 53s - loss: 4.7799 - acc: 0.01 - ETA: 51s - loss: 4.7800 - acc: 0.01 - ETA: 50s - loss: 4.7805 - acc: 0.01 - ETA: 49s - loss: 4.7803 - acc: 0.01 - ETA: 48s - loss: 4.7804 - acc: 0.01 - ETA: 47s - loss: 4.7804 - acc: 0.01 - ETA: 46s - loss: 4.7805 - acc: 0.01 - ETA: 45s - loss: 4.7802 - acc: 0.01 - ETA: 44s - loss: 4.7801 - acc: 0.01 - ETA: 43s - loss: 4.7801 - acc: 0.01 - ETA: 42s - loss: 4.7806 - acc: 0.01 - ETA: 41s - loss: 4.7803 - acc: 0.01 - ETA: 40s - loss: 4.7801 - acc: 0.01 - ETA: 39s - loss: 4.7796 - acc: 0.01 - ETA: 38s - loss: 4.7795 - acc: 0.01 - ETA: 37s - loss: 4.7799 - acc: 0.01 - ETA: 36s - loss: 4.7799 - acc: 0.01 - ETA: 35s - loss: 4.7798 - acc: 0.01 - ETA: 34s - loss: 4.7800 - acc: 0.01 - ETA: 33s - loss: 4.7798 - acc: 0.01 - ETA: 32s - loss: 4.7798 - acc: 0.01 - ETA: 31s - loss: 4.7798 - acc: 0.01 - ETA: 30s - loss: 4.7797 - acc: 0.01 - ETA: 29s - loss: 4.7798 - acc: 0.01 - ETA: 28s - loss: 4.7792 - acc: 0.01 - ETA: 27s - loss: 4.7791 - acc: 0.01 - ETA: 26s - loss: 4.7789 - acc: 0.01 - ETA: 24s - loss: 4.7788 - acc: 0.01 - ETA: 23s - loss: 4.7785 - acc: 0.01 - ETA: 22s - loss: 4.7778 - acc: 0.01 - ETA: 21s - loss: 4.7779 - acc: 0.01 - ETA: 20s - loss: 4.7776 - acc: 0.01 - ETA: 19s - loss: 4.7772 - acc: 0.01 - ETA: 18s - loss: 4.7770 - acc: 0.01 - ETA: 17s - loss: 4.7770 - acc: 0.01 - ETA: 16s - loss: 4.7766 - acc: 0.01 - ETA: 15s - loss: 4.7765 - acc: 0.01 - ETA: 14s - loss: 4.7763 - acc: 0.01 - ETA: 13s - loss: 4.7770 - acc: 0.01 - ETA: 12s - loss: 4.7764 - acc: 0.01 - ETA: 11s - loss: 4.7762 - acc: 0.01 - ETA: 10s - loss: 4.7762 - acc: 0.01 - ETA: 9s - loss: 4.7757 - acc: 0.0157 - ETA: 8s - loss: 4.7750 - acc: 0.016 - ETA: 7s - loss: 4.7753 - acc: 0.016 - ETA: 6s - loss: 4.7756 - acc: 0.016 - ETA: 5s - loss: 4.7759 - acc: 0.016 - ETA: 4s - loss: 4.7758 - acc: 0.016 - ETA: 3s - loss: 4.7758 - acc: 0.016 - ETA: 2s - loss: 4.7761 - acc: 0.016 - ETA: 1s - loss: 4.7764 - acc: 0.016 - 365s 55ms/step - loss: 4.7766 - acc: 0.0160 - val_loss: 4.7802 - val_acc: 0.0228\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.81646 to 4.78016, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 5/10\n",
      "4080/6680 [=================>............] - ETA: 5:47 - loss: 4.8585 - acc: 0.0000e+0 - ETA: 5:40 - loss: 4.7827 - acc: 0.0000e+0 - ETA: 5:38 - loss: 4.7848 - acc: 0.0167    - ETA: 5:37 - loss: 4.8061 - acc: 0.025 - ETA: 5:36 - loss: 4.8117 - acc: 0.020 - ETA: 5:35 - loss: 4.8005 - acc: 0.025 - ETA: 5:33 - loss: 4.7925 - acc: 0.021 - ETA: 5:33 - loss: 4.7565 - acc: 0.025 - ETA: 5:31 - loss: 4.7554 - acc: 0.027 - ETA: 5:30 - loss: 4.7509 - acc: 0.025 - ETA: 5:29 - loss: 4.7663 - acc: 0.027 - ETA: 5:28 - loss: 4.7574 - acc: 0.025 - ETA: 5:27 - loss: 4.7443 - acc: 0.026 - ETA: 5:26 - loss: 4.7529 - acc: 0.028 - ETA: 5:26 - loss: 4.7585 - acc: 0.026 - ETA: 5:25 - loss: 4.7496 - acc: 0.028 - ETA: 5:24 - loss: 4.7442 - acc: 0.029 - ETA: 5:23 - loss: 4.7377 - acc: 0.030 - ETA: 5:23 - loss: 4.7391 - acc: 0.031 - ETA: 5:22 - loss: 4.7409 - acc: 0.032 - ETA: 5:21 - loss: 4.7455 - acc: 0.033 - ETA: 5:21 - loss: 4.7458 - acc: 0.031 - ETA: 5:22 - loss: 4.7460 - acc: 0.032 - ETA: 5:20 - loss: 4.7507 - acc: 0.031 - ETA: 5:19 - loss: 4.7455 - acc: 0.032 - ETA: 5:18 - loss: 4.7450 - acc: 0.030 - ETA: 5:17 - loss: 4.7478 - acc: 0.029 - ETA: 5:15 - loss: 4.7472 - acc: 0.028 - ETA: 5:14 - loss: 4.7481 - acc: 0.027 - ETA: 5:13 - loss: 4.7480 - acc: 0.028 - ETA: 5:12 - loss: 4.7491 - acc: 0.027 - ETA: 5:11 - loss: 4.7499 - acc: 0.026 - ETA: 5:10 - loss: 4.7481 - acc: 0.025 - ETA: 5:10 - loss: 4.7436 - acc: 0.025 - ETA: 5:10 - loss: 4.7413 - acc: 0.024 - ETA: 5:09 - loss: 4.7381 - acc: 0.023 - ETA: 5:09 - loss: 4.7419 - acc: 0.023 - ETA: 5:08 - loss: 4.7426 - acc: 0.022 - ETA: 5:07 - loss: 4.7422 - acc: 0.021 - ETA: 5:05 - loss: 4.7439 - acc: 0.021 - ETA: 5:04 - loss: 4.7416 - acc: 0.020 - ETA: 5:03 - loss: 4.7377 - acc: 0.020 - ETA: 5:02 - loss: 4.7371 - acc: 0.022 - ETA: 5:01 - loss: 4.7371 - acc: 0.021 - ETA: 5:00 - loss: 4.7363 - acc: 0.021 - ETA: 4:59 - loss: 4.7397 - acc: 0.020 - ETA: 4:58 - loss: 4.7375 - acc: 0.020 - ETA: 4:57 - loss: 4.7353 - acc: 0.019 - ETA: 4:56 - loss: 4.7356 - acc: 0.019 - ETA: 4:55 - loss: 4.7334 - acc: 0.020 - ETA: 4:53 - loss: 4.7337 - acc: 0.019 - ETA: 4:52 - loss: 4.7318 - acc: 0.020 - ETA: 4:51 - loss: 4.7298 - acc: 0.021 - ETA: 4:50 - loss: 4.7323 - acc: 0.022 - ETA: 4:49 - loss: 4.7323 - acc: 0.021 - ETA: 4:48 - loss: 4.7326 - acc: 0.022 - ETA: 4:47 - loss: 4.7318 - acc: 0.022 - ETA: 4:46 - loss: 4.7360 - acc: 0.022 - ETA: 4:45 - loss: 4.7377 - acc: 0.022 - ETA: 4:44 - loss: 4.7377 - acc: 0.021 - ETA: 4:43 - loss: 4.7382 - acc: 0.021 - ETA: 4:41 - loss: 4.7378 - acc: 0.021 - ETA: 4:41 - loss: 4.7395 - acc: 0.021 - ETA: 4:40 - loss: 4.7378 - acc: 0.021 - ETA: 4:38 - loss: 4.7368 - acc: 0.020 - ETA: 4:38 - loss: 4.7367 - acc: 0.020 - ETA: 4:37 - loss: 4.7366 - acc: 0.021 - ETA: 4:35 - loss: 4.7344 - acc: 0.022 - ETA: 4:34 - loss: 4.7359 - acc: 0.023 - ETA: 4:33 - loss: 4.7368 - acc: 0.023 - ETA: 4:32 - loss: 4.7365 - acc: 0.023 - ETA: 4:31 - loss: 4.7369 - acc: 0.024 - ETA: 4:30 - loss: 4.7355 - acc: 0.024 - ETA: 4:29 - loss: 4.7363 - acc: 0.025 - ETA: 4:28 - loss: 4.7344 - acc: 0.024 - ETA: 4:27 - loss: 4.7343 - acc: 0.025 - ETA: 4:26 - loss: 4.7339 - acc: 0.024 - ETA: 4:25 - loss: 4.7351 - acc: 0.024 - ETA: 4:23 - loss: 4.7334 - acc: 0.024 - ETA: 4:22 - loss: 4.7333 - acc: 0.024 - ETA: 4:21 - loss: 4.7341 - acc: 0.024 - ETA: 4:20 - loss: 4.7367 - acc: 0.023 - ETA: 4:19 - loss: 4.7359 - acc: 0.024 - ETA: 4:18 - loss: 4.7350 - acc: 0.024 - ETA: 4:17 - loss: 4.7340 - acc: 0.024 - ETA: 4:16 - loss: 4.7334 - acc: 0.023 - ETA: 4:15 - loss: 4.7330 - acc: 0.023 - ETA: 4:14 - loss: 4.7326 - acc: 0.023 - ETA: 4:13 - loss: 4.7339 - acc: 0.023 - ETA: 4:12 - loss: 4.7335 - acc: 0.023 - ETA: 4:11 - loss: 4.7319 - acc: 0.023 - ETA: 4:10 - loss: 4.7305 - acc: 0.023 - ETA: 4:09 - loss: 4.7296 - acc: 0.023 - ETA: 4:08 - loss: 4.7293 - acc: 0.023 - ETA: 4:07 - loss: 4.7283 - acc: 0.023 - ETA: 4:05 - loss: 4.7280 - acc: 0.022 - ETA: 4:04 - loss: 4.7296 - acc: 0.022 - ETA: 4:03 - loss: 4.7306 - acc: 0.023 - ETA: 4:02 - loss: 4.7298 - acc: 0.022 - ETA: 4:01 - loss: 4.7298 - acc: 0.022 - ETA: 4:00 - loss: 4.7309 - acc: 0.022 - ETA: 3:59 - loss: 4.7306 - acc: 0.022 - ETA: 3:58 - loss: 4.7307 - acc: 0.021 - ETA: 3:57 - loss: 4.7307 - acc: 0.021 - ETA: 3:56 - loss: 4.7319 - acc: 0.021 - ETA: 3:55 - loss: 4.7305 - acc: 0.021 - ETA: 3:54 - loss: 4.7303 - acc: 0.021 - ETA: 3:53 - loss: 4.7319 - acc: 0.020 - ETA: 3:52 - loss: 4.7326 - acc: 0.020 - ETA: 3:51 - loss: 4.7339 - acc: 0.020 - ETA: 3:50 - loss: 4.7351 - acc: 0.020 - ETA: 3:49 - loss: 4.7349 - acc: 0.021 - ETA: 3:48 - loss: 4.7345 - acc: 0.020 - ETA: 3:47 - loss: 4.7337 - acc: 0.020 - ETA: 3:46 - loss: 4.7327 - acc: 0.020 - ETA: 3:45 - loss: 4.7335 - acc: 0.021 - ETA: 3:43 - loss: 4.7340 - acc: 0.021 - ETA: 3:42 - loss: 4.7340 - acc: 0.022 - ETA: 3:41 - loss: 4.7345 - acc: 0.021 - ETA: 3:40 - loss: 4.7346 - acc: 0.021 - ETA: 3:39 - loss: 4.7340 - acc: 0.021 - ETA: 3:38 - loss: 4.7339 - acc: 0.021 - ETA: 3:37 - loss: 4.7342 - acc: 0.021 - ETA: 3:36 - loss: 4.7344 - acc: 0.021 - ETA: 3:35 - loss: 4.7350 - acc: 0.021 - ETA: 3:34 - loss: 4.7349 - acc: 0.021 - ETA: 3:33 - loss: 4.7341 - acc: 0.021 - ETA: 3:32 - loss: 4.7359 - acc: 0.021 - ETA: 3:31 - loss: 4.7367 - acc: 0.021 - ETA: 3:30 - loss: 4.7360 - acc: 0.021 - ETA: 3:29 - loss: 4.7355 - acc: 0.021 - ETA: 3:28 - loss: 4.7369 - acc: 0.020 - ETA: 3:27 - loss: 4.7375 - acc: 0.021 - ETA: 3:26 - loss: 4.7383 - acc: 0.020 - ETA: 3:25 - loss: 4.7392 - acc: 0.021 - ETA: 3:24 - loss: 4.7389 - acc: 0.021 - ETA: 3:23 - loss: 4.7391 - acc: 0.021 - ETA: 3:22 - loss: 4.7397 - acc: 0.021 - ETA: 3:21 - loss: 4.7387 - acc: 0.021 - ETA: 3:19 - loss: 4.7384 - acc: 0.022 - ETA: 3:18 - loss: 4.7389 - acc: 0.022 - ETA: 3:17 - loss: 4.7388 - acc: 0.022 - ETA: 3:16 - loss: 4.7389 - acc: 0.022 - ETA: 3:15 - loss: 4.7390 - acc: 0.022 - ETA: 3:14 - loss: 4.7374 - acc: 0.022 - ETA: 3:13 - loss: 4.7374 - acc: 0.022 - ETA: 3:12 - loss: 4.7352 - acc: 0.022 - ETA: 3:11 - loss: 4.7353 - acc: 0.022 - ETA: 3:10 - loss: 4.7353 - acc: 0.022 - ETA: 3:09 - loss: 4.7346 - acc: 0.022 - ETA: 3:08 - loss: 4.7350 - acc: 0.021 - ETA: 3:07 - loss: 4.7344 - acc: 0.021 - ETA: 3:06 - loss: 4.7346 - acc: 0.021 - ETA: 3:05 - loss: 4.7354 - acc: 0.021 - ETA: 3:04 - loss: 4.7352 - acc: 0.021 - ETA: 3:03 - loss: 4.7365 - acc: 0.021 - ETA: 3:02 - loss: 4.7357 - acc: 0.021 - ETA: 3:01 - loss: 4.7360 - acc: 0.020 - ETA: 3:00 - loss: 4.7369 - acc: 0.020 - ETA: 2:59 - loss: 4.7366 - acc: 0.020 - ETA: 2:58 - loss: 4.7368 - acc: 0.021 - ETA: 2:57 - loss: 4.7369 - acc: 0.021 - ETA: 2:56 - loss: 4.7373 - acc: 0.020 - ETA: 2:55 - loss: 4.7387 - acc: 0.020 - ETA: 2:54 - loss: 4.7385 - acc: 0.020 - ETA: 2:53 - loss: 4.7374 - acc: 0.020 - ETA: 2:52 - loss: 4.7376 - acc: 0.020 - ETA: 2:51 - loss: 4.7371 - acc: 0.020 - ETA: 2:49 - loss: 4.7366 - acc: 0.020 - ETA: 2:48 - loss: 4.7373 - acc: 0.020 - ETA: 2:47 - loss: 4.7366 - acc: 0.021 - ETA: 2:46 - loss: 4.7370 - acc: 0.021 - ETA: 2:45 - loss: 4.7372 - acc: 0.021 - ETA: 2:44 - loss: 4.7367 - acc: 0.021 - ETA: 2:43 - loss: 4.7368 - acc: 0.021 - ETA: 2:42 - loss: 4.7353 - acc: 0.021 - ETA: 2:41 - loss: 4.7352 - acc: 0.021 - ETA: 2:40 - loss: 4.7355 - acc: 0.021 - ETA: 2:39 - loss: 4.7348 - acc: 0.021 - ETA: 2:38 - loss: 4.7346 - acc: 0.021 - ETA: 2:37 - loss: 4.7338 - acc: 0.021 - ETA: 2:36 - loss: 4.7353 - acc: 0.020 - ETA: 2:35 - loss: 4.7344 - acc: 0.021 - ETA: 2:34 - loss: 4.7337 - acc: 0.021 - ETA: 2:33 - loss: 4.7344 - acc: 0.021 - ETA: 2:32 - loss: 4.7336 - acc: 0.021 - ETA: 2:31 - loss: 4.7338 - acc: 0.021 - ETA: 2:30 - loss: 4.7327 - acc: 0.021 - ETA: 2:29 - loss: 4.7326 - acc: 0.021 - ETA: 2:28 - loss: 4.7318 - acc: 0.022 - ETA: 2:27 - loss: 4.7312 - acc: 0.022 - ETA: 2:26 - loss: 4.7307 - acc: 0.022 - ETA: 2:25 - loss: 4.7324 - acc: 0.022 - ETA: 2:24 - loss: 4.7326 - acc: 0.022 - ETA: 2:23 - loss: 4.7316 - acc: 0.022 - ETA: 2:22 - loss: 4.7314 - acc: 0.022 - ETA: 2:21 - loss: 4.7329 - acc: 0.022 - ETA: 2:20 - loss: 4.7328 - acc: 0.022 - ETA: 2:19 - loss: 4.7323 - acc: 0.022 - ETA: 2:17 - loss: 4.7326 - acc: 0.022 - ETA: 2:16 - loss: 4.7332 - acc: 0.022 - ETA: 2:15 - loss: 4.7328 - acc: 0.023 - ETA: 2:14 - loss: 4.7327 - acc: 0.022 - ETA: 2:13 - loss: 4.7327 - acc: 0.02286680/6680 [==============================] - ETA: 2:12 - loss: 4.7326 - acc: 0.022 - ETA: 2:11 - loss: 4.7328 - acc: 0.022 - ETA: 2:10 - loss: 4.7324 - acc: 0.022 - ETA: 2:09 - loss: 4.7326 - acc: 0.022 - ETA: 2:08 - loss: 4.7328 - acc: 0.022 - ETA: 2:07 - loss: 4.7327 - acc: 0.022 - ETA: 2:06 - loss: 4.7319 - acc: 0.022 - ETA: 2:05 - loss: 4.7316 - acc: 0.022 - ETA: 2:04 - loss: 4.7314 - acc: 0.022 - ETA: 2:03 - loss: 4.7315 - acc: 0.022 - ETA: 2:02 - loss: 4.7313 - acc: 0.022 - ETA: 2:01 - loss: 4.7306 - acc: 0.022 - ETA: 2:00 - loss: 4.7313 - acc: 0.022 - ETA: 1:59 - loss: 4.7312 - acc: 0.022 - ETA: 1:58 - loss: 4.7323 - acc: 0.022 - ETA: 1:57 - loss: 4.7316 - acc: 0.022 - ETA: 1:56 - loss: 4.7322 - acc: 0.022 - ETA: 1:55 - loss: 4.7328 - acc: 0.022 - ETA: 1:54 - loss: 4.7329 - acc: 0.022 - ETA: 1:53 - loss: 4.7335 - acc: 0.022 - ETA: 1:52 - loss: 4.7336 - acc: 0.022 - ETA: 1:51 - loss: 4.7335 - acc: 0.022 - ETA: 1:50 - loss: 4.7334 - acc: 0.022 - ETA: 1:49 - loss: 4.7333 - acc: 0.021 - ETA: 1:48 - loss: 4.7336 - acc: 0.022 - ETA: 1:47 - loss: 4.7332 - acc: 0.022 - ETA: 1:46 - loss: 4.7332 - acc: 0.022 - ETA: 1:45 - loss: 4.7328 - acc: 0.022 - ETA: 1:44 - loss: 4.7328 - acc: 0.022 - ETA: 1:43 - loss: 4.7322 - acc: 0.022 - ETA: 1:42 - loss: 4.7330 - acc: 0.022 - ETA: 1:41 - loss: 4.7330 - acc: 0.022 - ETA: 1:40 - loss: 4.7341 - acc: 0.022 - ETA: 1:39 - loss: 4.7344 - acc: 0.022 - ETA: 1:38 - loss: 4.7340 - acc: 0.022 - ETA: 1:37 - loss: 4.7336 - acc: 0.022 - ETA: 1:36 - loss: 4.7335 - acc: 0.022 - ETA: 1:35 - loss: 4.7331 - acc: 0.021 - ETA: 1:34 - loss: 4.7331 - acc: 0.021 - ETA: 1:33 - loss: 4.7344 - acc: 0.021 - ETA: 1:31 - loss: 4.7352 - acc: 0.021 - ETA: 1:30 - loss: 4.7338 - acc: 0.021 - ETA: 1:29 - loss: 4.7345 - acc: 0.021 - ETA: 1:28 - loss: 4.7340 - acc: 0.021 - ETA: 1:27 - loss: 4.7341 - acc: 0.021 - ETA: 1:26 - loss: 4.7343 - acc: 0.021 - ETA: 1:25 - loss: 4.7336 - acc: 0.021 - ETA: 1:24 - loss: 4.7344 - acc: 0.021 - ETA: 1:23 - loss: 4.7350 - acc: 0.021 - ETA: 1:22 - loss: 4.7349 - acc: 0.021 - ETA: 1:21 - loss: 4.7349 - acc: 0.021 - ETA: 1:20 - loss: 4.7354 - acc: 0.021 - ETA: 1:19 - loss: 4.7360 - acc: 0.021 - ETA: 1:18 - loss: 4.7358 - acc: 0.021 - ETA: 1:17 - loss: 4.7356 - acc: 0.021 - ETA: 1:16 - loss: 4.7359 - acc: 0.021 - ETA: 1:15 - loss: 4.7363 - acc: 0.021 - ETA: 1:14 - loss: 4.7361 - acc: 0.021 - ETA: 1:13 - loss: 4.7363 - acc: 0.021 - ETA: 1:12 - loss: 4.7363 - acc: 0.021 - ETA: 1:11 - loss: 4.7359 - acc: 0.021 - ETA: 1:10 - loss: 4.7349 - acc: 0.021 - ETA: 1:09 - loss: 4.7342 - acc: 0.021 - ETA: 1:08 - loss: 4.7337 - acc: 0.021 - ETA: 1:07 - loss: 4.7336 - acc: 0.021 - ETA: 1:06 - loss: 4.7340 - acc: 0.021 - ETA: 1:05 - loss: 4.7342 - acc: 0.021 - ETA: 1:04 - loss: 4.7343 - acc: 0.021 - ETA: 1:03 - loss: 4.7342 - acc: 0.021 - ETA: 1:02 - loss: 4.7346 - acc: 0.021 - ETA: 1:01 - loss: 4.7343 - acc: 0.021 - ETA: 1:00 - loss: 4.7345 - acc: 0.021 - ETA: 59s - loss: 4.7339 - acc: 0.021 - ETA: 58s - loss: 4.7337 - acc: 0.02 - ETA: 57s - loss: 4.7334 - acc: 0.02 - ETA: 56s - loss: 4.7339 - acc: 0.02 - ETA: 55s - loss: 4.7338 - acc: 0.02 - ETA: 54s - loss: 4.7337 - acc: 0.02 - ETA: 53s - loss: 4.7339 - acc: 0.02 - ETA: 52s - loss: 4.7336 - acc: 0.02 - ETA: 51s - loss: 4.7339 - acc: 0.02 - ETA: 50s - loss: 4.7340 - acc: 0.02 - ETA: 49s - loss: 4.7340 - acc: 0.02 - ETA: 48s - loss: 4.7337 - acc: 0.02 - ETA: 47s - loss: 4.7334 - acc: 0.02 - ETA: 45s - loss: 4.7337 - acc: 0.02 - ETA: 44s - loss: 4.7343 - acc: 0.02 - ETA: 43s - loss: 4.7341 - acc: 0.02 - ETA: 42s - loss: 4.7338 - acc: 0.02 - ETA: 41s - loss: 4.7337 - acc: 0.02 - ETA: 40s - loss: 4.7340 - acc: 0.02 - ETA: 39s - loss: 4.7339 - acc: 0.02 - ETA: 38s - loss: 4.7343 - acc: 0.02 - ETA: 37s - loss: 4.7344 - acc: 0.02 - ETA: 36s - loss: 4.7338 - acc: 0.02 - ETA: 35s - loss: 4.7337 - acc: 0.02 - ETA: 34s - loss: 4.7335 - acc: 0.02 - ETA: 33s - loss: 4.7338 - acc: 0.02 - ETA: 32s - loss: 4.7338 - acc: 0.02 - ETA: 31s - loss: 4.7335 - acc: 0.02 - ETA: 30s - loss: 4.7332 - acc: 0.02 - ETA: 29s - loss: 4.7325 - acc: 0.02 - ETA: 28s - loss: 4.7322 - acc: 0.02 - ETA: 27s - loss: 4.7325 - acc: 0.02 - ETA: 26s - loss: 4.7325 - acc: 0.02 - ETA: 25s - loss: 4.7323 - acc: 0.02 - ETA: 24s - loss: 4.7319 - acc: 0.02 - ETA: 23s - loss: 4.7320 - acc: 0.02 - ETA: 22s - loss: 4.7326 - acc: 0.02 - ETA: 21s - loss: 4.7325 - acc: 0.02 - ETA: 20s - loss: 4.7323 - acc: 0.02 - ETA: 19s - loss: 4.7321 - acc: 0.02 - ETA: 18s - loss: 4.7326 - acc: 0.02 - ETA: 16s - loss: 4.7337 - acc: 0.02 - ETA: 15s - loss: 4.7340 - acc: 0.02 - ETA: 14s - loss: 4.7333 - acc: 0.02 - ETA: 13s - loss: 4.7328 - acc: 0.02 - ETA: 12s - loss: 4.7325 - acc: 0.02 - ETA: 11s - loss: 4.7331 - acc: 0.02 - ETA: 10s - loss: 4.7332 - acc: 0.02 - ETA: 9s - loss: 4.7338 - acc: 0.0217 - ETA: 8s - loss: 4.7334 - acc: 0.021 - ETA: 7s - loss: 4.7335 - acc: 0.021 - ETA: 6s - loss: 4.7335 - acc: 0.021 - ETA: 5s - loss: 4.7335 - acc: 0.021 - ETA: 4s - loss: 4.7332 - acc: 0.021 - ETA: 3s - loss: 4.7338 - acc: 0.021 - ETA: 2s - loss: 4.7332 - acc: 0.021 - ETA: 1s - loss: 4.7328 - acc: 0.021 - 378s 57ms/step - loss: 4.7326 - acc: 0.0217 - val_loss: 4.7536 - val_acc: 0.0216\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.78016 to 4.75362, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 6:30 - loss: 4.5774 - acc: 0.0000e+0 - ETA: 6:22 - loss: 4.6356 - acc: 0.0000e+0 - ETA: 6:25 - loss: 4.6171 - acc: 0.0167    - ETA: 6:22 - loss: 4.6293 - acc: 0.025 - ETA: 6:22 - loss: 4.6599 - acc: 0.030 - ETA: 6:21 - loss: 4.6669 - acc: 0.025 - ETA: 6:20 - loss: 4.6522 - acc: 0.021 - ETA: 6:15 - loss: 4.6718 - acc: 0.018 - ETA: 6:15 - loss: 4.6777 - acc: 0.016 - ETA: 6:14 - loss: 4.6773 - acc: 0.025 - ETA: 6:14 - loss: 4.6862 - acc: 0.022 - ETA: 6:14 - loss: 4.7192 - acc: 0.020 - ETA: 6:15 - loss: 4.7165 - acc: 0.019 - ETA: 6:14 - loss: 4.7180 - acc: 0.021 - ETA: 6:13 - loss: 4.7224 - acc: 0.020 - ETA: 6:12 - loss: 4.7120 - acc: 0.021 - ETA: 6:12 - loss: 4.7349 - acc: 0.020 - ETA: 6:11 - loss: 4.7366 - acc: 0.019 - ETA: 6:10 - loss: 4.7285 - acc: 0.018 - ETA: 6:09 - loss: 4.7286 - acc: 0.017 - ETA: 6:09 - loss: 4.7240 - acc: 0.021 - ETA: 6:08 - loss: 4.7224 - acc: 0.020 - ETA: 6:05 - loss: 4.7160 - acc: 0.023 - ETA: 6:02 - loss: 4.7182 - acc: 0.025 - ETA: 5:59 - loss: 4.7152 - acc: 0.024 - ETA: 5:59 - loss: 4.7135 - acc: 0.023 - ETA: 5:58 - loss: 4.7125 - acc: 0.022 - ETA: 5:57 - loss: 4.7168 - acc: 0.021 - ETA: 5:57 - loss: 4.7105 - acc: 0.022 - ETA: 5:56 - loss: 4.7079 - acc: 0.023 - ETA: 5:55 - loss: 4.7109 - acc: 0.022 - ETA: 5:53 - loss: 4.7060 - acc: 0.023 - ETA: 5:50 - loss: 4.7093 - acc: 0.022 - ETA: 5:49 - loss: 4.7064 - acc: 0.023 - ETA: 5:48 - loss: 4.7091 - acc: 0.022 - ETA: 5:46 - loss: 4.7070 - acc: 0.022 - ETA: 5:44 - loss: 4.7079 - acc: 0.023 - ETA: 5:43 - loss: 4.7123 - acc: 0.022 - ETA: 5:41 - loss: 4.7127 - acc: 0.021 - ETA: 5:39 - loss: 4.7074 - acc: 0.022 - ETA: 5:37 - loss: 4.7108 - acc: 0.022 - ETA: 5:35 - loss: 4.7071 - acc: 0.026 - ETA: 5:33 - loss: 4.7042 - acc: 0.025 - ETA: 5:32 - loss: 4.7045 - acc: 0.027 - ETA: 5:30 - loss: 4.7033 - acc: 0.026 - ETA: 5:28 - loss: 4.6983 - acc: 0.028 - ETA: 5:26 - loss: 4.6986 - acc: 0.027 - ETA: 5:24 - loss: 4.7008 - acc: 0.027 - ETA: 5:23 - loss: 4.7030 - acc: 0.026 - ETA: 5:21 - loss: 4.7037 - acc: 0.026 - ETA: 5:19 - loss: 4.7015 - acc: 0.027 - ETA: 5:18 - loss: 4.6985 - acc: 0.027 - ETA: 5:16 - loss: 4.6999 - acc: 0.028 - ETA: 5:14 - loss: 4.6990 - acc: 0.028 - ETA: 5:14 - loss: 4.6989 - acc: 0.029 - ETA: 5:13 - loss: 4.6960 - acc: 0.029 - ETA: 5:12 - loss: 4.6971 - acc: 0.028 - ETA: 5:11 - loss: 4.6980 - acc: 0.028 - ETA: 5:10 - loss: 4.6976 - acc: 0.028 - ETA: 5:09 - loss: 4.6967 - acc: 0.027 - ETA: 5:08 - loss: 4.6953 - acc: 0.027 - ETA: 5:08 - loss: 4.6934 - acc: 0.027 - ETA: 5:07 - loss: 4.6913 - acc: 0.027 - ETA: 5:06 - loss: 4.6916 - acc: 0.027 - ETA: 5:05 - loss: 4.6912 - acc: 0.026 - ETA: 5:04 - loss: 4.6885 - acc: 0.026 - ETA: 5:04 - loss: 4.6911 - acc: 0.026 - ETA: 5:03 - loss: 4.6911 - acc: 0.025 - ETA: 5:02 - loss: 4.6919 - acc: 0.025 - ETA: 5:01 - loss: 4.6919 - acc: 0.025 - ETA: 5:00 - loss: 4.6876 - acc: 0.025 - ETA: 4:59 - loss: 4.6871 - acc: 0.025 - ETA: 4:57 - loss: 4.6893 - acc: 0.024 - ETA: 4:56 - loss: 4.6896 - acc: 0.024 - ETA: 4:54 - loss: 4.6863 - acc: 0.024 - ETA: 4:53 - loss: 4.6863 - acc: 0.024 - ETA: 4:51 - loss: 4.6884 - acc: 0.024 - ETA: 4:50 - loss: 4.6868 - acc: 0.023 - ETA: 4:48 - loss: 4.6870 - acc: 0.023 - ETA: 4:47 - loss: 4.6874 - acc: 0.023 - ETA: 4:46 - loss: 4.6885 - acc: 0.022 - ETA: 4:44 - loss: 4.6906 - acc: 0.022 - ETA: 4:43 - loss: 4.6893 - acc: 0.022 - ETA: 4:41 - loss: 4.6888 - acc: 0.023 - ETA: 4:40 - loss: 4.6895 - acc: 0.022 - ETA: 4:39 - loss: 4.6898 - acc: 0.023 - ETA: 4:38 - loss: 4.6911 - acc: 0.023 - ETA: 4:37 - loss: 4.6912 - acc: 0.022 - ETA: 4:36 - loss: 4.6913 - acc: 0.023 - ETA: 4:35 - loss: 4.6882 - acc: 0.023 - ETA: 4:34 - loss: 4.6907 - acc: 0.023 - ETA: 4:33 - loss: 4.6905 - acc: 0.023 - ETA: 4:32 - loss: 4.6897 - acc: 0.024 - ETA: 4:31 - loss: 4.6870 - acc: 0.023 - ETA: 4:30 - loss: 4.6861 - acc: 0.023 - ETA: 4:29 - loss: 4.6843 - acc: 0.024 - ETA: 4:28 - loss: 4.6837 - acc: 0.024 - ETA: 4:27 - loss: 4.6845 - acc: 0.024 - ETA: 4:26 - loss: 4.6882 - acc: 0.024 - ETA: 4:26 - loss: 4.6902 - acc: 0.024 - ETA: 4:25 - loss: 4.6923 - acc: 0.024 - ETA: 4:24 - loss: 4.6934 - acc: 0.024 - ETA: 4:22 - loss: 4.6923 - acc: 0.023 - ETA: 4:21 - loss: 4.6932 - acc: 0.023 - ETA: 4:20 - loss: 4.6934 - acc: 0.023 - ETA: 4:19 - loss: 4.6920 - acc: 0.023 - ETA: 4:18 - loss: 4.6917 - acc: 0.022 - ETA: 4:17 - loss: 4.6930 - acc: 0.023 - ETA: 4:16 - loss: 4.6923 - acc: 0.023 - ETA: 4:15 - loss: 4.6927 - acc: 0.023 - ETA: 4:14 - loss: 4.6923 - acc: 0.024 - ETA: 4:13 - loss: 4.6926 - acc: 0.024 - ETA: 4:12 - loss: 4.6919 - acc: 0.024 - ETA: 4:11 - loss: 4.6910 - acc: 0.024 - ETA: 4:10 - loss: 4.6914 - acc: 0.024 - ETA: 4:09 - loss: 4.6900 - acc: 0.024 - ETA: 4:08 - loss: 4.6915 - acc: 0.024 - ETA: 4:06 - loss: 4.6925 - acc: 0.025 - ETA: 4:05 - loss: 4.6920 - acc: 0.024 - ETA: 4:04 - loss: 4.6909 - acc: 0.025 - ETA: 4:03 - loss: 4.6928 - acc: 0.024 - ETA: 4:01 - loss: 4.6933 - acc: 0.024 - ETA: 4:00 - loss: 4.6922 - acc: 0.024 - ETA: 3:59 - loss: 4.6920 - acc: 0.025 - ETA: 3:57 - loss: 4.6911 - acc: 0.025 - ETA: 3:56 - loss: 4.6908 - acc: 0.025 - ETA: 3:55 - loss: 4.6919 - acc: 0.025 - ETA: 3:54 - loss: 4.6912 - acc: 0.025 - ETA: 3:53 - loss: 4.6912 - acc: 0.025 - ETA: 3:52 - loss: 4.6912 - acc: 0.025 - ETA: 3:51 - loss: 4.6920 - acc: 0.024 - ETA: 3:50 - loss: 4.6931 - acc: 0.024 - ETA: 3:49 - loss: 4.6920 - acc: 0.024 - ETA: 3:48 - loss: 4.6936 - acc: 0.024 - ETA: 3:47 - loss: 4.6937 - acc: 0.024 - ETA: 3:46 - loss: 4.6932 - acc: 0.023 - ETA: 3:45 - loss: 4.6927 - acc: 0.023 - ETA: 3:44 - loss: 4.6922 - acc: 0.023 - ETA: 3:43 - loss: 4.6909 - acc: 0.023 - ETA: 3:42 - loss: 4.6902 - acc: 0.023 - ETA: 3:41 - loss: 4.6924 - acc: 0.023 - ETA: 3:40 - loss: 4.6926 - acc: 0.023 - ETA: 3:39 - loss: 4.6926 - acc: 0.023 - ETA: 3:38 - loss: 4.6906 - acc: 0.023 - ETA: 3:36 - loss: 4.6875 - acc: 0.024 - ETA: 3:35 - loss: 4.6911 - acc: 0.024 - ETA: 3:34 - loss: 4.6913 - acc: 0.024 - ETA: 3:33 - loss: 4.6929 - acc: 0.024 - ETA: 3:32 - loss: 4.6940 - acc: 0.023 - ETA: 3:31 - loss: 4.6931 - acc: 0.023 - ETA: 3:30 - loss: 4.6928 - acc: 0.023 - ETA: 3:29 - loss: 4.6915 - acc: 0.024 - ETA: 3:28 - loss: 4.6915 - acc: 0.024 - ETA: 3:26 - loss: 4.6914 - acc: 0.024 - ETA: 3:25 - loss: 4.6907 - acc: 0.024 - ETA: 3:24 - loss: 4.6892 - acc: 0.024 - ETA: 3:23 - loss: 4.6886 - acc: 0.024 - ETA: 3:22 - loss: 4.6877 - acc: 0.024 - ETA: 3:21 - loss: 4.6892 - acc: 0.024 - ETA: 3:20 - loss: 4.6887 - acc: 0.024 - ETA: 3:19 - loss: 4.6893 - acc: 0.024 - ETA: 3:18 - loss: 4.6907 - acc: 0.024 - ETA: 3:17 - loss: 4.6913 - acc: 0.024 - ETA: 3:16 - loss: 4.6929 - acc: 0.024 - ETA: 3:14 - loss: 4.6917 - acc: 0.024 - ETA: 3:13 - loss: 4.6918 - acc: 0.025 - ETA: 3:12 - loss: 4.6918 - acc: 0.024 - ETA: 3:11 - loss: 4.6921 - acc: 0.025 - ETA: 3:10 - loss: 4.6907 - acc: 0.025 - ETA: 3:09 - loss: 4.6902 - acc: 0.025 - ETA: 3:08 - loss: 4.6903 - acc: 0.025 - ETA: 3:07 - loss: 4.6904 - acc: 0.025 - ETA: 3:06 - loss: 4.6910 - acc: 0.025 - ETA: 3:04 - loss: 4.6922 - acc: 0.025 - ETA: 3:03 - loss: 4.6918 - acc: 0.024 - ETA: 3:02 - loss: 4.6926 - acc: 0.024 - ETA: 3:01 - loss: 4.6930 - acc: 0.024 - ETA: 3:00 - loss: 4.6927 - acc: 0.024 - ETA: 3:00 - loss: 4.6927 - acc: 0.024 - ETA: 2:58 - loss: 4.6909 - acc: 0.024 - ETA: 2:57 - loss: 4.6900 - acc: 0.024 - ETA: 2:56 - loss: 4.6899 - acc: 0.025 - ETA: 2:54 - loss: 4.6912 - acc: 0.024 - ETA: 2:53 - loss: 4.6910 - acc: 0.025 - ETA: 2:52 - loss: 4.6895 - acc: 0.025 - ETA: 2:51 - loss: 4.6894 - acc: 0.025 - ETA: 2:50 - loss: 4.6884 - acc: 0.025 - ETA: 2:48 - loss: 4.6877 - acc: 0.025 - ETA: 2:49 - loss: 4.6874 - acc: 0.025 - ETA: 2:48 - loss: 4.6878 - acc: 0.025 - ETA: 2:47 - loss: 4.6881 - acc: 0.025 - ETA: 2:46 - loss: 4.6889 - acc: 0.025 - ETA: 2:45 - loss: 4.6899 - acc: 0.025 - ETA: 2:44 - loss: 4.6908 - acc: 0.025 - ETA: 2:42 - loss: 4.6912 - acc: 0.024 - ETA: 2:41 - loss: 4.6922 - acc: 0.024 - ETA: 2:40 - loss: 4.6925 - acc: 0.024 - ETA: 2:39 - loss: 4.6937 - acc: 0.024 - ETA: 2:38 - loss: 4.6940 - acc: 0.024 - ETA: 2:37 - loss: 4.6934 - acc: 0.024 - ETA: 2:35 - loss: 4.6934 - acc: 0.024 - ETA: 2:34 - loss: 4.6945 - acc: 0.024 - ETA: 2:33 - loss: 4.6933 - acc: 0.024 - ETA: 2:32 - loss: 4.6933 - acc: 0.0248"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2:31 - loss: 4.6924 - acc: 0.024 - ETA: 2:29 - loss: 4.6920 - acc: 0.025 - ETA: 2:28 - loss: 4.6921 - acc: 0.024 - ETA: 2:27 - loss: 4.6921 - acc: 0.024 - ETA: 2:26 - loss: 4.6930 - acc: 0.024 - ETA: 2:25 - loss: 4.6927 - acc: 0.024 - ETA: 2:24 - loss: 4.6922 - acc: 0.025 - ETA: 2:22 - loss: 4.6927 - acc: 0.025 - ETA: 2:21 - loss: 4.6929 - acc: 0.024 - ETA: 2:20 - loss: 4.6920 - acc: 0.025 - ETA: 2:19 - loss: 4.6913 - acc: 0.024 - ETA: 2:18 - loss: 4.6922 - acc: 0.024 - ETA: 2:17 - loss: 4.6921 - acc: 0.024 - ETA: 2:15 - loss: 4.6922 - acc: 0.025 - ETA: 2:14 - loss: 4.6907 - acc: 0.025 - ETA: 2:13 - loss: 4.6908 - acc: 0.025 - ETA: 2:12 - loss: 4.6904 - acc: 0.024 - ETA: 2:11 - loss: 4.6902 - acc: 0.024 - ETA: 2:09 - loss: 4.6910 - acc: 0.024 - ETA: 2:08 - loss: 4.6898 - acc: 0.025 - ETA: 2:07 - loss: 4.6896 - acc: 0.024 - ETA: 2:06 - loss: 4.6909 - acc: 0.024 - ETA: 2:05 - loss: 4.6910 - acc: 0.024 - ETA: 2:04 - loss: 4.6904 - acc: 0.024 - ETA: 2:02 - loss: 4.6909 - acc: 0.024 - ETA: 2:01 - loss: 4.6904 - acc: 0.024 - ETA: 2:00 - loss: 4.6902 - acc: 0.024 - ETA: 1:59 - loss: 4.6893 - acc: 0.024 - ETA: 1:58 - loss: 4.6891 - acc: 0.025 - ETA: 1:56 - loss: 4.6897 - acc: 0.025 - ETA: 1:55 - loss: 4.6895 - acc: 0.025 - ETA: 1:54 - loss: 4.6903 - acc: 0.025 - ETA: 1:53 - loss: 4.6910 - acc: 0.025 - ETA: 1:52 - loss: 4.6905 - acc: 0.025 - ETA: 1:51 - loss: 4.6906 - acc: 0.025 - ETA: 1:49 - loss: 4.6899 - acc: 0.025 - ETA: 1:48 - loss: 4.6907 - acc: 0.025 - ETA: 1:47 - loss: 4.6906 - acc: 0.025 - ETA: 1:46 - loss: 4.6911 - acc: 0.025 - ETA: 1:45 - loss: 4.6907 - acc: 0.025 - ETA: 1:43 - loss: 4.6903 - acc: 0.025 - ETA: 1:42 - loss: 4.6902 - acc: 0.025 - ETA: 1:41 - loss: 4.6899 - acc: 0.025 - ETA: 1:40 - loss: 4.6894 - acc: 0.025 - ETA: 1:39 - loss: 4.6895 - acc: 0.025 - ETA: 1:38 - loss: 4.6904 - acc: 0.025 - ETA: 1:36 - loss: 4.6908 - acc: 0.025 - ETA: 1:35 - loss: 4.6907 - acc: 0.025 - ETA: 1:34 - loss: 4.6911 - acc: 0.025 - ETA: 1:33 - loss: 4.6906 - acc: 0.025 - ETA: 1:32 - loss: 4.6910 - acc: 0.025 - ETA: 1:31 - loss: 4.6907 - acc: 0.025 - ETA: 1:29 - loss: 4.6909 - acc: 0.025 - ETA: 1:28 - loss: 4.6906 - acc: 0.025 - ETA: 1:27 - loss: 4.6909 - acc: 0.025 - ETA: 1:26 - loss: 4.6909 - acc: 0.025 - ETA: 1:25 - loss: 4.6902 - acc: 0.025 - ETA: 1:23 - loss: 4.6908 - acc: 0.025 - ETA: 1:22 - loss: 4.6914 - acc: 0.025 - ETA: 1:21 - loss: 4.6914 - acc: 0.025 - ETA: 1:20 - loss: 4.6914 - acc: 0.025 - ETA: 1:19 - loss: 4.6909 - acc: 0.025 - ETA: 1:17 - loss: 4.6917 - acc: 0.025 - ETA: 1:16 - loss: 4.6912 - acc: 0.025 - ETA: 1:15 - loss: 4.6919 - acc: 0.025 - ETA: 1:14 - loss: 4.6915 - acc: 0.025 - ETA: 1:13 - loss: 4.6918 - acc: 0.025 - ETA: 1:12 - loss: 4.6926 - acc: 0.025 - ETA: 1:10 - loss: 4.6926 - acc: 0.025 - ETA: 1:09 - loss: 4.6935 - acc: 0.025 - ETA: 1:08 - loss: 4.6931 - acc: 0.025 - ETA: 1:07 - loss: 4.6932 - acc: 0.025 - ETA: 1:06 - loss: 4.6935 - acc: 0.025 - ETA: 1:05 - loss: 4.6929 - acc: 0.025 - ETA: 1:03 - loss: 4.6934 - acc: 0.025 - ETA: 1:02 - loss: 4.6945 - acc: 0.025 - ETA: 1:01 - loss: 4.6947 - acc: 0.025 - ETA: 1:00 - loss: 4.6950 - acc: 0.025 - ETA: 59s - loss: 4.6952 - acc: 0.025 - ETA: 57s - loss: 4.6949 - acc: 0.02 - ETA: 56s - loss: 4.6951 - acc: 0.02 - ETA: 55s - loss: 4.6955 - acc: 0.02 - ETA: 54s - loss: 4.6966 - acc: 0.02 - ETA: 53s - loss: 4.6961 - acc: 0.02 - ETA: 52s - loss: 4.6959 - acc: 0.02 - ETA: 50s - loss: 4.6964 - acc: 0.02 - ETA: 49s - loss: 4.6966 - acc: 0.02 - ETA: 48s - loss: 4.6965 - acc: 0.02 - ETA: 47s - loss: 4.6963 - acc: 0.02 - ETA: 46s - loss: 4.6960 - acc: 0.02 - ETA: 45s - loss: 4.6966 - acc: 0.02 - ETA: 43s - loss: 4.6964 - acc: 0.02 - ETA: 42s - loss: 4.6964 - acc: 0.02 - ETA: 41s - loss: 4.6969 - acc: 0.02 - ETA: 40s - loss: 4.6971 - acc: 0.02 - ETA: 39s - loss: 4.6969 - acc: 0.02 - ETA: 38s - loss: 4.6965 - acc: 0.02 - ETA: 36s - loss: 4.6971 - acc: 0.02 - ETA: 35s - loss: 4.6976 - acc: 0.02 - ETA: 34s - loss: 4.6974 - acc: 0.02 - ETA: 33s - loss: 4.6968 - acc: 0.02 - ETA: 32s - loss: 4.6971 - acc: 0.02 - ETA: 31s - loss: 4.6969 - acc: 0.02 - ETA: 29s - loss: 4.6966 - acc: 0.02 - ETA: 28s - loss: 4.6968 - acc: 0.02 - ETA: 27s - loss: 4.6962 - acc: 0.02 - ETA: 26s - loss: 4.6962 - acc: 0.02 - ETA: 25s - loss: 4.6966 - acc: 0.02 - ETA: 24s - loss: 4.6967 - acc: 0.02 - ETA: 23s - loss: 4.6963 - acc: 0.02 - ETA: 21s - loss: 4.6964 - acc: 0.02 - ETA: 20s - loss: 4.6965 - acc: 0.02 - ETA: 19s - loss: 4.6969 - acc: 0.02 - ETA: 18s - loss: 4.6965 - acc: 0.02 - ETA: 17s - loss: 4.6963 - acc: 0.02 - ETA: 16s - loss: 4.6965 - acc: 0.02 - ETA: 14s - loss: 4.6959 - acc: 0.02 - ETA: 13s - loss: 4.6959 - acc: 0.02 - ETA: 12s - loss: 4.6956 - acc: 0.02 - ETA: 11s - loss: 4.6965 - acc: 0.02 - ETA: 10s - loss: 4.6968 - acc: 0.02 - ETA: 9s - loss: 4.6969 - acc: 0.0261 - ETA: 8s - loss: 4.6972 - acc: 0.026 - ETA: 6s - loss: 4.6969 - acc: 0.025 - ETA: 5s - loss: 4.6973 - acc: 0.026 - ETA: 4s - loss: 4.6972 - acc: 0.026 - ETA: 3s - loss: 4.6969 - acc: 0.026 - ETA: 2s - loss: 4.6972 - acc: 0.025 - ETA: 1s - loss: 4.6966 - acc: 0.025 - 402s 60ms/step - loss: 4.6964 - acc: 0.0259 - val_loss: 4.7463 - val_acc: 0.0287\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.75362 to 4.74630, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 7/10\n",
      "4080/6680 [=================>............] - ETA: 5:43 - loss: 4.5567 - acc: 0.100 - ETA: 5:45 - loss: 4.5801 - acc: 0.050 - ETA: 5:45 - loss: 4.5638 - acc: 0.066 - ETA: 5:51 - loss: 4.6254 - acc: 0.050 - ETA: 5:51 - loss: 4.6732 - acc: 0.040 - ETA: 5:50 - loss: 4.6710 - acc: 0.041 - ETA: 5:48 - loss: 4.6953 - acc: 0.035 - ETA: 5:49 - loss: 4.6731 - acc: 0.037 - ETA: 5:47 - loss: 4.7062 - acc: 0.033 - ETA: 5:47 - loss: 4.6768 - acc: 0.045 - ETA: 5:46 - loss: 4.6821 - acc: 0.040 - ETA: 5:44 - loss: 4.6932 - acc: 0.037 - ETA: 5:43 - loss: 4.6801 - acc: 0.034 - ETA: 5:42 - loss: 4.6813 - acc: 0.035 - ETA: 5:40 - loss: 4.6927 - acc: 0.033 - ETA: 5:38 - loss: 4.6886 - acc: 0.034 - ETA: 5:36 - loss: 4.6840 - acc: 0.032 - ETA: 5:35 - loss: 4.6676 - acc: 0.038 - ETA: 5:34 - loss: 4.6614 - acc: 0.042 - ETA: 5:32 - loss: 4.6696 - acc: 0.045 - ETA: 5:30 - loss: 4.6722 - acc: 0.047 - ETA: 5:29 - loss: 4.6759 - acc: 0.047 - ETA: 5:28 - loss: 4.6664 - acc: 0.045 - ETA: 5:28 - loss: 4.6601 - acc: 0.045 - ETA: 5:26 - loss: 4.6545 - acc: 0.044 - ETA: 5:25 - loss: 4.6613 - acc: 0.044 - ETA: 5:24 - loss: 4.6526 - acc: 0.042 - ETA: 5:22 - loss: 4.6570 - acc: 0.041 - ETA: 5:21 - loss: 4.6524 - acc: 0.039 - ETA: 5:20 - loss: 4.6489 - acc: 0.038 - ETA: 5:19 - loss: 4.6592 - acc: 0.037 - ETA: 5:18 - loss: 4.6590 - acc: 0.035 - ETA: 5:17 - loss: 4.6640 - acc: 0.034 - ETA: 5:15 - loss: 4.6640 - acc: 0.035 - ETA: 5:14 - loss: 4.6786 - acc: 0.034 - ETA: 5:13 - loss: 4.6756 - acc: 0.034 - ETA: 5:12 - loss: 4.6747 - acc: 0.033 - ETA: 5:11 - loss: 4.6712 - acc: 0.034 - ETA: 5:10 - loss: 4.6713 - acc: 0.033 - ETA: 5:08 - loss: 4.6678 - acc: 0.035 - ETA: 5:07 - loss: 4.6655 - acc: 0.036 - ETA: 5:06 - loss: 4.6663 - acc: 0.038 - ETA: 5:05 - loss: 4.6645 - acc: 0.037 - ETA: 5:04 - loss: 4.6601 - acc: 0.037 - ETA: 5:03 - loss: 4.6629 - acc: 0.038 - ETA: 5:02 - loss: 4.6642 - acc: 0.039 - ETA: 5:01 - loss: 4.6605 - acc: 0.038 - ETA: 5:01 - loss: 4.6594 - acc: 0.039 - ETA: 5:00 - loss: 4.6589 - acc: 0.039 - ETA: 4:59 - loss: 4.6607 - acc: 0.039 - ETA: 4:58 - loss: 4.6565 - acc: 0.038 - ETA: 4:57 - loss: 4.6557 - acc: 0.037 - ETA: 4:56 - loss: 4.6553 - acc: 0.037 - ETA: 4:54 - loss: 4.6601 - acc: 0.038 - ETA: 4:53 - loss: 4.6582 - acc: 0.038 - ETA: 4:52 - loss: 4.6597 - acc: 0.038 - ETA: 4:51 - loss: 4.6614 - acc: 0.037 - ETA: 4:50 - loss: 4.6604 - acc: 0.037 - ETA: 4:49 - loss: 4.6602 - acc: 0.036 - ETA: 4:47 - loss: 4.6603 - acc: 0.037 - ETA: 4:46 - loss: 4.6616 - acc: 0.036 - ETA: 4:45 - loss: 4.6601 - acc: 0.036 - ETA: 4:44 - loss: 4.6599 - acc: 0.035 - ETA: 4:43 - loss: 4.6617 - acc: 0.035 - ETA: 4:42 - loss: 4.6591 - acc: 0.036 - ETA: 4:40 - loss: 4.6591 - acc: 0.035 - ETA: 4:39 - loss: 4.6604 - acc: 0.035 - ETA: 4:38 - loss: 4.6631 - acc: 0.034 - ETA: 4:37 - loss: 4.6629 - acc: 0.034 - ETA: 4:36 - loss: 4.6609 - acc: 0.034 - ETA: 4:35 - loss: 4.6609 - acc: 0.034 - ETA: 4:34 - loss: 4.6607 - acc: 0.034 - ETA: 4:32 - loss: 4.6646 - acc: 0.033 - ETA: 4:31 - loss: 4.6627 - acc: 0.033 - ETA: 4:30 - loss: 4.6624 - acc: 0.032 - ETA: 4:29 - loss: 4.6614 - acc: 0.032 - ETA: 4:28 - loss: 4.6627 - acc: 0.033 - ETA: 4:27 - loss: 4.6627 - acc: 0.033 - ETA: 4:26 - loss: 4.6630 - acc: 0.034 - ETA: 4:25 - loss: 4.6604 - acc: 0.033 - ETA: 4:24 - loss: 4.6619 - acc: 0.034 - ETA: 4:23 - loss: 4.6652 - acc: 0.034 - ETA: 4:22 - loss: 4.6659 - acc: 0.033 - ETA: 4:21 - loss: 4.6661 - acc: 0.033 - ETA: 4:20 - loss: 4.6651 - acc: 0.032 - ETA: 4:19 - loss: 4.6665 - acc: 0.032 - ETA: 4:18 - loss: 4.6648 - acc: 0.032 - ETA: 4:17 - loss: 4.6666 - acc: 0.031 - ETA: 4:16 - loss: 4.6670 - acc: 0.032 - ETA: 4:15 - loss: 4.6670 - acc: 0.031 - ETA: 4:14 - loss: 4.6669 - acc: 0.031 - ETA: 4:13 - loss: 4.6677 - acc: 0.031 - ETA: 4:12 - loss: 4.6683 - acc: 0.030 - ETA: 4:11 - loss: 4.6663 - acc: 0.030 - ETA: 4:10 - loss: 4.6648 - acc: 0.030 - ETA: 4:09 - loss: 4.6615 - acc: 0.030 - ETA: 4:07 - loss: 4.6603 - acc: 0.030 - ETA: 4:06 - loss: 4.6613 - acc: 0.030 - ETA: 4:05 - loss: 4.6628 - acc: 0.030 - ETA: 4:04 - loss: 4.6625 - acc: 0.030 - ETA: 4:03 - loss: 4.6622 - acc: 0.031 - ETA: 4:02 - loss: 4.6624 - acc: 0.031 - ETA: 4:01 - loss: 4.6631 - acc: 0.031 - ETA: 4:00 - loss: 4.6611 - acc: 0.031 - ETA: 3:59 - loss: 4.6646 - acc: 0.031 - ETA: 3:58 - loss: 4.6651 - acc: 0.031 - ETA: 3:57 - loss: 4.6633 - acc: 0.031 - ETA: 3:56 - loss: 4.6628 - acc: 0.031 - ETA: 3:55 - loss: 4.6608 - acc: 0.030 - ETA: 3:54 - loss: 4.6610 - acc: 0.030 - ETA: 3:53 - loss: 4.6642 - acc: 0.030 - ETA: 3:52 - loss: 4.6629 - acc: 0.030 - ETA: 3:51 - loss: 4.6626 - acc: 0.030 - ETA: 3:50 - loss: 4.6636 - acc: 0.030 - ETA: 3:49 - loss: 4.6631 - acc: 0.030 - ETA: 3:47 - loss: 4.6626 - acc: 0.030 - ETA: 3:46 - loss: 4.6622 - acc: 0.030 - ETA: 3:45 - loss: 4.6596 - acc: 0.030 - ETA: 3:44 - loss: 4.6609 - acc: 0.031 - ETA: 3:43 - loss: 4.6611 - acc: 0.030 - ETA: 3:42 - loss: 4.6599 - acc: 0.031 - ETA: 3:41 - loss: 4.6590 - acc: 0.031 - ETA: 3:40 - loss: 4.6582 - acc: 0.031 - ETA: 3:39 - loss: 4.6569 - acc: 0.031 - ETA: 3:38 - loss: 4.6578 - acc: 0.032 - ETA: 3:37 - loss: 4.6587 - acc: 0.031 - ETA: 3:36 - loss: 4.6583 - acc: 0.031 - ETA: 3:35 - loss: 4.6596 - acc: 0.031 - ETA: 3:34 - loss: 4.6598 - acc: 0.031 - ETA: 3:33 - loss: 4.6603 - acc: 0.031 - ETA: 3:32 - loss: 4.6609 - acc: 0.030 - ETA: 3:31 - loss: 4.6603 - acc: 0.031 - ETA: 3:30 - loss: 4.6608 - acc: 0.030 - ETA: 3:29 - loss: 4.6607 - acc: 0.031 - ETA: 3:28 - loss: 4.6594 - acc: 0.031 - ETA: 3:27 - loss: 4.6596 - acc: 0.031 - ETA: 3:26 - loss: 4.6583 - acc: 0.031 - ETA: 3:25 - loss: 4.6589 - acc: 0.031 - ETA: 3:24 - loss: 4.6594 - acc: 0.030 - ETA: 3:23 - loss: 4.6603 - acc: 0.031 - ETA: 3:22 - loss: 4.6614 - acc: 0.031 - ETA: 3:21 - loss: 4.6627 - acc: 0.031 - ETA: 3:20 - loss: 4.6643 - acc: 0.031 - ETA: 3:18 - loss: 4.6641 - acc: 0.030 - ETA: 3:17 - loss: 4.6632 - acc: 0.031 - ETA: 3:16 - loss: 4.6635 - acc: 0.031 - ETA: 3:16 - loss: 4.6634 - acc: 0.031 - ETA: 3:14 - loss: 4.6630 - acc: 0.031 - ETA: 3:13 - loss: 4.6640 - acc: 0.031 - ETA: 3:12 - loss: 4.6640 - acc: 0.031 - ETA: 3:11 - loss: 4.6638 - acc: 0.031 - ETA: 3:10 - loss: 4.6642 - acc: 0.030 - ETA: 3:09 - loss: 4.6641 - acc: 0.030 - ETA: 3:08 - loss: 4.6650 - acc: 0.030 - ETA: 3:07 - loss: 4.6650 - acc: 0.030 - ETA: 3:06 - loss: 4.6649 - acc: 0.030 - ETA: 3:05 - loss: 4.6658 - acc: 0.029 - ETA: 3:04 - loss: 4.6675 - acc: 0.029 - ETA: 3:03 - loss: 4.6670 - acc: 0.029 - ETA: 3:02 - loss: 4.6672 - acc: 0.029 - ETA: 3:01 - loss: 4.6684 - acc: 0.029 - ETA: 3:00 - loss: 4.6679 - acc: 0.029 - ETA: 2:59 - loss: 4.6678 - acc: 0.029 - ETA: 2:58 - loss: 4.6665 - acc: 0.029 - ETA: 2:57 - loss: 4.6666 - acc: 0.029 - ETA: 2:56 - loss: 4.6671 - acc: 0.029 - ETA: 2:55 - loss: 4.6656 - acc: 0.029 - ETA: 2:54 - loss: 4.6642 - acc: 0.029 - ETA: 2:53 - loss: 4.6625 - acc: 0.029 - ETA: 2:52 - loss: 4.6620 - acc: 0.029 - ETA: 2:51 - loss: 4.6631 - acc: 0.029 - ETA: 2:50 - loss: 4.6638 - acc: 0.029 - ETA: 2:49 - loss: 4.6628 - acc: 0.030 - ETA: 2:48 - loss: 4.6632 - acc: 0.029 - ETA: 2:47 - loss: 4.6638 - acc: 0.029 - ETA: 2:46 - loss: 4.6636 - acc: 0.029 - ETA: 2:44 - loss: 4.6645 - acc: 0.029 - ETA: 2:43 - loss: 4.6647 - acc: 0.029 - ETA: 2:42 - loss: 4.6655 - acc: 0.029 - ETA: 2:41 - loss: 4.6658 - acc: 0.029 - ETA: 2:40 - loss: 4.6666 - acc: 0.029 - ETA: 2:39 - loss: 4.6659 - acc: 0.029 - ETA: 2:38 - loss: 4.6652 - acc: 0.029 - ETA: 2:37 - loss: 4.6647 - acc: 0.029 - ETA: 2:36 - loss: 4.6643 - acc: 0.029 - ETA: 2:35 - loss: 4.6634 - acc: 0.029 - ETA: 2:34 - loss: 4.6619 - acc: 0.029 - ETA: 2:33 - loss: 4.6602 - acc: 0.029 - ETA: 2:32 - loss: 4.6601 - acc: 0.029 - ETA: 2:31 - loss: 4.6602 - acc: 0.029 - ETA: 2:29 - loss: 4.6592 - acc: 0.029 - ETA: 2:28 - loss: 4.6586 - acc: 0.029 - ETA: 2:27 - loss: 4.6590 - acc: 0.029 - ETA: 2:26 - loss: 4.6584 - acc: 0.029 - ETA: 2:25 - loss: 4.6589 - acc: 0.029 - ETA: 2:24 - loss: 4.6598 - acc: 0.029 - ETA: 2:23 - loss: 4.6588 - acc: 0.029 - ETA: 2:22 - loss: 4.6597 - acc: 0.029 - ETA: 2:21 - loss: 4.6602 - acc: 0.029 - ETA: 2:20 - loss: 4.6605 - acc: 0.029 - ETA: 2:19 - loss: 4.6603 - acc: 0.030 - ETA: 2:18 - loss: 4.6589 - acc: 0.030 - ETA: 2:17 - loss: 4.6573 - acc: 0.030 - ETA: 2:16 - loss: 4.6568 - acc: 0.03096680/6680 [==============================] - ETA: 2:15 - loss: 4.6576 - acc: 0.030 - ETA: 2:14 - loss: 4.6579 - acc: 0.030 - ETA: 2:13 - loss: 4.6581 - acc: 0.030 - ETA: 2:12 - loss: 4.6578 - acc: 0.030 - ETA: 2:10 - loss: 4.6583 - acc: 0.030 - ETA: 2:09 - loss: 4.6591 - acc: 0.030 - ETA: 2:08 - loss: 4.6593 - acc: 0.030 - ETA: 2:07 - loss: 4.6601 - acc: 0.030 - ETA: 2:06 - loss: 4.6601 - acc: 0.030 - ETA: 2:05 - loss: 4.6603 - acc: 0.030 - ETA: 2:04 - loss: 4.6607 - acc: 0.030 - ETA: 2:03 - loss: 4.6612 - acc: 0.030 - ETA: 2:02 - loss: 4.6605 - acc: 0.030 - ETA: 2:01 - loss: 4.6604 - acc: 0.030 - ETA: 2:00 - loss: 4.6598 - acc: 0.030 - ETA: 1:59 - loss: 4.6595 - acc: 0.030 - ETA: 1:58 - loss: 4.6601 - acc: 0.030 - ETA: 1:57 - loss: 4.6600 - acc: 0.030 - ETA: 1:56 - loss: 4.6597 - acc: 0.030 - ETA: 1:55 - loss: 4.6594 - acc: 0.030 - ETA: 1:53 - loss: 4.6601 - acc: 0.030 - ETA: 1:52 - loss: 4.6599 - acc: 0.030 - ETA: 1:51 - loss: 4.6593 - acc: 0.030 - ETA: 1:50 - loss: 4.6596 - acc: 0.030 - ETA: 1:49 - loss: 4.6599 - acc: 0.030 - ETA: 1:48 - loss: 4.6597 - acc: 0.030 - ETA: 1:47 - loss: 4.6589 - acc: 0.030 - ETA: 1:46 - loss: 4.6594 - acc: 0.030 - ETA: 1:45 - loss: 4.6599 - acc: 0.030 - ETA: 1:44 - loss: 4.6600 - acc: 0.030 - ETA: 1:43 - loss: 4.6618 - acc: 0.030 - ETA: 1:42 - loss: 4.6636 - acc: 0.029 - ETA: 1:41 - loss: 4.6640 - acc: 0.030 - ETA: 1:40 - loss: 4.6649 - acc: 0.030 - ETA: 1:39 - loss: 4.6648 - acc: 0.030 - ETA: 1:38 - loss: 4.6645 - acc: 0.030 - ETA: 1:37 - loss: 4.6644 - acc: 0.030 - ETA: 1:36 - loss: 4.6646 - acc: 0.030 - ETA: 1:35 - loss: 4.6642 - acc: 0.030 - ETA: 1:33 - loss: 4.6645 - acc: 0.029 - ETA: 1:32 - loss: 4.6646 - acc: 0.029 - ETA: 1:31 - loss: 4.6646 - acc: 0.029 - ETA: 1:30 - loss: 4.6657 - acc: 0.029 - ETA: 1:29 - loss: 4.6657 - acc: 0.029 - ETA: 1:28 - loss: 4.6658 - acc: 0.029 - ETA: 1:27 - loss: 4.6657 - acc: 0.029 - ETA: 1:26 - loss: 4.6652 - acc: 0.029 - ETA: 1:25 - loss: 4.6654 - acc: 0.030 - ETA: 1:24 - loss: 4.6654 - acc: 0.029 - ETA: 1:23 - loss: 4.6662 - acc: 0.029 - ETA: 1:22 - loss: 4.6670 - acc: 0.029 - ETA: 1:21 - loss: 4.6674 - acc: 0.029 - ETA: 1:20 - loss: 4.6671 - acc: 0.029 - ETA: 1:19 - loss: 4.6676 - acc: 0.029 - ETA: 1:18 - loss: 4.6681 - acc: 0.029 - ETA: 1:17 - loss: 4.6675 - acc: 0.029 - ETA: 1:16 - loss: 4.6675 - acc: 0.029 - ETA: 1:15 - loss: 4.6672 - acc: 0.030 - ETA: 1:14 - loss: 4.6674 - acc: 0.030 - ETA: 1:13 - loss: 4.6671 - acc: 0.030 - ETA: 1:11 - loss: 4.6670 - acc: 0.030 - ETA: 1:10 - loss: 4.6666 - acc: 0.030 - ETA: 1:09 - loss: 4.6660 - acc: 0.030 - ETA: 1:08 - loss: 4.6666 - acc: 0.030 - ETA: 1:07 - loss: 4.6673 - acc: 0.030 - ETA: 1:06 - loss: 4.6671 - acc: 0.030 - ETA: 1:05 - loss: 4.6670 - acc: 0.030 - ETA: 1:04 - loss: 4.6670 - acc: 0.030 - ETA: 1:03 - loss: 4.6673 - acc: 0.030 - ETA: 1:02 - loss: 4.6669 - acc: 0.029 - ETA: 1:01 - loss: 4.6668 - acc: 0.030 - ETA: 1:00 - loss: 4.6673 - acc: 0.030 - ETA: 59s - loss: 4.6672 - acc: 0.030 - ETA: 58s - loss: 4.6681 - acc: 0.03 - ETA: 57s - loss: 4.6677 - acc: 0.02 - ETA: 56s - loss: 4.6674 - acc: 0.03 - ETA: 55s - loss: 4.6676 - acc: 0.02 - ETA: 54s - loss: 4.6676 - acc: 0.02 - ETA: 53s - loss: 4.6676 - acc: 0.02 - ETA: 52s - loss: 4.6676 - acc: 0.02 - ETA: 51s - loss: 4.6664 - acc: 0.02 - ETA: 50s - loss: 4.6663 - acc: 0.02 - ETA: 49s - loss: 4.6656 - acc: 0.02 - ETA: 47s - loss: 4.6651 - acc: 0.02 - ETA: 46s - loss: 4.6644 - acc: 0.02 - ETA: 45s - loss: 4.6648 - acc: 0.02 - ETA: 44s - loss: 4.6653 - acc: 0.02 - ETA: 43s - loss: 4.6650 - acc: 0.02 - ETA: 42s - loss: 4.6653 - acc: 0.02 - ETA: 41s - loss: 4.6654 - acc: 0.02 - ETA: 40s - loss: 4.6657 - acc: 0.02 - ETA: 39s - loss: 4.6656 - acc: 0.02 - ETA: 38s - loss: 4.6647 - acc: 0.03 - ETA: 37s - loss: 4.6644 - acc: 0.02 - ETA: 36s - loss: 4.6651 - acc: 0.02 - ETA: 35s - loss: 4.6654 - acc: 0.02 - ETA: 34s - loss: 4.6650 - acc: 0.02 - ETA: 33s - loss: 4.6647 - acc: 0.02 - ETA: 32s - loss: 4.6650 - acc: 0.02 - ETA: 31s - loss: 4.6645 - acc: 0.02 - ETA: 30s - loss: 4.6642 - acc: 0.02 - ETA: 29s - loss: 4.6654 - acc: 0.02 - ETA: 28s - loss: 4.6651 - acc: 0.02 - ETA: 27s - loss: 4.6655 - acc: 0.02 - ETA: 26s - loss: 4.6661 - acc: 0.02 - ETA: 25s - loss: 4.6663 - acc: 0.02 - ETA: 24s - loss: 4.6660 - acc: 0.02 - ETA: 22s - loss: 4.6661 - acc: 0.02 - ETA: 21s - loss: 4.6658 - acc: 0.02 - ETA: 20s - loss: 4.6662 - acc: 0.02 - ETA: 19s - loss: 4.6652 - acc: 0.02 - ETA: 18s - loss: 4.6659 - acc: 0.02 - ETA: 17s - loss: 4.6662 - acc: 0.02 - ETA: 16s - loss: 4.6668 - acc: 0.02 - ETA: 15s - loss: 4.6672 - acc: 0.02 - ETA: 14s - loss: 4.6673 - acc: 0.02 - ETA: 13s - loss: 4.6673 - acc: 0.02 - ETA: 12s - loss: 4.6670 - acc: 0.02 - ETA: 11s - loss: 4.6665 - acc: 0.02 - ETA: 10s - loss: 4.6658 - acc: 0.02 - ETA: 9s - loss: 4.6659 - acc: 0.0295 - ETA: 8s - loss: 4.6657 - acc: 0.029 - ETA: 7s - loss: 4.6655 - acc: 0.029 - ETA: 6s - loss: 4.6652 - acc: 0.029 - ETA: 5s - loss: 4.6651 - acc: 0.029 - ETA: 4s - loss: 4.6651 - acc: 0.029 - ETA: 3s - loss: 4.6648 - acc: 0.029 - ETA: 2s - loss: 4.6642 - acc: 0.029 - ETA: 1s - loss: 4.6646 - acc: 0.029 - 363s 54ms/step - loss: 4.6648 - acc: 0.0298 - val_loss: 4.7099 - val_acc: 0.0287\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.74630 to 4.70990, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 5:22 - loss: 4.7092 - acc: 0.100 - ETA: 5:18 - loss: 4.6666 - acc: 0.100 - ETA: 5:18 - loss: 4.5986 - acc: 0.066 - ETA: 5:19 - loss: 4.6098 - acc: 0.050 - ETA: 5:18 - loss: 4.6332 - acc: 0.040 - ETA: 5:17 - loss: 4.6158 - acc: 0.033 - ETA: 5:17 - loss: 4.6139 - acc: 0.035 - ETA: 5:17 - loss: 4.6195 - acc: 0.031 - ETA: 5:16 - loss: 4.5969 - acc: 0.038 - ETA: 5:17 - loss: 4.6270 - acc: 0.035 - ETA: 5:17 - loss: 4.6349 - acc: 0.031 - ETA: 5:15 - loss: 4.6340 - acc: 0.033 - ETA: 5:15 - loss: 4.6461 - acc: 0.030 - ETA: 5:13 - loss: 4.6479 - acc: 0.028 - ETA: 5:12 - loss: 4.6471 - acc: 0.026 - ETA: 5:12 - loss: 4.6323 - acc: 0.034 - ETA: 5:10 - loss: 4.6244 - acc: 0.041 - ETA: 5:09 - loss: 4.6224 - acc: 0.041 - ETA: 5:08 - loss: 4.6253 - acc: 0.042 - ETA: 5:07 - loss: 4.6147 - acc: 0.042 - ETA: 5:06 - loss: 4.5989 - acc: 0.042 - ETA: 5:05 - loss: 4.5947 - acc: 0.040 - ETA: 5:04 - loss: 4.6025 - acc: 0.041 - ETA: 5:03 - loss: 4.6166 - acc: 0.039 - ETA: 5:02 - loss: 4.6249 - acc: 0.040 - ETA: 5:01 - loss: 4.6232 - acc: 0.038 - ETA: 5:00 - loss: 4.6211 - acc: 0.038 - ETA: 4:59 - loss: 4.6183 - acc: 0.037 - ETA: 4:58 - loss: 4.6173 - acc: 0.036 - ETA: 4:57 - loss: 4.6191 - acc: 0.035 - ETA: 4:56 - loss: 4.6145 - acc: 0.033 - ETA: 4:55 - loss: 4.6172 - acc: 0.032 - ETA: 4:54 - loss: 4.6216 - acc: 0.031 - ETA: 4:53 - loss: 4.6184 - acc: 0.033 - ETA: 4:52 - loss: 4.6211 - acc: 0.034 - ETA: 4:51 - loss: 4.6255 - acc: 0.033 - ETA: 4:50 - loss: 4.6272 - acc: 0.033 - ETA: 4:49 - loss: 4.6267 - acc: 0.032 - ETA: 4:48 - loss: 4.6231 - acc: 0.033 - ETA: 4:47 - loss: 4.6160 - acc: 0.032 - ETA: 4:46 - loss: 4.6147 - acc: 0.031 - ETA: 4:45 - loss: 4.6122 - acc: 0.032 - ETA: 4:44 - loss: 4.6199 - acc: 0.032 - ETA: 4:43 - loss: 4.6201 - acc: 0.031 - ETA: 4:42 - loss: 4.6191 - acc: 0.031 - ETA: 4:41 - loss: 4.6194 - acc: 0.030 - ETA: 4:40 - loss: 4.6218 - acc: 0.030 - ETA: 4:39 - loss: 4.6263 - acc: 0.030 - ETA: 4:38 - loss: 4.6296 - acc: 0.030 - ETA: 4:37 - loss: 4.6247 - acc: 0.030 - ETA: 4:36 - loss: 4.6248 - acc: 0.029 - ETA: 4:35 - loss: 4.6247 - acc: 0.028 - ETA: 4:34 - loss: 4.6236 - acc: 0.028 - ETA: 4:33 - loss: 4.6210 - acc: 0.028 - ETA: 4:32 - loss: 4.6212 - acc: 0.029 - ETA: 4:32 - loss: 4.6253 - acc: 0.028 - ETA: 4:30 - loss: 4.6227 - acc: 0.028 - ETA: 4:29 - loss: 4.6231 - acc: 0.029 - ETA: 4:28 - loss: 4.6237 - acc: 0.031 - ETA: 4:28 - loss: 4.6196 - acc: 0.031 - ETA: 4:27 - loss: 4.6164 - acc: 0.031 - ETA: 4:26 - loss: 4.6217 - acc: 0.030 - ETA: 4:25 - loss: 4.6204 - acc: 0.030 - ETA: 4:24 - loss: 4.6254 - acc: 0.030 - ETA: 4:22 - loss: 4.6300 - acc: 0.030 - ETA: 4:21 - loss: 4.6262 - acc: 0.030 - ETA: 4:20 - loss: 4.6247 - acc: 0.029 - ETA: 4:19 - loss: 4.6264 - acc: 0.029 - ETA: 4:18 - loss: 4.6260 - acc: 0.029 - ETA: 4:17 - loss: 4.6261 - acc: 0.029 - ETA: 4:16 - loss: 4.6260 - acc: 0.030 - ETA: 4:15 - loss: 4.6307 - acc: 0.029 - ETA: 4:14 - loss: 4.6294 - acc: 0.031 - ETA: 4:13 - loss: 4.6301 - acc: 0.031 - ETA: 4:12 - loss: 4.6315 - acc: 0.030 - ETA: 4:11 - loss: 4.6313 - acc: 0.030 - ETA: 4:10 - loss: 4.6333 - acc: 0.029 - ETA: 4:10 - loss: 4.6341 - acc: 0.029 - ETA: 4:09 - loss: 4.6348 - acc: 0.029 - ETA: 4:08 - loss: 4.6336 - acc: 0.030 - ETA: 4:06 - loss: 4.6339 - acc: 0.031 - ETA: 4:05 - loss: 4.6336 - acc: 0.031 - ETA: 4:05 - loss: 4.6334 - acc: 0.031 - ETA: 4:04 - loss: 4.6362 - acc: 0.031 - ETA: 4:03 - loss: 4.6406 - acc: 0.031 - ETA: 4:01 - loss: 4.6374 - acc: 0.033 - ETA: 4:00 - loss: 4.6358 - acc: 0.034 - ETA: 3:59 - loss: 4.6368 - acc: 0.034 - ETA: 3:58 - loss: 4.6376 - acc: 0.034 - ETA: 3:57 - loss: 4.6371 - acc: 0.034 - ETA: 3:56 - loss: 4.6385 - acc: 0.034 - ETA: 3:55 - loss: 4.6386 - acc: 0.034 - ETA: 3:54 - loss: 4.6370 - acc: 0.036 - ETA: 3:53 - loss: 4.6380 - acc: 0.035 - ETA: 3:52 - loss: 4.6384 - acc: 0.035 - ETA: 3:51 - loss: 4.6413 - acc: 0.034 - ETA: 3:50 - loss: 4.6413 - acc: 0.034 - ETA: 3:49 - loss: 4.6388 - acc: 0.034 - ETA: 3:48 - loss: 4.6370 - acc: 0.034 - ETA: 3:47 - loss: 4.6397 - acc: 0.034 - ETA: 3:46 - loss: 4.6407 - acc: 0.034 - ETA: 3:45 - loss: 4.6412 - acc: 0.033 - ETA: 3:44 - loss: 4.6413 - acc: 0.033 - ETA: 3:43 - loss: 4.6414 - acc: 0.033 - ETA: 3:42 - loss: 4.6403 - acc: 0.033 - ETA: 3:42 - loss: 4.6402 - acc: 0.033 - ETA: 3:41 - loss: 4.6412 - acc: 0.033 - ETA: 3:40 - loss: 4.6426 - acc: 0.032 - ETA: 3:39 - loss: 4.6440 - acc: 0.033 - ETA: 3:38 - loss: 4.6433 - acc: 0.033 - ETA: 3:37 - loss: 4.6450 - acc: 0.032 - ETA: 3:36 - loss: 4.6444 - acc: 0.033 - ETA: 3:35 - loss: 4.6424 - acc: 0.032 - ETA: 3:34 - loss: 4.6425 - acc: 0.032 - ETA: 3:33 - loss: 4.6422 - acc: 0.033 - ETA: 3:32 - loss: 4.6422 - acc: 0.033 - ETA: 3:31 - loss: 4.6417 - acc: 0.033 - ETA: 3:30 - loss: 4.6419 - acc: 0.033 - ETA: 3:29 - loss: 4.6449 - acc: 0.033 - ETA: 3:28 - loss: 4.6452 - acc: 0.033 - ETA: 3:27 - loss: 4.6452 - acc: 0.033 - ETA: 3:26 - loss: 4.6448 - acc: 0.032 - ETA: 3:25 - loss: 4.6432 - acc: 0.032 - ETA: 3:24 - loss: 4.6422 - acc: 0.032 - ETA: 3:23 - loss: 4.6400 - acc: 0.032 - ETA: 3:22 - loss: 4.6394 - acc: 0.032 - ETA: 3:21 - loss: 4.6392 - acc: 0.032 - ETA: 3:20 - loss: 4.6402 - acc: 0.033 - ETA: 3:19 - loss: 4.6403 - acc: 0.033 - ETA: 3:18 - loss: 4.6409 - acc: 0.033 - ETA: 3:17 - loss: 4.6422 - acc: 0.032 - ETA: 3:16 - loss: 4.6407 - acc: 0.033 - ETA: 3:15 - loss: 4.6424 - acc: 0.032 - ETA: 3:14 - loss: 4.6424 - acc: 0.032 - ETA: 3:13 - loss: 4.6425 - acc: 0.032 - ETA: 3:12 - loss: 4.6430 - acc: 0.032 - ETA: 3:11 - loss: 4.6440 - acc: 0.032 - ETA: 3:10 - loss: 4.6425 - acc: 0.032 - ETA: 3:09 - loss: 4.6428 - acc: 0.032 - ETA: 3:08 - loss: 4.6426 - acc: 0.032 - ETA: 3:07 - loss: 4.6418 - acc: 0.032 - ETA: 3:06 - loss: 4.6423 - acc: 0.032 - ETA: 3:05 - loss: 4.6417 - acc: 0.032 - ETA: 3:04 - loss: 4.6412 - acc: 0.032 - ETA: 3:03 - loss: 4.6419 - acc: 0.032 - ETA: 3:02 - loss: 4.6431 - acc: 0.032 - ETA: 3:01 - loss: 4.6436 - acc: 0.032 - ETA: 3:00 - loss: 4.6436 - acc: 0.031 - ETA: 2:59 - loss: 4.6439 - acc: 0.031 - ETA: 2:58 - loss: 4.6448 - acc: 0.031 - ETA: 2:57 - loss: 4.6450 - acc: 0.031 - ETA: 2:56 - loss: 4.6450 - acc: 0.032 - ETA: 2:55 - loss: 4.6439 - acc: 0.032 - ETA: 2:54 - loss: 4.6449 - acc: 0.032 - ETA: 2:53 - loss: 4.6439 - acc: 0.034 - ETA: 2:52 - loss: 4.6449 - acc: 0.034 - ETA: 2:51 - loss: 4.6442 - acc: 0.034 - ETA: 2:50 - loss: 4.6442 - acc: 0.034 - ETA: 2:49 - loss: 4.6440 - acc: 0.034 - ETA: 2:48 - loss: 4.6447 - acc: 0.034 - ETA: 2:47 - loss: 4.6443 - acc: 0.034 - ETA: 2:46 - loss: 4.6446 - acc: 0.034 - ETA: 2:45 - loss: 4.6431 - acc: 0.034 - ETA: 2:44 - loss: 4.6429 - acc: 0.033 - ETA: 2:43 - loss: 4.6444 - acc: 0.033 - ETA: 2:42 - loss: 4.6429 - acc: 0.033 - ETA: 2:41 - loss: 4.6439 - acc: 0.033 - ETA: 2:40 - loss: 4.6441 - acc: 0.033 - ETA: 2:39 - loss: 4.6459 - acc: 0.033 - ETA: 2:38 - loss: 4.6455 - acc: 0.033 - ETA: 2:37 - loss: 4.6442 - acc: 0.033 - ETA: 2:36 - loss: 4.6447 - acc: 0.033 - ETA: 2:36 - loss: 4.6448 - acc: 0.033 - ETA: 2:35 - loss: 4.6446 - acc: 0.033 - ETA: 2:34 - loss: 4.6455 - acc: 0.033 - ETA: 2:33 - loss: 4.6450 - acc: 0.033 - ETA: 2:32 - loss: 4.6430 - acc: 0.033 - ETA: 2:31 - loss: 4.6422 - acc: 0.033 - ETA: 2:30 - loss: 4.6420 - acc: 0.033 - ETA: 2:29 - loss: 4.6412 - acc: 0.034 - ETA: 2:28 - loss: 4.6419 - acc: 0.034 - ETA: 2:27 - loss: 4.6414 - acc: 0.034 - ETA: 2:26 - loss: 4.6413 - acc: 0.034 - ETA: 2:25 - loss: 4.6413 - acc: 0.034 - ETA: 2:24 - loss: 4.6414 - acc: 0.033 - ETA: 2:23 - loss: 4.6405 - acc: 0.033 - ETA: 2:22 - loss: 4.6410 - acc: 0.033 - ETA: 2:21 - loss: 4.6390 - acc: 0.033 - ETA: 2:20 - loss: 4.6402 - acc: 0.033 - ETA: 2:19 - loss: 4.6407 - acc: 0.033 - ETA: 2:18 - loss: 4.6395 - acc: 0.033 - ETA: 2:17 - loss: 4.6396 - acc: 0.033 - ETA: 2:16 - loss: 4.6398 - acc: 0.032 - ETA: 2:15 - loss: 4.6392 - acc: 0.032 - ETA: 2:14 - loss: 4.6395 - acc: 0.032 - ETA: 2:13 - loss: 4.6409 - acc: 0.032 - ETA: 2:12 - loss: 4.6428 - acc: 0.032 - ETA: 2:11 - loss: 4.6426 - acc: 0.032 - ETA: 2:10 - loss: 4.6427 - acc: 0.032 - ETA: 2:09 - loss: 4.6433 - acc: 0.033 - ETA: 2:08 - loss: 4.6436 - acc: 0.033 - ETA: 2:07 - loss: 4.6427 - acc: 0.033 - ETA: 2:06 - loss: 4.6428 - acc: 0.033 - ETA: 2:05 - loss: 4.6430 - acc: 0.0331"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2:04 - loss: 4.6429 - acc: 0.033 - ETA: 2:03 - loss: 4.6441 - acc: 0.033 - ETA: 2:02 - loss: 4.6433 - acc: 0.033 - ETA: 2:01 - loss: 4.6433 - acc: 0.033 - ETA: 2:00 - loss: 4.6431 - acc: 0.033 - ETA: 1:59 - loss: 4.6418 - acc: 0.033 - ETA: 1:58 - loss: 4.6412 - acc: 0.033 - ETA: 1:58 - loss: 4.6416 - acc: 0.033 - ETA: 1:57 - loss: 4.6411 - acc: 0.033 - ETA: 1:56 - loss: 4.6424 - acc: 0.032 - ETA: 1:55 - loss: 4.6432 - acc: 0.032 - ETA: 1:54 - loss: 4.6441 - acc: 0.032 - ETA: 1:53 - loss: 4.6434 - acc: 0.032 - ETA: 1:52 - loss: 4.6436 - acc: 0.032 - ETA: 1:51 - loss: 4.6434 - acc: 0.032 - ETA: 1:50 - loss: 4.6427 - acc: 0.032 - ETA: 1:49 - loss: 4.6417 - acc: 0.032 - ETA: 1:48 - loss: 4.6418 - acc: 0.032 - ETA: 1:47 - loss: 4.6414 - acc: 0.032 - ETA: 1:46 - loss: 4.6420 - acc: 0.032 - ETA: 1:45 - loss: 4.6426 - acc: 0.032 - ETA: 1:44 - loss: 4.6422 - acc: 0.032 - ETA: 1:43 - loss: 4.6419 - acc: 0.032 - ETA: 1:42 - loss: 4.6414 - acc: 0.032 - ETA: 1:41 - loss: 4.6415 - acc: 0.032 - ETA: 1:40 - loss: 4.6404 - acc: 0.032 - ETA: 1:39 - loss: 4.6400 - acc: 0.032 - ETA: 1:38 - loss: 4.6392 - acc: 0.033 - ETA: 1:37 - loss: 4.6402 - acc: 0.032 - ETA: 1:36 - loss: 4.6401 - acc: 0.032 - ETA: 1:35 - loss: 4.6398 - acc: 0.032 - ETA: 1:34 - loss: 4.6406 - acc: 0.032 - ETA: 1:33 - loss: 4.6406 - acc: 0.032 - ETA: 1:33 - loss: 4.6410 - acc: 0.032 - ETA: 1:32 - loss: 4.6404 - acc: 0.032 - ETA: 1:31 - loss: 4.6407 - acc: 0.032 - ETA: 1:30 - loss: 4.6414 - acc: 0.032 - ETA: 1:29 - loss: 4.6413 - acc: 0.032 - ETA: 1:28 - loss: 4.6412 - acc: 0.032 - ETA: 1:27 - loss: 4.6408 - acc: 0.032 - ETA: 1:26 - loss: 4.6401 - acc: 0.032 - ETA: 1:25 - loss: 4.6401 - acc: 0.032 - ETA: 1:24 - loss: 4.6390 - acc: 0.032 - ETA: 1:23 - loss: 4.6384 - acc: 0.032 - ETA: 1:22 - loss: 4.6392 - acc: 0.032 - ETA: 1:21 - loss: 4.6396 - acc: 0.032 - ETA: 1:20 - loss: 4.6397 - acc: 0.032 - ETA: 1:19 - loss: 4.6391 - acc: 0.032 - ETA: 1:18 - loss: 4.6397 - acc: 0.032 - ETA: 1:18 - loss: 4.6391 - acc: 0.032 - ETA: 1:17 - loss: 4.6389 - acc: 0.032 - ETA: 1:16 - loss: 4.6393 - acc: 0.032 - ETA: 1:15 - loss: 4.6392 - acc: 0.032 - ETA: 1:14 - loss: 4.6390 - acc: 0.032 - ETA: 1:13 - loss: 4.6383 - acc: 0.033 - ETA: 1:12 - loss: 4.6384 - acc: 0.033 - ETA: 1:12 - loss: 4.6383 - acc: 0.033 - ETA: 1:11 - loss: 4.6383 - acc: 0.033 - ETA: 1:10 - loss: 4.6395 - acc: 0.033 - ETA: 1:09 - loss: 4.6399 - acc: 0.033 - ETA: 1:08 - loss: 4.6395 - acc: 0.033 - ETA: 1:07 - loss: 4.6397 - acc: 0.033 - ETA: 1:06 - loss: 4.6398 - acc: 0.033 - ETA: 1:05 - loss: 4.6399 - acc: 0.033 - ETA: 1:04 - loss: 4.6394 - acc: 0.033 - ETA: 1:03 - loss: 4.6398 - acc: 0.033 - ETA: 1:02 - loss: 4.6405 - acc: 0.033 - ETA: 1:01 - loss: 4.6400 - acc: 0.033 - ETA: 1:00 - loss: 4.6417 - acc: 0.033 - ETA: 59s - loss: 4.6418 - acc: 0.033 - ETA: 58s - loss: 4.6414 - acc: 0.03 - ETA: 57s - loss: 4.6415 - acc: 0.03 - ETA: 56s - loss: 4.6423 - acc: 0.03 - ETA: 55s - loss: 4.6433 - acc: 0.03 - ETA: 54s - loss: 4.6432 - acc: 0.03 - ETA: 53s - loss: 4.6425 - acc: 0.03 - ETA: 52s - loss: 4.6428 - acc: 0.03 - ETA: 51s - loss: 4.6429 - acc: 0.03 - ETA: 50s - loss: 4.6441 - acc: 0.03 - ETA: 49s - loss: 4.6436 - acc: 0.03 - ETA: 48s - loss: 4.6436 - acc: 0.03 - ETA: 4:26 - loss: 4.6438 - acc: 0.033 - ETA: 4:20 - loss: 4.6444 - acc: 0.033 - ETA: 4:14 - loss: 4.6445 - acc: 0.033 - ETA: 4:08 - loss: 4.6438 - acc: 0.033 - ETA: 4:02 - loss: 4.6442 - acc: 0.033 - ETA: 3:56 - loss: 4.6440 - acc: 0.033 - ETA: 3:50 - loss: 4.6432 - acc: 0.032 - ETA: 3:44 - loss: 4.6434 - acc: 0.033 - ETA: 3:38 - loss: 4.6431 - acc: 0.033 - ETA: 3:32 - loss: 4.6434 - acc: 0.033 - ETA: 3:26 - loss: 4.6439 - acc: 0.033 - ETA: 3:20 - loss: 4.6438 - acc: 0.033 - ETA: 3:14 - loss: 4.6448 - acc: 0.033 - ETA: 3:08 - loss: 4.6445 - acc: 0.033 - ETA: 3:02 - loss: 4.6451 - acc: 0.033 - ETA: 2:56 - loss: 4.6452 - acc: 0.033 - ETA: 2:50 - loss: 4.6451 - acc: 0.033 - ETA: 2:44 - loss: 4.6449 - acc: 0.033 - ETA: 2:39 - loss: 4.6447 - acc: 0.033 - ETA: 2:33 - loss: 4.6453 - acc: 0.033 - ETA: 2:27 - loss: 4.6449 - acc: 0.033 - ETA: 2:22 - loss: 4.6439 - acc: 0.033 - ETA: 2:16 - loss: 4.6440 - acc: 0.033 - ETA: 2:11 - loss: 4.6436 - acc: 0.033 - ETA: 2:05 - loss: 4.6428 - acc: 0.033 - ETA: 1:59 - loss: 4.6434 - acc: 0.033 - ETA: 1:54 - loss: 4.6436 - acc: 0.033 - ETA: 1:48 - loss: 4.6431 - acc: 0.033 - ETA: 1:43 - loss: 4.6430 - acc: 0.033 - ETA: 1:38 - loss: 4.6426 - acc: 0.033 - ETA: 1:32 - loss: 4.6429 - acc: 0.033 - ETA: 1:27 - loss: 4.6427 - acc: 0.033 - ETA: 1:21 - loss: 4.6422 - acc: 0.033 - ETA: 1:16 - loss: 4.6422 - acc: 0.033 - ETA: 1:11 - loss: 4.6424 - acc: 0.033 - ETA: 1:06 - loss: 4.6429 - acc: 0.033 - ETA: 1:00 - loss: 4.6428 - acc: 0.033 - ETA: 55s - loss: 4.6435 - acc: 0.033 - ETA: 50s - loss: 4.6428 - acc: 0.03 - ETA: 45s - loss: 4.6435 - acc: 0.03 - ETA: 40s - loss: 4.6434 - acc: 0.03 - ETA: 35s - loss: 4.6433 - acc: 0.03 - ETA: 30s - loss: 4.6434 - acc: 0.03 - ETA: 24s - loss: 4.6429 - acc: 0.03 - ETA: 19s - loss: 4.6417 - acc: 0.03 - ETA: 14s - loss: 4.6413 - acc: 0.03 - ETA: 9s - loss: 4.6420 - acc: 0.0339 - ETA: 4s - loss: 4.6429 - acc: 0.033 - 1666s 249ms/step - loss: 4.6427 - acc: 0.0340 - val_loss: 4.6876 - val_acc: 0.0323\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.70990 to 4.68758, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 9/10\n",
      "4080/6680 [=================>............] - ETA: 7:03 - loss: 4.8468 - acc: 0.0000e+0 - ETA: 7:07 - loss: 4.6448 - acc: 0.0250    - ETA: 7:19 - loss: 4.6571 - acc: 0.016 - ETA: 7:14 - loss: 4.6231 - acc: 0.025 - ETA: 6:58 - loss: 4.6039 - acc: 0.020 - ETA: 6:49 - loss: 4.6072 - acc: 0.025 - ETA: 6:42 - loss: 4.6323 - acc: 0.021 - ETA: 6:38 - loss: 4.6219 - acc: 0.037 - ETA: 7:21 - loss: 4.6523 - acc: 0.033 - ETA: 7:43 - loss: 4.6407 - acc: 0.040 - ETA: 7:46 - loss: 4.6437 - acc: 0.036 - ETA: 7:45 - loss: 4.6458 - acc: 0.033 - ETA: 7:43 - loss: 4.6478 - acc: 0.030 - ETA: 7:35 - loss: 4.6242 - acc: 0.035 - ETA: 7:34 - loss: 4.6120 - acc: 0.033 - ETA: 7:29 - loss: 4.6063 - acc: 0.034 - ETA: 7:19 - loss: 4.6052 - acc: 0.035 - ETA: 7:06 - loss: 4.5995 - acc: 0.036 - ETA: 6:56 - loss: 4.6056 - acc: 0.034 - ETA: 6:46 - loss: 4.6083 - acc: 0.035 - ETA: 6:38 - loss: 4.6053 - acc: 0.035 - ETA: 6:34 - loss: 4.5998 - acc: 0.040 - ETA: 6:35 - loss: 4.5955 - acc: 0.041 - ETA: 6:36 - loss: 4.5880 - acc: 0.041 - ETA: 6:37 - loss: 4.5832 - acc: 0.044 - ETA: 6:35 - loss: 4.5882 - acc: 0.042 - ETA: 6:35 - loss: 4.5972 - acc: 0.042 - ETA: 6:31 - loss: 4.6036 - acc: 0.042 - ETA: 6:27 - loss: 4.6174 - acc: 0.041 - ETA: 6:24 - loss: 4.6260 - acc: 0.040 - ETA: 6:20 - loss: 4.6230 - acc: 0.040 - ETA: 6:17 - loss: 4.6216 - acc: 0.039 - ETA: 6:12 - loss: 4.6202 - acc: 0.039 - ETA: 6:07 - loss: 4.6203 - acc: 0.038 - ETA: 6:03 - loss: 4.6170 - acc: 0.040 - ETA: 5:59 - loss: 4.6223 - acc: 0.038 - ETA: 5:55 - loss: 4.6209 - acc: 0.039 - ETA: 5:52 - loss: 4.6183 - acc: 0.040 - ETA: 5:49 - loss: 4.6167 - acc: 0.042 - ETA: 5:48 - loss: 4.6094 - acc: 0.043 - ETA: 5:44 - loss: 4.6075 - acc: 0.042 - ETA: 5:41 - loss: 4.6064 - acc: 0.042 - ETA: 5:38 - loss: 4.6041 - acc: 0.043 - ETA: 5:36 - loss: 4.6046 - acc: 0.043 - ETA: 5:34 - loss: 4.6108 - acc: 0.042 - ETA: 5:31 - loss: 4.6098 - acc: 0.043 - ETA: 5:29 - loss: 4.6084 - acc: 0.043 - ETA: 5:26 - loss: 4.6075 - acc: 0.042 - ETA: 5:24 - loss: 4.6077 - acc: 0.042 - ETA: 5:23 - loss: 4.6087 - acc: 0.043 - ETA: 5:21 - loss: 4.6097 - acc: 0.042 - ETA: 5:19 - loss: 4.6085 - acc: 0.044 - ETA: 5:16 - loss: 4.6068 - acc: 0.045 - ETA: 5:15 - loss: 4.6081 - acc: 0.044 - ETA: 5:13 - loss: 4.6074 - acc: 0.044 - ETA: 5:11 - loss: 4.6054 - acc: 0.044 - ETA: 5:10 - loss: 4.6004 - acc: 0.047 - ETA: 5:09 - loss: 4.5975 - acc: 0.047 - ETA: 5:08 - loss: 4.5992 - acc: 0.046 - ETA: 5:05 - loss: 4.5941 - acc: 0.046 - ETA: 5:03 - loss: 4.5908 - acc: 0.046 - ETA: 5:01 - loss: 4.5947 - acc: 0.046 - ETA: 4:59 - loss: 4.5965 - acc: 0.045 - ETA: 4:57 - loss: 4.5956 - acc: 0.045 - ETA: 4:55 - loss: 4.5936 - acc: 0.044 - ETA: 4:53 - loss: 4.5924 - acc: 0.045 - ETA: 4:51 - loss: 4.5879 - acc: 0.045 - ETA: 4:50 - loss: 4.5869 - acc: 0.046 - ETA: 4:48 - loss: 4.5911 - acc: 0.045 - ETA: 4:46 - loss: 4.5881 - acc: 0.045 - ETA: 4:45 - loss: 4.5870 - acc: 0.045 - ETA: 4:43 - loss: 4.5909 - acc: 0.045 - ETA: 4:41 - loss: 4.5899 - acc: 0.045 - ETA: 4:39 - loss: 4.5899 - acc: 0.045 - ETA: 4:38 - loss: 4.5862 - acc: 0.046 - ETA: 4:36 - loss: 4.5830 - acc: 0.046 - ETA: 4:35 - loss: 4.5827 - acc: 0.046 - ETA: 4:34 - loss: 4.5835 - acc: 0.046 - ETA: 4:33 - loss: 4.5851 - acc: 0.046 - ETA: 4:32 - loss: 4.5850 - acc: 0.045 - ETA: 4:30 - loss: 4.5848 - acc: 0.045 - ETA: 4:29 - loss: 4.5865 - acc: 0.045 - ETA: 4:27 - loss: 4.5861 - acc: 0.044 - ETA: 4:26 - loss: 4.5867 - acc: 0.044 - ETA: 4:24 - loss: 4.5877 - acc: 0.043 - ETA: 4:23 - loss: 4.5883 - acc: 0.043 - ETA: 4:22 - loss: 4.5899 - acc: 0.043 - ETA: 4:20 - loss: 4.5915 - acc: 0.043 - ETA: 4:18 - loss: 4.5939 - acc: 0.042 - ETA: 4:17 - loss: 4.5937 - acc: 0.042 - ETA: 4:16 - loss: 4.5943 - acc: 0.042 - ETA: 4:14 - loss: 4.5940 - acc: 0.042 - ETA: 4:13 - loss: 4.5912 - acc: 0.042 - ETA: 4:11 - loss: 4.5954 - acc: 0.042 - ETA: 4:10 - loss: 4.5948 - acc: 0.042 - ETA: 4:09 - loss: 4.5971 - acc: 0.042 - ETA: 4:08 - loss: 4.5963 - acc: 0.042 - ETA: 4:07 - loss: 4.5954 - acc: 0.041 - ETA: 4:06 - loss: 4.5947 - acc: 0.041 - ETA: 4:05 - loss: 4.5960 - acc: 0.041 - ETA: 4:04 - loss: 4.5984 - acc: 0.041 - ETA: 4:02 - loss: 4.5998 - acc: 0.041 - ETA: 4:01 - loss: 4.5994 - acc: 0.040 - ETA: 3:59 - loss: 4.5993 - acc: 0.040 - ETA: 3:58 - loss: 4.6014 - acc: 0.040 - ETA: 3:57 - loss: 4.6006 - acc: 0.040 - ETA: 3:56 - loss: 4.5997 - acc: 0.039 - ETA: 3:54 - loss: 4.5992 - acc: 0.039 - ETA: 3:53 - loss: 4.6001 - acc: 0.039 - ETA: 3:52 - loss: 4.5970 - acc: 0.040 - ETA: 3:51 - loss: 4.5974 - acc: 0.040 - ETA: 3:49 - loss: 4.5967 - acc: 0.039 - ETA: 3:48 - loss: 4.5976 - acc: 0.039 - ETA: 3:47 - loss: 4.5981 - acc: 0.039 - ETA: 3:45 - loss: 4.5952 - acc: 0.039 - ETA: 3:44 - loss: 4.5959 - acc: 0.039 - ETA: 3:43 - loss: 4.5941 - acc: 0.039 - ETA: 3:42 - loss: 4.5936 - acc: 0.039 - ETA: 3:41 - loss: 4.5935 - acc: 0.039 - ETA: 3:40 - loss: 4.5963 - acc: 0.039 - ETA: 3:38 - loss: 4.5986 - acc: 0.038 - ETA: 3:37 - loss: 4.6018 - acc: 0.038 - ETA: 3:36 - loss: 4.6007 - acc: 0.039 - ETA: 3:35 - loss: 4.6015 - acc: 0.039 - ETA: 3:34 - loss: 4.6011 - acc: 0.038 - ETA: 3:33 - loss: 4.6016 - acc: 0.038 - ETA: 3:32 - loss: 4.6033 - acc: 0.039 - ETA: 3:30 - loss: 4.6003 - acc: 0.038 - ETA: 3:29 - loss: 4.6023 - acc: 0.038 - ETA: 3:28 - loss: 4.6019 - acc: 0.038 - ETA: 3:27 - loss: 4.6060 - acc: 0.037 - ETA: 3:26 - loss: 4.6050 - acc: 0.037 - ETA: 3:25 - loss: 4.6049 - acc: 0.038 - ETA: 3:24 - loss: 4.6063 - acc: 0.037 - ETA: 3:23 - loss: 4.6066 - acc: 0.037 - ETA: 3:22 - loss: 4.6063 - acc: 0.037 - ETA: 3:22 - loss: 4.6069 - acc: 0.038 - ETA: 3:21 - loss: 4.6069 - acc: 0.037 - ETA: 3:20 - loss: 4.6077 - acc: 0.037 - ETA: 3:19 - loss: 4.6073 - acc: 0.037 - ETA: 3:17 - loss: 4.6082 - acc: 0.036 - ETA: 3:17 - loss: 4.6082 - acc: 0.036 - ETA: 3:16 - loss: 4.6073 - acc: 0.036 - ETA: 3:15 - loss: 4.6070 - acc: 0.036 - ETA: 3:14 - loss: 4.6071 - acc: 0.036 - ETA: 3:12 - loss: 4.6075 - acc: 0.037 - ETA: 3:11 - loss: 4.6058 - acc: 0.037 - ETA: 3:10 - loss: 4.6039 - acc: 0.037 - ETA: 3:09 - loss: 4.6037 - acc: 0.037 - ETA: 3:08 - loss: 4.6046 - acc: 0.037 - ETA: 3:07 - loss: 4.6040 - acc: 0.037 - ETA: 3:06 - loss: 4.6034 - acc: 0.037 - ETA: 3:05 - loss: 4.6024 - acc: 0.038 - ETA: 3:04 - loss: 4.6020 - acc: 0.038 - ETA: 3:02 - loss: 4.6030 - acc: 0.038 - ETA: 3:01 - loss: 4.6035 - acc: 0.038 - ETA: 3:00 - loss: 4.6027 - acc: 0.038 - ETA: 2:59 - loss: 4.6028 - acc: 0.038 - ETA: 2:58 - loss: 4.6036 - acc: 0.038 - ETA: 2:57 - loss: 4.6022 - acc: 0.039 - ETA: 2:56 - loss: 4.6031 - acc: 0.039 - ETA: 2:55 - loss: 4.6032 - acc: 0.039 - ETA: 2:54 - loss: 4.6033 - acc: 0.039 - ETA: 2:53 - loss: 4.6038 - acc: 0.039 - ETA: 2:52 - loss: 4.6033 - acc: 0.039 - ETA: 2:51 - loss: 4.6028 - acc: 0.039 - ETA: 2:50 - loss: 4.6029 - acc: 0.038 - ETA: 2:49 - loss: 4.6023 - acc: 0.039 - ETA: 2:48 - loss: 4.6032 - acc: 0.039 - ETA: 2:46 - loss: 4.6031 - acc: 0.038 - ETA: 2:45 - loss: 4.6026 - acc: 0.038 - ETA: 2:44 - loss: 4.6046 - acc: 0.039 - ETA: 2:43 - loss: 4.6060 - acc: 0.038 - ETA: 2:42 - loss: 4.6074 - acc: 0.038 - ETA: 2:41 - loss: 4.6075 - acc: 0.038 - ETA: 2:40 - loss: 4.6072 - acc: 0.038 - ETA: 2:39 - loss: 4.6070 - acc: 0.038 - ETA: 2:38 - loss: 4.6089 - acc: 0.038 - ETA: 2:36 - loss: 4.6085 - acc: 0.038 - ETA: 2:35 - loss: 4.6068 - acc: 0.038 - ETA: 2:34 - loss: 4.6063 - acc: 0.038 - ETA: 2:33 - loss: 4.6054 - acc: 0.039 - ETA: 2:32 - loss: 4.6061 - acc: 0.038 - ETA: 2:31 - loss: 4.6061 - acc: 0.038 - ETA: 2:30 - loss: 4.6063 - acc: 0.038 - ETA: 2:28 - loss: 4.6074 - acc: 0.038 - ETA: 2:27 - loss: 4.6064 - acc: 0.039 - ETA: 2:26 - loss: 4.6069 - acc: 0.039 - ETA: 2:25 - loss: 4.6067 - acc: 0.039 - ETA: 2:24 - loss: 4.6063 - acc: 0.039 - ETA: 2:23 - loss: 4.6067 - acc: 0.039 - ETA: 2:22 - loss: 4.6072 - acc: 0.039 - ETA: 2:21 - loss: 4.6071 - acc: 0.038 - ETA: 2:19 - loss: 4.6061 - acc: 0.038 - ETA: 2:18 - loss: 4.6069 - acc: 0.039 - ETA: 2:17 - loss: 4.6072 - acc: 0.039 - ETA: 2:16 - loss: 4.6086 - acc: 0.039 - ETA: 2:15 - loss: 4.6080 - acc: 0.039 - ETA: 2:14 - loss: 4.6094 - acc: 0.039 - ETA: 2:13 - loss: 4.6100 - acc: 0.039 - ETA: 2:12 - loss: 4.6098 - acc: 0.039 - ETA: 2:11 - loss: 4.6105 - acc: 0.039 - ETA: 2:10 - loss: 4.6102 - acc: 0.038 - ETA: 2:09 - loss: 4.6103 - acc: 0.03876680/6680 [==============================] - ETA: 2:08 - loss: 4.6111 - acc: 0.039 - ETA: 2:07 - loss: 4.6099 - acc: 0.039 - ETA: 2:05 - loss: 4.6099 - acc: 0.039 - ETA: 2:04 - loss: 4.6100 - acc: 0.039 - ETA: 2:03 - loss: 4.6106 - acc: 0.039 - ETA: 2:02 - loss: 4.6093 - acc: 0.039 - ETA: 2:01 - loss: 4.6096 - acc: 0.038 - ETA: 2:00 - loss: 4.6090 - acc: 0.038 - ETA: 1:59 - loss: 4.6100 - acc: 0.038 - ETA: 1:58 - loss: 4.6095 - acc: 0.038 - ETA: 1:57 - loss: 4.6090 - acc: 0.038 - ETA: 1:56 - loss: 4.6077 - acc: 0.038 - ETA: 1:55 - loss: 4.6072 - acc: 0.038 - ETA: 1:54 - loss: 4.6069 - acc: 0.038 - ETA: 1:53 - loss: 4.6076 - acc: 0.038 - ETA: 1:52 - loss: 4.6086 - acc: 0.038 - ETA: 1:51 - loss: 4.6086 - acc: 0.038 - ETA: 1:50 - loss: 4.6083 - acc: 0.038 - ETA: 1:49 - loss: 4.6077 - acc: 0.038 - ETA: 1:48 - loss: 4.6077 - acc: 0.038 - ETA: 1:47 - loss: 4.6075 - acc: 0.038 - ETA: 1:46 - loss: 4.6071 - acc: 0.038 - ETA: 1:45 - loss: 4.6069 - acc: 0.038 - ETA: 1:44 - loss: 4.6072 - acc: 0.038 - ETA: 1:43 - loss: 4.6082 - acc: 0.038 - ETA: 1:42 - loss: 4.6082 - acc: 0.038 - ETA: 1:41 - loss: 4.6082 - acc: 0.038 - ETA: 1:40 - loss: 4.6076 - acc: 0.037 - ETA: 1:39 - loss: 4.6088 - acc: 0.037 - ETA: 1:38 - loss: 4.6091 - acc: 0.038 - ETA: 1:37 - loss: 4.6094 - acc: 0.037 - ETA: 1:36 - loss: 4.6107 - acc: 0.037 - ETA: 1:35 - loss: 4.6111 - acc: 0.037 - ETA: 1:34 - loss: 4.6113 - acc: 0.037 - ETA: 1:33 - loss: 4.6109 - acc: 0.037 - ETA: 1:32 - loss: 4.6104 - acc: 0.037 - ETA: 1:31 - loss: 4.6114 - acc: 0.037 - ETA: 1:30 - loss: 4.6121 - acc: 0.037 - ETA: 1:29 - loss: 4.6121 - acc: 0.037 - ETA: 1:28 - loss: 4.6111 - acc: 0.037 - ETA: 1:27 - loss: 4.6108 - acc: 0.037 - ETA: 1:26 - loss: 4.6100 - acc: 0.037 - ETA: 1:25 - loss: 4.6102 - acc: 0.037 - ETA: 1:24 - loss: 4.6106 - acc: 0.037 - ETA: 1:23 - loss: 4.6115 - acc: 0.037 - ETA: 1:22 - loss: 4.6117 - acc: 0.037 - ETA: 1:21 - loss: 4.6106 - acc: 0.037 - ETA: 1:20 - loss: 4.6111 - acc: 0.037 - ETA: 1:19 - loss: 4.6125 - acc: 0.037 - ETA: 1:18 - loss: 4.6125 - acc: 0.037 - ETA: 1:17 - loss: 4.6139 - acc: 0.037 - ETA: 1:16 - loss: 4.6141 - acc: 0.036 - ETA: 1:15 - loss: 4.6135 - acc: 0.037 - ETA: 1:14 - loss: 4.6136 - acc: 0.037 - ETA: 1:13 - loss: 4.6138 - acc: 0.036 - ETA: 1:12 - loss: 4.6133 - acc: 0.036 - ETA: 1:11 - loss: 4.6131 - acc: 0.036 - ETA: 1:10 - loss: 4.6119 - acc: 0.037 - ETA: 1:09 - loss: 4.6124 - acc: 0.036 - ETA: 1:08 - loss: 4.6129 - acc: 0.036 - ETA: 1:07 - loss: 4.6133 - acc: 0.036 - ETA: 1:06 - loss: 4.6133 - acc: 0.036 - ETA: 1:05 - loss: 4.6131 - acc: 0.036 - ETA: 1:04 - loss: 4.6141 - acc: 0.036 - ETA: 1:03 - loss: 4.6138 - acc: 0.037 - ETA: 1:02 - loss: 4.6139 - acc: 0.036 - ETA: 1:01 - loss: 4.6145 - acc: 0.036 - ETA: 1:00 - loss: 4.6140 - acc: 0.036 - ETA: 59s - loss: 4.6137 - acc: 0.036 - ETA: 58s - loss: 4.6133 - acc: 0.03 - ETA: 57s - loss: 4.6123 - acc: 0.03 - ETA: 56s - loss: 4.6136 - acc: 0.03 - ETA: 55s - loss: 4.6144 - acc: 0.03 - ETA: 54s - loss: 4.6157 - acc: 0.03 - ETA: 53s - loss: 4.6159 - acc: 0.03 - ETA: 52s - loss: 4.6162 - acc: 0.03 - ETA: 51s - loss: 4.6155 - acc: 0.03 - ETA: 50s - loss: 4.6165 - acc: 0.03 - ETA: 49s - loss: 4.6165 - acc: 0.03 - ETA: 48s - loss: 4.6170 - acc: 0.03 - ETA: 47s - loss: 4.6172 - acc: 0.03 - ETA: 46s - loss: 4.6174 - acc: 0.03 - ETA: 45s - loss: 4.6173 - acc: 0.03 - ETA: 44s - loss: 4.6175 - acc: 0.03 - ETA: 43s - loss: 4.6170 - acc: 0.03 - ETA: 42s - loss: 4.6161 - acc: 0.03 - ETA: 41s - loss: 4.6168 - acc: 0.03 - ETA: 40s - loss: 4.6170 - acc: 0.03 - ETA: 39s - loss: 4.6172 - acc: 0.03 - ETA: 38s - loss: 4.6168 - acc: 0.03 - ETA: 37s - loss: 4.6174 - acc: 0.03 - ETA: 36s - loss: 4.6176 - acc: 0.03 - ETA: 35s - loss: 4.6165 - acc: 0.03 - ETA: 34s - loss: 4.6164 - acc: 0.03 - ETA: 33s - loss: 4.6182 - acc: 0.03 - ETA: 32s - loss: 4.6182 - acc: 0.03 - ETA: 31s - loss: 4.6181 - acc: 0.03 - ETA: 30s - loss: 4.6182 - acc: 0.03 - ETA: 29s - loss: 4.6178 - acc: 0.03 - ETA: 28s - loss: 4.6175 - acc: 0.03 - ETA: 28s - loss: 4.6176 - acc: 0.03 - ETA: 27s - loss: 4.6178 - acc: 0.03 - ETA: 26s - loss: 4.6182 - acc: 0.03 - ETA: 25s - loss: 4.6191 - acc: 0.03 - ETA: 24s - loss: 4.6191 - acc: 0.03 - ETA: 23s - loss: 4.6190 - acc: 0.03 - ETA: 22s - loss: 4.6189 - acc: 0.03 - ETA: 21s - loss: 4.6190 - acc: 0.03 - ETA: 20s - loss: 4.6194 - acc: 0.03 - ETA: 19s - loss: 4.6203 - acc: 0.03 - ETA: 18s - loss: 4.6200 - acc: 0.03 - ETA: 17s - loss: 4.6194 - acc: 0.03 - ETA: 16s - loss: 4.6200 - acc: 0.03 - ETA: 15s - loss: 4.6200 - acc: 0.03 - ETA: 14s - loss: 4.6196 - acc: 0.03 - ETA: 13s - loss: 4.6197 - acc: 0.03 - ETA: 12s - loss: 4.6195 - acc: 0.03 - ETA: 11s - loss: 4.6200 - acc: 0.03 - ETA: 10s - loss: 4.6198 - acc: 0.03 - ETA: 9s - loss: 4.6193 - acc: 0.0369 - ETA: 8s - loss: 4.6190 - acc: 0.036 - ETA: 7s - loss: 4.6193 - acc: 0.036 - ETA: 6s - loss: 4.6188 - acc: 0.036 - ETA: 5s - loss: 4.6192 - acc: 0.036 - ETA: 4s - loss: 4.6197 - acc: 0.036 - ETA: 3s - loss: 4.6200 - acc: 0.036 - ETA: 2s - loss: 4.6196 - acc: 0.036 - ETA: 1s - loss: 4.6193 - acc: 0.036 - ETA: 0s - loss: 4.6193 - acc: 0.036 - 337s 50ms/step - loss: 4.6193 - acc: 0.0364 - val_loss: 4.6768 - val_acc: 0.0287\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.68758 to 4.67682, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 6:06 - loss: 4.6920 - acc: 0.0000e+0 - ETA: 5:29 - loss: 4.5702 - acc: 0.0250    - ETA: 5:20 - loss: 4.5426 - acc: 0.016 - ETA: 5:38 - loss: 4.4907 - acc: 0.012 - ETA: 5:35 - loss: 4.5154 - acc: 0.010 - ETA: 5:42 - loss: 4.4907 - acc: 0.025 - ETA: 5:41 - loss: 4.5069 - acc: 0.021 - ETA: 5:39 - loss: 4.5085 - acc: 0.031 - ETA: 5:34 - loss: 4.5175 - acc: 0.033 - ETA: 5:28 - loss: 4.5197 - acc: 0.030 - ETA: 5:23 - loss: 4.5372 - acc: 0.031 - ETA: 5:28 - loss: 4.5232 - acc: 0.029 - ETA: 5:29 - loss: 4.5357 - acc: 0.030 - ETA: 5:35 - loss: 4.5543 - acc: 0.028 - ETA: 5:37 - loss: 4.5516 - acc: 0.026 - ETA: 5:40 - loss: 4.5590 - acc: 0.025 - ETA: 5:40 - loss: 4.5667 - acc: 0.023 - ETA: 5:38 - loss: 4.5674 - acc: 0.025 - ETA: 5:37 - loss: 4.5772 - acc: 0.023 - ETA: 5:36 - loss: 4.5655 - acc: 0.025 - ETA: 5:34 - loss: 4.5776 - acc: 0.023 - ETA: 5:34 - loss: 4.5771 - acc: 0.022 - ETA: 5:31 - loss: 4.5683 - acc: 0.023 - ETA: 5:28 - loss: 4.5638 - acc: 0.022 - ETA: 5:24 - loss: 4.5644 - acc: 0.022 - ETA: 5:20 - loss: 4.5500 - acc: 0.025 - ETA: 5:17 - loss: 4.5498 - acc: 0.024 - ETA: 5:15 - loss: 4.5494 - acc: 0.025 - ETA: 5:11 - loss: 4.5494 - acc: 0.025 - ETA: 5:09 - loss: 4.5494 - acc: 0.025 - ETA: 5:07 - loss: 4.5573 - acc: 0.025 - ETA: 5:05 - loss: 4.5584 - acc: 0.026 - ETA: 5:04 - loss: 4.5561 - acc: 0.028 - ETA: 5:03 - loss: 4.5636 - acc: 0.030 - ETA: 5:04 - loss: 4.5580 - acc: 0.030 - ETA: 5:03 - loss: 4.5540 - acc: 0.029 - ETA: 5:01 - loss: 4.5530 - acc: 0.029 - ETA: 5:01 - loss: 4.5576 - acc: 0.028 - ETA: 4:59 - loss: 4.5560 - acc: 0.028 - ETA: 4:58 - loss: 4.5520 - acc: 0.031 - ETA: 4:57 - loss: 4.5528 - acc: 0.030 - ETA: 4:55 - loss: 4.5471 - acc: 0.033 - ETA: 4:54 - loss: 4.5484 - acc: 0.032 - ETA: 4:53 - loss: 4.5530 - acc: 0.031 - ETA: 4:51 - loss: 4.5518 - acc: 0.033 - ETA: 4:50 - loss: 4.5539 - acc: 0.033 - ETA: 4:49 - loss: 4.5509 - acc: 0.033 - ETA: 4:47 - loss: 4.5491 - acc: 0.032 - ETA: 4:45 - loss: 4.5503 - acc: 0.032 - ETA: 4:45 - loss: 4.5546 - acc: 0.033 - ETA: 4:45 - loss: 4.5564 - acc: 0.033 - ETA: 4:45 - loss: 4.5570 - acc: 0.034 - ETA: 4:44 - loss: 4.5541 - acc: 0.034 - ETA: 4:43 - loss: 4.5557 - acc: 0.035 - ETA: 4:43 - loss: 4.5574 - acc: 0.034 - ETA: 4:42 - loss: 4.5533 - acc: 0.035 - ETA: 4:42 - loss: 4.5508 - acc: 0.035 - ETA: 4:42 - loss: 4.5475 - acc: 0.036 - ETA: 4:41 - loss: 4.5443 - acc: 0.037 - ETA: 4:40 - loss: 4.5445 - acc: 0.037 - ETA: 4:39 - loss: 4.5470 - acc: 0.036 - ETA: 4:38 - loss: 4.5452 - acc: 0.037 - ETA: 4:37 - loss: 4.5441 - acc: 0.038 - ETA: 4:36 - loss: 4.5401 - acc: 0.037 - ETA: 4:35 - loss: 4.5429 - acc: 0.036 - ETA: 4:34 - loss: 4.5411 - acc: 0.037 - ETA: 4:33 - loss: 4.5434 - acc: 0.036 - ETA: 4:32 - loss: 4.5460 - acc: 0.036 - ETA: 4:31 - loss: 4.5529 - acc: 0.035 - ETA: 4:30 - loss: 4.5546 - acc: 0.035 - ETA: 4:29 - loss: 4.5565 - acc: 0.035 - ETA: 4:28 - loss: 4.5540 - acc: 0.036 - ETA: 4:26 - loss: 4.5542 - acc: 0.037 - ETA: 4:24 - loss: 4.5541 - acc: 0.037 - ETA: 4:23 - loss: 4.5552 - acc: 0.036 - ETA: 4:21 - loss: 4.5560 - acc: 0.036 - ETA: 4:20 - loss: 4.5538 - acc: 0.037 - ETA: 4:19 - loss: 4.5517 - acc: 0.037 - ETA: 4:19 - loss: 4.5557 - acc: 0.038 - ETA: 4:18 - loss: 4.5547 - acc: 0.038 - ETA: 4:17 - loss: 4.5521 - acc: 0.037 - ETA: 4:15 - loss: 4.5520 - acc: 0.037 - ETA: 4:15 - loss: 4.5571 - acc: 0.038 - ETA: 4:14 - loss: 4.5569 - acc: 0.037 - ETA: 4:13 - loss: 4.5583 - acc: 0.037 - ETA: 4:11 - loss: 4.5581 - acc: 0.036 - ETA: 4:10 - loss: 4.5574 - acc: 0.037 - ETA: 4:08 - loss: 4.5549 - acc: 0.037 - ETA: 4:07 - loss: 4.5585 - acc: 0.038 - ETA: 4:06 - loss: 4.5598 - acc: 0.037 - ETA: 4:05 - loss: 4.5584 - acc: 0.037 - ETA: 4:04 - loss: 4.5588 - acc: 0.037 - ETA: 4:03 - loss: 4.5644 - acc: 0.036 - ETA: 4:02 - loss: 4.5649 - acc: 0.036 - ETA: 4:01 - loss: 4.5638 - acc: 0.036 - ETA: 3:59 - loss: 4.5637 - acc: 0.036 - ETA: 3:58 - loss: 4.5616 - acc: 0.036 - ETA: 3:57 - loss: 4.5626 - acc: 0.037 - ETA: 3:55 - loss: 4.5662 - acc: 0.037 - ETA: 3:54 - loss: 4.5662 - acc: 0.038 - ETA: 3:53 - loss: 4.5675 - acc: 0.038 - ETA: 3:52 - loss: 4.5691 - acc: 0.037 - ETA: 3:50 - loss: 4.5702 - acc: 0.037 - ETA: 3:49 - loss: 4.5721 - acc: 0.037 - ETA: 3:48 - loss: 4.5739 - acc: 0.037 - ETA: 3:47 - loss: 4.5741 - acc: 0.038 - ETA: 3:45 - loss: 4.5735 - acc: 0.038 - ETA: 3:44 - loss: 4.5749 - acc: 0.038 - ETA: 3:43 - loss: 4.5754 - acc: 0.039 - ETA: 3:42 - loss: 4.5740 - acc: 0.040 - ETA: 3:41 - loss: 4.5736 - acc: 0.039 - ETA: 3:39 - loss: 4.5740 - acc: 0.039 - ETA: 3:38 - loss: 4.5720 - acc: 0.040 - ETA: 3:37 - loss: 4.5735 - acc: 0.039 - ETA: 3:36 - loss: 4.5729 - acc: 0.040 - ETA: 3:35 - loss: 4.5745 - acc: 0.039 - ETA: 3:34 - loss: 4.5739 - acc: 0.039 - ETA: 3:32 - loss: 4.5755 - acc: 0.039 - ETA: 3:31 - loss: 4.5764 - acc: 0.039 - ETA: 3:30 - loss: 4.5750 - acc: 0.040 - ETA: 3:29 - loss: 4.5749 - acc: 0.039 - ETA: 3:28 - loss: 4.5764 - acc: 0.039 - ETA: 3:27 - loss: 4.5764 - acc: 0.039 - ETA: 3:25 - loss: 4.5766 - acc: 0.039 - ETA: 3:24 - loss: 4.5781 - acc: 0.038 - ETA: 3:23 - loss: 4.5782 - acc: 0.038 - ETA: 3:22 - loss: 4.5791 - acc: 0.038 - ETA: 3:21 - loss: 4.5768 - acc: 0.039 - ETA: 3:20 - loss: 4.5779 - acc: 0.039 - ETA: 3:19 - loss: 4.5808 - acc: 0.038 - ETA: 3:18 - loss: 4.5831 - acc: 0.038 - ETA: 3:16 - loss: 4.5820 - acc: 0.039 - ETA: 3:15 - loss: 4.5832 - acc: 0.039 - ETA: 3:14 - loss: 4.5827 - acc: 0.039 - ETA: 3:13 - loss: 4.5835 - acc: 0.039 - ETA: 3:11 - loss: 4.5820 - acc: 0.040 - ETA: 3:10 - loss: 4.5812 - acc: 0.040 - ETA: 3:09 - loss: 4.5801 - acc: 0.040 - ETA: 3:08 - loss: 4.5807 - acc: 0.040 - ETA: 3:07 - loss: 4.5819 - acc: 0.040 - ETA: 3:06 - loss: 4.5808 - acc: 0.040 - ETA: 3:05 - loss: 4.5800 - acc: 0.040 - ETA: 3:04 - loss: 4.5787 - acc: 0.040 - ETA: 3:03 - loss: 4.5798 - acc: 0.039 - ETA: 3:02 - loss: 4.5805 - acc: 0.040 - ETA: 3:01 - loss: 4.5812 - acc: 0.039 - ETA: 2:59 - loss: 4.5801 - acc: 0.039 - ETA: 2:58 - loss: 4.5784 - acc: 0.039 - ETA: 2:57 - loss: 4.5779 - acc: 0.039 - ETA: 2:56 - loss: 4.5792 - acc: 0.039 - ETA: 2:55 - loss: 4.5810 - acc: 0.039 - ETA: 2:54 - loss: 4.5804 - acc: 0.039 - ETA: 2:53 - loss: 4.5812 - acc: 0.039 - ETA: 2:51 - loss: 4.5810 - acc: 0.039 - ETA: 2:50 - loss: 4.5804 - acc: 0.039 - ETA: 2:49 - loss: 4.5794 - acc: 0.038 - ETA: 2:48 - loss: 4.5810 - acc: 0.038 - ETA: 2:47 - loss: 4.5824 - acc: 0.038 - ETA: 2:46 - loss: 4.5812 - acc: 0.038 - ETA: 2:44 - loss: 4.5787 - acc: 0.038 - ETA: 2:43 - loss: 4.5796 - acc: 0.038 - ETA: 2:42 - loss: 4.5789 - acc: 0.038 - ETA: 2:41 - loss: 4.5781 - acc: 0.038 - ETA: 2:40 - loss: 4.5777 - acc: 0.038 - ETA: 2:39 - loss: 4.5776 - acc: 0.038 - ETA: 2:38 - loss: 4.5791 - acc: 0.038 - ETA: 2:37 - loss: 4.5795 - acc: 0.038 - ETA: 2:36 - loss: 4.5822 - acc: 0.038 - ETA: 2:35 - loss: 4.5825 - acc: 0.038 - ETA: 2:34 - loss: 4.5814 - acc: 0.039 - ETA: 2:33 - loss: 4.5810 - acc: 0.038 - ETA: 2:32 - loss: 4.5801 - acc: 0.039 - ETA: 2:31 - loss: 4.5801 - acc: 0.039 - ETA: 2:30 - loss: 4.5782 - acc: 0.039 - ETA: 2:29 - loss: 4.5783 - acc: 0.040 - ETA: 2:28 - loss: 4.5776 - acc: 0.040 - ETA: 2:27 - loss: 4.5785 - acc: 0.039 - ETA: 2:26 - loss: 4.5778 - acc: 0.039 - ETA: 2:25 - loss: 4.5770 - acc: 0.039 - ETA: 2:24 - loss: 4.5763 - acc: 0.040 - ETA: 2:23 - loss: 4.5766 - acc: 0.040 - ETA: 2:22 - loss: 4.5773 - acc: 0.040 - ETA: 2:21 - loss: 4.5777 - acc: 0.040 - ETA: 2:20 - loss: 4.5785 - acc: 0.039 - ETA: 2:19 - loss: 4.5777 - acc: 0.040 - ETA: 2:18 - loss: 4.5763 - acc: 0.040 - ETA: 2:17 - loss: 4.5762 - acc: 0.039 - ETA: 2:16 - loss: 4.5766 - acc: 0.040 - ETA: 2:15 - loss: 4.5751 - acc: 0.040 - ETA: 2:14 - loss: 4.5753 - acc: 0.040 - ETA: 2:13 - loss: 4.5764 - acc: 0.039 - ETA: 2:12 - loss: 4.5765 - acc: 0.039 - ETA: 2:11 - loss: 4.5814 - acc: 0.039 - ETA: 2:10 - loss: 4.5821 - acc: 0.039 - ETA: 2:09 - loss: 4.5822 - acc: 0.040 - ETA: 2:08 - loss: 4.5813 - acc: 0.039 - ETA: 2:07 - loss: 4.5829 - acc: 0.039 - ETA: 2:06 - loss: 4.5836 - acc: 0.039 - ETA: 2:05 - loss: 4.5846 - acc: 0.039 - ETA: 2:04 - loss: 4.5853 - acc: 0.039 - ETA: 2:03 - loss: 4.5861 - acc: 0.038 - ETA: 2:02 - loss: 4.5862 - acc: 0.038 - ETA: 2:01 - loss: 4.5865 - acc: 0.038 - ETA: 2:00 - loss: 4.5871 - acc: 0.0385"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1:59 - loss: 4.5874 - acc: 0.038 - ETA: 1:58 - loss: 4.5872 - acc: 0.038 - ETA: 1:57 - loss: 4.5875 - acc: 0.038 - ETA: 1:56 - loss: 4.5871 - acc: 0.038 - ETA: 1:55 - loss: 4.5868 - acc: 0.037 - ETA: 1:54 - loss: 4.5874 - acc: 0.037 - ETA: 1:53 - loss: 4.5876 - acc: 0.037 - ETA: 1:52 - loss: 4.5886 - acc: 0.037 - ETA: 1:51 - loss: 4.5880 - acc: 0.037 - ETA: 1:50 - loss: 4.5876 - acc: 0.037 - ETA: 1:49 - loss: 4.5870 - acc: 0.037 - ETA: 1:48 - loss: 4.5869 - acc: 0.038 - ETA: 1:47 - loss: 4.5869 - acc: 0.038 - ETA: 1:47 - loss: 4.5862 - acc: 0.038 - ETA: 1:46 - loss: 4.5857 - acc: 0.038 - ETA: 1:45 - loss: 4.5854 - acc: 0.038 - ETA: 1:44 - loss: 4.5852 - acc: 0.037 - ETA: 1:43 - loss: 4.5839 - acc: 0.038 - ETA: 1:42 - loss: 4.5832 - acc: 0.038 - ETA: 1:41 - loss: 4.5831 - acc: 0.038 - ETA: 1:40 - loss: 4.5833 - acc: 0.038 - ETA: 1:39 - loss: 4.5835 - acc: 0.038 - ETA: 1:38 - loss: 4.5840 - acc: 0.038 - ETA: 1:37 - loss: 4.5825 - acc: 0.038 - ETA: 1:36 - loss: 4.5840 - acc: 0.038 - ETA: 1:35 - loss: 4.5848 - acc: 0.038 - ETA: 1:34 - loss: 4.5853 - acc: 0.038 - ETA: 1:33 - loss: 4.5853 - acc: 0.038 - ETA: 1:32 - loss: 4.5860 - acc: 0.038 - ETA: 1:31 - loss: 4.5855 - acc: 0.038 - ETA: 1:30 - loss: 4.5859 - acc: 0.038 - ETA: 1:29 - loss: 4.5863 - acc: 0.039 - ETA: 1:28 - loss: 4.5868 - acc: 0.038 - ETA: 1:27 - loss: 4.5876 - acc: 0.038 - ETA: 1:26 - loss: 4.5874 - acc: 0.038 - ETA: 1:25 - loss: 4.5880 - acc: 0.038 - ETA: 1:25 - loss: 4.5876 - acc: 0.038 - ETA: 1:24 - loss: 4.5877 - acc: 0.038 - ETA: 1:23 - loss: 4.5869 - acc: 0.038 - ETA: 1:22 - loss: 4.5866 - acc: 0.037 - ETA: 1:21 - loss: 4.5874 - acc: 0.038 - ETA: 1:20 - loss: 4.5873 - acc: 0.037 - ETA: 1:19 - loss: 4.5874 - acc: 0.037 - ETA: 1:18 - loss: 4.5876 - acc: 0.038 - ETA: 1:17 - loss: 4.5883 - acc: 0.038 - ETA: 1:16 - loss: 4.5893 - acc: 0.037 - ETA: 1:15 - loss: 4.5897 - acc: 0.037 - ETA: 1:14 - loss: 4.5903 - acc: 0.037 - ETA: 1:13 - loss: 4.5909 - acc: 0.037 - ETA: 1:12 - loss: 4.5900 - acc: 0.037 - ETA: 1:11 - loss: 4.5899 - acc: 0.037 - ETA: 1:10 - loss: 4.5910 - acc: 0.037 - ETA: 1:10 - loss: 4.5911 - acc: 0.037 - ETA: 1:09 - loss: 4.5918 - acc: 0.037 - ETA: 1:08 - loss: 4.5917 - acc: 0.038 - ETA: 1:07 - loss: 4.5921 - acc: 0.037 - ETA: 1:06 - loss: 4.5919 - acc: 0.038 - ETA: 1:05 - loss: 4.5925 - acc: 0.038 - ETA: 1:04 - loss: 4.5933 - acc: 0.038 - ETA: 1:03 - loss: 4.5926 - acc: 0.037 - ETA: 1:02 - loss: 4.5932 - acc: 0.037 - ETA: 1:01 - loss: 4.5919 - acc: 0.037 - ETA: 1:00 - loss: 4.5912 - acc: 0.037 - ETA: 59s - loss: 4.5929 - acc: 0.037 - ETA: 58s - loss: 4.5926 - acc: 0.03 - ETA: 57s - loss: 4.5931 - acc: 0.03 - ETA: 56s - loss: 4.5928 - acc: 0.03 - ETA: 55s - loss: 4.5931 - acc: 0.03 - ETA: 55s - loss: 4.5939 - acc: 0.03 - ETA: 54s - loss: 4.5936 - acc: 0.03 - ETA: 53s - loss: 4.5943 - acc: 0.03 - ETA: 52s - loss: 4.5933 - acc: 0.03 - ETA: 51s - loss: 4.5929 - acc: 0.03 - ETA: 50s - loss: 4.5918 - acc: 0.03 - ETA: 49s - loss: 4.5920 - acc: 0.03 - ETA: 48s - loss: 4.5920 - acc: 0.03 - ETA: 47s - loss: 4.5916 - acc: 0.03 - ETA: 46s - loss: 4.5908 - acc: 0.03 - ETA: 45s - loss: 4.5908 - acc: 0.03 - ETA: 44s - loss: 4.5911 - acc: 0.03 - ETA: 43s - loss: 4.5906 - acc: 0.03 - ETA: 43s - loss: 4.5916 - acc: 0.03 - ETA: 42s - loss: 4.5914 - acc: 0.03 - ETA: 41s - loss: 4.5920 - acc: 0.03 - ETA: 40s - loss: 4.5921 - acc: 0.03 - ETA: 39s - loss: 4.5921 - acc: 0.03 - ETA: 38s - loss: 4.5917 - acc: 0.03 - ETA: 37s - loss: 4.5918 - acc: 0.03 - ETA: 36s - loss: 4.5915 - acc: 0.03 - ETA: 35s - loss: 4.5911 - acc: 0.03 - ETA: 34s - loss: 4.5914 - acc: 0.03 - ETA: 33s - loss: 4.5901 - acc: 0.03 - ETA: 33s - loss: 4.5897 - acc: 0.03 - ETA: 32s - loss: 4.5892 - acc: 0.03 - ETA: 31s - loss: 4.5892 - acc: 0.03 - ETA: 30s - loss: 4.5892 - acc: 0.03 - ETA: 29s - loss: 4.5893 - acc: 0.03 - ETA: 28s - loss: 4.5883 - acc: 0.03 - ETA: 27s - loss: 4.5890 - acc: 0.03 - ETA: 26s - loss: 4.5884 - acc: 0.03 - ETA: 25s - loss: 4.5891 - acc: 0.03 - ETA: 25s - loss: 4.5890 - acc: 0.03 - ETA: 24s - loss: 4.5887 - acc: 0.03 - ETA: 23s - loss: 4.5884 - acc: 0.03 - ETA: 22s - loss: 4.5887 - acc: 0.03 - ETA: 21s - loss: 4.5891 - acc: 0.03 - ETA: 20s - loss: 4.5882 - acc: 0.03 - ETA: 19s - loss: 4.5884 - acc: 0.03 - ETA: 18s - loss: 4.5881 - acc: 0.03 - ETA: 17s - loss: 4.5874 - acc: 0.04 - ETA: 16s - loss: 4.5875 - acc: 0.04 - ETA: 16s - loss: 4.5874 - acc: 0.04 - ETA: 15s - loss: 4.5880 - acc: 0.04 - ETA: 14s - loss: 4.5885 - acc: 0.03 - ETA: 13s - loss: 4.5889 - acc: 0.04 - ETA: 12s - loss: 4.5882 - acc: 0.04 - ETA: 11s - loss: 4.5881 - acc: 0.04 - ETA: 10s - loss: 4.5892 - acc: 0.04 - ETA: 9s - loss: 4.5889 - acc: 0.0402 - ETA: 8s - loss: 4.5891 - acc: 0.040 - ETA: 8s - loss: 4.5894 - acc: 0.040 - ETA: 7s - loss: 4.5905 - acc: 0.040 - ETA: 6s - loss: 4.5910 - acc: 0.040 - ETA: 5s - loss: 4.5913 - acc: 0.040 - ETA: 4s - loss: 4.5913 - acc: 0.040 - ETA: 3s - loss: 4.5918 - acc: 0.040 - ETA: 2s - loss: 4.5926 - acc: 0.040 - ETA: 1s - loss: 4.5927 - acc: 0.040 - ETA: 0s - loss: 4.5934 - acc: 0.040 - 309s 46ms/step - loss: 4.5934 - acc: 0.0401 - val_loss: 4.6619 - val_acc: 0.0287\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.67682 to 4.66195, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad1a31bc18>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 3.3493%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 5:45 - loss: 14.4888 - acc: 0.05 - ETA: 44s - loss: 15.3952 - acc: 0.0063 - ETA: 24s - loss: 14.9634 - acc: 0.010 - ETA: 17s - loss: 14.7342 - acc: 0.011 - ETA: 13s - loss: 14.4828 - acc: 0.015 - ETA: 10s - loss: 14.3095 - acc: 0.022 - ETA: 9s - loss: 14.0672 - acc: 0.032 - ETA: 8s - loss: 13.8887 - acc: 0.03 - ETA: 7s - loss: 13.7539 - acc: 0.04 - ETA: 6s - loss: 13.7469 - acc: 0.04 - ETA: 5s - loss: 13.6611 - acc: 0.04 - ETA: 5s - loss: 13.6005 - acc: 0.04 - ETA: 4s - loss: 13.5022 - acc: 0.05 - ETA: 4s - loss: 13.4170 - acc: 0.05 - ETA: 4s - loss: 13.3820 - acc: 0.05 - ETA: 4s - loss: 13.3558 - acc: 0.05 - ETA: 3s - loss: 13.2947 - acc: 0.06 - ETA: 3s - loss: 13.2566 - acc: 0.06 - ETA: 3s - loss: 13.1915 - acc: 0.06 - ETA: 3s - loss: 13.1046 - acc: 0.06 - ETA: 2s - loss: 13.0487 - acc: 0.07 - ETA: 2s - loss: 13.0071 - acc: 0.07 - ETA: 2s - loss: 12.9484 - acc: 0.07 - ETA: 2s - loss: 12.9433 - acc: 0.07 - ETA: 2s - loss: 12.8840 - acc: 0.08 - ETA: 2s - loss: 12.8575 - acc: 0.08 - ETA: 2s - loss: 12.7851 - acc: 0.08 - ETA: 1s - loss: 12.7179 - acc: 0.08 - ETA: 1s - loss: 12.6182 - acc: 0.09 - ETA: 1s - loss: 12.5817 - acc: 0.09 - ETA: 1s - loss: 12.5347 - acc: 0.10 - ETA: 1s - loss: 12.4919 - acc: 0.10 - ETA: 1s - loss: 12.4427 - acc: 0.10 - ETA: 1s - loss: 12.3832 - acc: 0.11 - ETA: 1s - loss: 12.3013 - acc: 0.11 - ETA: 1s - loss: 12.2673 - acc: 0.11 - ETA: 0s - loss: 12.2294 - acc: 0.11 - ETA: 0s - loss: 12.1864 - acc: 0.11 - ETA: 0s - loss: 12.1401 - acc: 0.12 - ETA: 0s - loss: 12.1362 - acc: 0.12 - ETA: 0s - loss: 12.0907 - acc: 0.12 - ETA: 0s - loss: 12.0495 - acc: 0.12 - ETA: 0s - loss: 12.0120 - acc: 0.13 - ETA: 0s - loss: 11.9726 - acc: 0.13 - ETA: 0s - loss: 11.9132 - acc: 0.13 - ETA: 0s - loss: 11.8824 - acc: 0.13 - ETA: 0s - loss: 11.8292 - acc: 0.14 - ETA: 0s - loss: 11.7684 - acc: 0.14 - 4s 572us/step - loss: 11.7497 - acc: 0.1478 - val_loss: 9.9546 - val_acc: 0.2587\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.95461, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 8.8309 - acc: 0.300 - ETA: 2s - loss: 9.2000 - acc: 0.333 - ETA: 2s - loss: 9.0207 - acc: 0.331 - ETA: 2s - loss: 9.1394 - acc: 0.336 - ETA: 2s - loss: 9.4075 - acc: 0.320 - ETA: 2s - loss: 9.5255 - acc: 0.315 - ETA: 2s - loss: 9.6316 - acc: 0.309 - ETA: 1s - loss: 9.5177 - acc: 0.316 - ETA: 1s - loss: 9.4190 - acc: 0.318 - ETA: 1s - loss: 9.3912 - acc: 0.320 - ETA: 1s - loss: 9.2008 - acc: 0.330 - ETA: 1s - loss: 9.3413 - acc: 0.324 - ETA: 1s - loss: 9.3569 - acc: 0.320 - ETA: 1s - loss: 9.3182 - acc: 0.320 - ETA: 1s - loss: 9.3186 - acc: 0.321 - ETA: 1s - loss: 9.3685 - acc: 0.316 - ETA: 1s - loss: 9.3261 - acc: 0.316 - ETA: 1s - loss: 9.4149 - acc: 0.311 - ETA: 1s - loss: 9.3917 - acc: 0.311 - ETA: 1s - loss: 9.4167 - acc: 0.311 - ETA: 1s - loss: 9.3947 - acc: 0.309 - ETA: 1s - loss: 9.4021 - acc: 0.309 - ETA: 1s - loss: 9.3873 - acc: 0.310 - ETA: 1s - loss: 9.3427 - acc: 0.312 - ETA: 1s - loss: 9.3514 - acc: 0.311 - ETA: 1s - loss: 9.3580 - acc: 0.311 - ETA: 1s - loss: 9.3376 - acc: 0.314 - ETA: 0s - loss: 9.3196 - acc: 0.314 - ETA: 0s - loss: 9.3071 - acc: 0.314 - ETA: 0s - loss: 9.2925 - acc: 0.316 - ETA: 0s - loss: 9.3084 - acc: 0.316 - ETA: 0s - loss: 9.3226 - acc: 0.315 - ETA: 0s - loss: 9.2928 - acc: 0.317 - ETA: 0s - loss: 9.2721 - acc: 0.319 - ETA: 0s - loss: 9.2465 - acc: 0.320 - ETA: 0s - loss: 9.2596 - acc: 0.320 - ETA: 0s - loss: 9.2430 - acc: 0.321 - ETA: 0s - loss: 9.2400 - acc: 0.322 - ETA: 0s - loss: 9.2277 - acc: 0.322 - ETA: 0s - loss: 9.2351 - acc: 0.321 - ETA: 0s - loss: 9.2321 - acc: 0.321 - ETA: 0s - loss: 9.2301 - acc: 0.322 - ETA: 0s - loss: 9.2082 - acc: 0.324 - ETA: 0s - loss: 9.1857 - acc: 0.326 - ETA: 0s - loss: 9.2169 - acc: 0.325 - ETA: 0s - loss: 9.2053 - acc: 0.326 - ETA: 0s - loss: 9.1897 - acc: 0.327 - ETA: 0s - loss: 9.1912 - acc: 0.327 - 3s 404us/step - loss: 9.1929 - acc: 0.3274 - val_loss: 9.1360 - val_acc: 0.3317\n",
      "\n",
      "Epoch 00002: val_loss improved from 9.95461 to 9.13601, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 6.2328 - acc: 0.550 - ETA: 2s - loss: 7.7665 - acc: 0.444 - ETA: 2s - loss: 8.2514 - acc: 0.425 - ETA: 2s - loss: 8.3907 - acc: 0.409 - ETA: 2s - loss: 8.6071 - acc: 0.394 - ETA: 2s - loss: 8.7056 - acc: 0.392 - ETA: 2s - loss: 8.6029 - acc: 0.395 - ETA: 2s - loss: 8.4518 - acc: 0.404 - ETA: 2s - loss: 8.4911 - acc: 0.401 - ETA: 2s - loss: 8.4123 - acc: 0.409 - ETA: 2s - loss: 8.3423 - acc: 0.412 - ETA: 2s - loss: 8.3815 - acc: 0.406 - ETA: 1s - loss: 8.4152 - acc: 0.402 - ETA: 1s - loss: 8.3920 - acc: 0.401 - ETA: 1s - loss: 8.3485 - acc: 0.407 - ETA: 1s - loss: 8.3571 - acc: 0.409 - ETA: 1s - loss: 8.3936 - acc: 0.407 - ETA: 1s - loss: 8.4017 - acc: 0.406 - ETA: 1s - loss: 8.4191 - acc: 0.404 - ETA: 1s - loss: 8.3924 - acc: 0.403 - ETA: 1s - loss: 8.3579 - acc: 0.405 - ETA: 1s - loss: 8.3534 - acc: 0.405 - ETA: 1s - loss: 8.3410 - acc: 0.407 - ETA: 1s - loss: 8.3195 - acc: 0.409 - ETA: 1s - loss: 8.3310 - acc: 0.409 - ETA: 1s - loss: 8.3620 - acc: 0.406 - ETA: 1s - loss: 8.3658 - acc: 0.406 - ETA: 1s - loss: 8.3640 - acc: 0.406 - ETA: 1s - loss: 8.3469 - acc: 0.407 - ETA: 1s - loss: 8.3729 - acc: 0.405 - ETA: 0s - loss: 8.3675 - acc: 0.405 - ETA: 0s - loss: 8.4045 - acc: 0.403 - ETA: 0s - loss: 8.4247 - acc: 0.402 - ETA: 0s - loss: 8.4417 - acc: 0.402 - ETA: 0s - loss: 8.4511 - acc: 0.402 - ETA: 0s - loss: 8.4380 - acc: 0.402 - ETA: 0s - loss: 8.4186 - acc: 0.404 - ETA: 0s - loss: 8.4023 - acc: 0.405 - ETA: 0s - loss: 8.4278 - acc: 0.403 - ETA: 0s - loss: 8.4338 - acc: 0.404 - ETA: 0s - loss: 8.4184 - acc: 0.405 - ETA: 0s - loss: 8.4357 - acc: 0.404 - ETA: 0s - loss: 8.4522 - acc: 0.403 - ETA: 0s - loss: 8.4252 - acc: 0.404 - ETA: 0s - loss: 8.4198 - acc: 0.405 - ETA: 0s - loss: 8.4381 - acc: 0.404 - ETA: 0s - loss: 8.4306 - acc: 0.404 - ETA: 0s - loss: 8.4178 - acc: 0.406 - ETA: 0s - loss: 8.4270 - acc: 0.405 - 3s 416us/step - loss: 8.4354 - acc: 0.4051 - val_loss: 8.6515 - val_acc: 0.3760\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.13601 to 8.65150, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 6.4733 - acc: 0.600 - ETA: 2s - loss: 7.9750 - acc: 0.444 - ETA: 2s - loss: 7.4832 - acc: 0.471 - ETA: 2s - loss: 8.2022 - acc: 0.433 - ETA: 2s - loss: 7.7681 - acc: 0.456 - ETA: 2s - loss: 7.8619 - acc: 0.455 - ETA: 2s - loss: 7.8850 - acc: 0.453 - ETA: 2s - loss: 7.9450 - acc: 0.451 - ETA: 1s - loss: 7.8523 - acc: 0.454 - ETA: 1s - loss: 7.8646 - acc: 0.455 - ETA: 1s - loss: 7.8558 - acc: 0.457 - ETA: 1s - loss: 7.8362 - acc: 0.458 - ETA: 1s - loss: 7.7956 - acc: 0.463 - ETA: 1s - loss: 7.7874 - acc: 0.463 - ETA: 1s - loss: 7.8978 - acc: 0.457 - ETA: 1s - loss: 7.9395 - acc: 0.456 - ETA: 1s - loss: 7.9325 - acc: 0.456 - ETA: 1s - loss: 7.9097 - acc: 0.457 - ETA: 1s - loss: 7.8885 - acc: 0.457 - ETA: 1s - loss: 7.8862 - acc: 0.457 - ETA: 1s - loss: 7.8636 - acc: 0.459 - ETA: 1s - loss: 7.8720 - acc: 0.459 - ETA: 1s - loss: 7.8558 - acc: 0.460 - ETA: 1s - loss: 7.8872 - acc: 0.457 - ETA: 1s - loss: 7.8575 - acc: 0.458 - ETA: 1s - loss: 7.8518 - acc: 0.458 - ETA: 1s - loss: 7.8336 - acc: 0.459 - ETA: 1s - loss: 7.8125 - acc: 0.461 - ETA: 1s - loss: 7.8552 - acc: 0.459 - ETA: 0s - loss: 7.8832 - acc: 0.458 - ETA: 0s - loss: 7.9041 - acc: 0.456 - ETA: 0s - loss: 7.9092 - acc: 0.457 - ETA: 0s - loss: 7.9348 - acc: 0.455 - ETA: 0s - loss: 7.9367 - acc: 0.455 - ETA: 0s - loss: 7.9438 - acc: 0.456 - ETA: 0s - loss: 7.9676 - acc: 0.454 - ETA: 0s - loss: 8.0042 - acc: 0.452 - ETA: 0s - loss: 8.0192 - acc: 0.452 - ETA: 0s - loss: 8.0146 - acc: 0.452 - ETA: 0s - loss: 8.0004 - acc: 0.453 - ETA: 0s - loss: 8.0110 - acc: 0.452 - ETA: 0s - loss: 8.0182 - acc: 0.452 - ETA: 0s - loss: 8.0246 - acc: 0.451 - ETA: 0s - loss: 8.0052 - acc: 0.452 - ETA: 0s - loss: 8.0286 - acc: 0.451 - ETA: 0s - loss: 8.0367 - acc: 0.450 - ETA: 0s - loss: 8.0367 - acc: 0.450 - 3s 402us/step - loss: 8.0076 - acc: 0.4528 - val_loss: 8.4459 - val_acc: 0.3976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_loss improved from 8.65150 to 8.44587, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 8.2262 - acc: 0.450 - ETA: 2s - loss: 7.0698 - acc: 0.522 - ETA: 2s - loss: 7.6006 - acc: 0.496 - ETA: 2s - loss: 7.5062 - acc: 0.502 - ETA: 2s - loss: 7.4790 - acc: 0.503 - ETA: 2s - loss: 7.5076 - acc: 0.501 - ETA: 2s - loss: 7.6131 - acc: 0.497 - ETA: 2s - loss: 7.5787 - acc: 0.496 - ETA: 2s - loss: 7.5827 - acc: 0.495 - ETA: 1s - loss: 7.6308 - acc: 0.492 - ETA: 1s - loss: 7.6642 - acc: 0.491 - ETA: 1s - loss: 7.6239 - acc: 0.495 - ETA: 1s - loss: 7.6563 - acc: 0.491 - ETA: 1s - loss: 7.6740 - acc: 0.489 - ETA: 1s - loss: 7.6816 - acc: 0.490 - ETA: 1s - loss: 7.7017 - acc: 0.488 - ETA: 1s - loss: 7.6642 - acc: 0.489 - ETA: 1s - loss: 7.7011 - acc: 0.486 - ETA: 1s - loss: 7.6860 - acc: 0.485 - ETA: 1s - loss: 7.7373 - acc: 0.480 - ETA: 1s - loss: 7.8145 - acc: 0.475 - ETA: 1s - loss: 7.8339 - acc: 0.474 - ETA: 1s - loss: 7.8780 - acc: 0.471 - ETA: 1s - loss: 7.8757 - acc: 0.470 - ETA: 1s - loss: 7.8710 - acc: 0.470 - ETA: 1s - loss: 7.9015 - acc: 0.469 - ETA: 1s - loss: 7.9136 - acc: 0.469 - ETA: 1s - loss: 7.9015 - acc: 0.470 - ETA: 1s - loss: 7.9366 - acc: 0.469 - ETA: 0s - loss: 7.9410 - acc: 0.469 - ETA: 0s - loss: 7.9151 - acc: 0.471 - ETA: 0s - loss: 7.8861 - acc: 0.473 - ETA: 0s - loss: 7.9066 - acc: 0.472 - ETA: 0s - loss: 7.9203 - acc: 0.472 - ETA: 0s - loss: 7.9285 - acc: 0.470 - ETA: 0s - loss: 7.9273 - acc: 0.471 - ETA: 0s - loss: 7.9390 - acc: 0.470 - ETA: 0s - loss: 7.9073 - acc: 0.471 - ETA: 0s - loss: 7.8896 - acc: 0.472 - ETA: 0s - loss: 7.9048 - acc: 0.471 - ETA: 0s - loss: 7.9317 - acc: 0.469 - ETA: 0s - loss: 7.9363 - acc: 0.469 - ETA: 0s - loss: 7.9352 - acc: 0.469 - ETA: 0s - loss: 7.9094 - acc: 0.471 - ETA: 0s - loss: 7.8892 - acc: 0.472 - ETA: 0s - loss: 7.8758 - acc: 0.473 - ETA: 0s - loss: 7.8518 - acc: 0.474 - ETA: 0s - loss: 7.8450 - acc: 0.475 - ETA: 0s - loss: 7.8338 - acc: 0.475 - 3s 407us/step - loss: 7.8273 - acc: 0.4757 - val_loss: 8.3522 - val_acc: 0.4120\n",
      "\n",
      "Epoch 00005: val_loss improved from 8.44587 to 8.35216, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 5.0351 - acc: 0.650 - ETA: 2s - loss: 6.6370 - acc: 0.556 - ETA: 2s - loss: 7.5409 - acc: 0.510 - ETA: 2s - loss: 7.7131 - acc: 0.502 - ETA: 2s - loss: 7.4780 - acc: 0.517 - ETA: 2s - loss: 7.4127 - acc: 0.518 - ETA: 2s - loss: 7.3472 - acc: 0.520 - ETA: 2s - loss: 7.3823 - acc: 0.519 - ETA: 2s - loss: 7.4850 - acc: 0.515 - ETA: 2s - loss: 7.4380 - acc: 0.515 - ETA: 2s - loss: 7.3950 - acc: 0.518 - ETA: 2s - loss: 7.4505 - acc: 0.515 - ETA: 2s - loss: 7.4817 - acc: 0.514 - ETA: 1s - loss: 7.4737 - acc: 0.513 - ETA: 1s - loss: 7.4892 - acc: 0.512 - ETA: 1s - loss: 7.4585 - acc: 0.511 - ETA: 1s - loss: 7.5320 - acc: 0.506 - ETA: 1s - loss: 7.5941 - acc: 0.503 - ETA: 1s - loss: 7.5566 - acc: 0.506 - ETA: 1s - loss: 7.5448 - acc: 0.506 - ETA: 1s - loss: 7.5970 - acc: 0.503 - ETA: 1s - loss: 7.6214 - acc: 0.502 - ETA: 1s - loss: 7.6828 - acc: 0.498 - ETA: 1s - loss: 7.6801 - acc: 0.496 - ETA: 1s - loss: 7.7114 - acc: 0.493 - ETA: 1s - loss: 7.6847 - acc: 0.495 - ETA: 1s - loss: 7.6648 - acc: 0.497 - ETA: 1s - loss: 7.6732 - acc: 0.495 - ETA: 0s - loss: 7.6903 - acc: 0.494 - ETA: 0s - loss: 7.7001 - acc: 0.493 - ETA: 0s - loss: 7.7116 - acc: 0.493 - ETA: 0s - loss: 7.6718 - acc: 0.496 - ETA: 0s - loss: 7.6842 - acc: 0.495 - ETA: 0s - loss: 7.6878 - acc: 0.495 - ETA: 0s - loss: 7.6934 - acc: 0.494 - ETA: 0s - loss: 7.7261 - acc: 0.493 - ETA: 0s - loss: 7.7095 - acc: 0.493 - ETA: 0s - loss: 7.7033 - acc: 0.493 - ETA: 0s - loss: 7.6756 - acc: 0.494 - ETA: 0s - loss: 7.6673 - acc: 0.494 - ETA: 0s - loss: 7.6665 - acc: 0.494 - ETA: 0s - loss: 7.6764 - acc: 0.494 - ETA: 0s - loss: 7.6824 - acc: 0.493 - ETA: 0s - loss: 7.6639 - acc: 0.494 - ETA: 0s - loss: 7.6561 - acc: 0.495 - 3s 382us/step - loss: 7.6515 - acc: 0.4952 - val_loss: 8.1424 - val_acc: 0.4168\n",
      "\n",
      "Epoch 00006: val_loss improved from 8.35216 to 8.14243, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 8.0593 - acc: 0.500 - ETA: 2s - loss: 8.6688 - acc: 0.455 - ETA: 2s - loss: 8.3209 - acc: 0.461 - ETA: 2s - loss: 7.9824 - acc: 0.490 - ETA: 1s - loss: 7.6396 - acc: 0.509 - ETA: 1s - loss: 7.5408 - acc: 0.515 - ETA: 1s - loss: 7.4052 - acc: 0.520 - ETA: 1s - loss: 7.4152 - acc: 0.521 - ETA: 1s - loss: 7.4234 - acc: 0.519 - ETA: 1s - loss: 7.4774 - acc: 0.516 - ETA: 1s - loss: 7.5937 - acc: 0.508 - ETA: 1s - loss: 7.5860 - acc: 0.510 - ETA: 1s - loss: 7.5677 - acc: 0.510 - ETA: 1s - loss: 7.6136 - acc: 0.506 - ETA: 1s - loss: 7.5894 - acc: 0.505 - ETA: 1s - loss: 7.4993 - acc: 0.512 - ETA: 1s - loss: 7.4259 - acc: 0.516 - ETA: 1s - loss: 7.4028 - acc: 0.517 - ETA: 1s - loss: 7.4195 - acc: 0.515 - ETA: 1s - loss: 7.3936 - acc: 0.517 - ETA: 1s - loss: 7.4105 - acc: 0.516 - ETA: 1s - loss: 7.3840 - acc: 0.518 - ETA: 1s - loss: 7.4275 - acc: 0.515 - ETA: 1s - loss: 7.4465 - acc: 0.513 - ETA: 1s - loss: 7.4558 - acc: 0.512 - ETA: 0s - loss: 7.4477 - acc: 0.513 - ETA: 0s - loss: 7.4660 - acc: 0.511 - ETA: 0s - loss: 7.4669 - acc: 0.512 - ETA: 0s - loss: 7.4797 - acc: 0.510 - ETA: 0s - loss: 7.4600 - acc: 0.511 - ETA: 0s - loss: 7.4334 - acc: 0.512 - ETA: 0s - loss: 7.3879 - acc: 0.514 - ETA: 0s - loss: 7.3993 - acc: 0.513 - ETA: 0s - loss: 7.4689 - acc: 0.509 - ETA: 0s - loss: 7.4831 - acc: 0.508 - ETA: 0s - loss: 7.4722 - acc: 0.509 - ETA: 0s - loss: 7.4872 - acc: 0.508 - ETA: 0s - loss: 7.4792 - acc: 0.508 - ETA: 0s - loss: 7.4803 - acc: 0.507 - ETA: 0s - loss: 7.4796 - acc: 0.506 - ETA: 0s - loss: 7.4937 - acc: 0.505 - ETA: 0s - loss: 7.4858 - acc: 0.506 - ETA: 0s - loss: 7.4879 - acc: 0.506 - ETA: 0s - loss: 7.4837 - acc: 0.506 - ETA: 0s - loss: 7.4793 - acc: 0.506 - ETA: 0s - loss: 7.4796 - acc: 0.506 - ETA: 0s - loss: 7.4585 - acc: 0.507 - 3s 402us/step - loss: 7.4597 - acc: 0.5076 - val_loss: 8.1051 - val_acc: 0.4084\n",
      "\n",
      "Epoch 00007: val_loss improved from 8.14243 to 8.10514, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 1s - loss: 7.8220 - acc: 0.400 - ETA: 2s - loss: 8.2868 - acc: 0.438 - ETA: 2s - loss: 7.6618 - acc: 0.482 - ETA: 2s - loss: 7.6539 - acc: 0.486 - ETA: 2s - loss: 7.5159 - acc: 0.495 - ETA: 1s - loss: 7.6101 - acc: 0.496 - ETA: 1s - loss: 7.5292 - acc: 0.504 - ETA: 1s - loss: 7.4034 - acc: 0.512 - ETA: 1s - loss: 7.2766 - acc: 0.520 - ETA: 1s - loss: 7.4439 - acc: 0.510 - ETA: 1s - loss: 7.3317 - acc: 0.519 - ETA: 1s - loss: 7.3191 - acc: 0.519 - ETA: 1s - loss: 7.3478 - acc: 0.517 - ETA: 1s - loss: 7.2937 - acc: 0.520 - ETA: 1s - loss: 7.2414 - acc: 0.522 - ETA: 1s - loss: 7.2190 - acc: 0.523 - ETA: 1s - loss: 7.1966 - acc: 0.526 - ETA: 1s - loss: 7.1829 - acc: 0.525 - ETA: 1s - loss: 7.1154 - acc: 0.529 - ETA: 1s - loss: 7.1152 - acc: 0.530 - ETA: 1s - loss: 7.0815 - acc: 0.532 - ETA: 1s - loss: 7.0761 - acc: 0.532 - ETA: 1s - loss: 7.1298 - acc: 0.529 - ETA: 1s - loss: 7.1330 - acc: 0.528 - ETA: 1s - loss: 7.1419 - acc: 0.528 - ETA: 1s - loss: 7.1242 - acc: 0.528 - ETA: 1s - loss: 7.1413 - acc: 0.527 - ETA: 0s - loss: 7.1752 - acc: 0.525 - ETA: 0s - loss: 7.1860 - acc: 0.523 - ETA: 0s - loss: 7.1868 - acc: 0.524 - ETA: 0s - loss: 7.2173 - acc: 0.522 - ETA: 0s - loss: 7.2397 - acc: 0.520 - ETA: 0s - loss: 7.2099 - acc: 0.522 - ETA: 0s - loss: 7.2370 - acc: 0.520 - ETA: 0s - loss: 7.2414 - acc: 0.519 - ETA: 0s - loss: 7.2446 - acc: 0.520 - ETA: 0s - loss: 7.2407 - acc: 0.519 - ETA: 0s - loss: 7.2397 - acc: 0.520 - ETA: 0s - loss: 7.2518 - acc: 0.518 - ETA: 0s - loss: 7.2479 - acc: 0.518 - ETA: 0s - loss: 7.2338 - acc: 0.518 - ETA: 0s - loss: 7.2137 - acc: 0.520 - ETA: 0s - loss: 7.1873 - acc: 0.522 - ETA: 0s - loss: 7.2056 - acc: 0.521 - ETA: 0s - loss: 7.1981 - acc: 0.521 - ETA: 0s - loss: 7.1979 - acc: 0.521 - 3s 397us/step - loss: 7.1870 - acc: 0.5217 - val_loss: 7.7982 - val_acc: 0.4311\n",
      "\n",
      "Epoch 00008: val_loss improved from 8.10514 to 7.79825, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.9453 - acc: 0.650 - ETA: 2s - loss: 6.5028 - acc: 0.568 - ETA: 2s - loss: 6.5310 - acc: 0.560 - ETA: 2s - loss: 6.7730 - acc: 0.550 - ETA: 2s - loss: 6.6585 - acc: 0.553 - ETA: 2s - loss: 6.7974 - acc: 0.547 - ETA: 2s - loss: 6.8462 - acc: 0.543 - ETA: 2s - loss: 6.8853 - acc: 0.541 - ETA: 2s - loss: 6.9459 - acc: 0.539 - ETA: 2s - loss: 6.8734 - acc: 0.542 - ETA: 2s - loss: 6.8262 - acc: 0.546 - ETA: 2s - loss: 6.9218 - acc: 0.542 - ETA: 2s - loss: 6.9016 - acc: 0.545 - ETA: 2s - loss: 6.8396 - acc: 0.548 - ETA: 2s - loss: 6.8995 - acc: 0.545 - ETA: 1s - loss: 6.8982 - acc: 0.543 - ETA: 1s - loss: 6.8817 - acc: 0.544 - ETA: 1s - loss: 6.8865 - acc: 0.543 - ETA: 1s - loss: 6.8458 - acc: 0.544 - ETA: 1s - loss: 6.8210 - acc: 0.545 - ETA: 1s - loss: 6.7911 - acc: 0.547 - ETA: 1s - loss: 6.8388 - acc: 0.544 - ETA: 1s - loss: 6.9103 - acc: 0.540 - ETA: 1s - loss: 6.8919 - acc: 0.542 - ETA: 1s - loss: 6.9228 - acc: 0.540 - ETA: 1s - loss: 6.9085 - acc: 0.540 - ETA: 1s - loss: 6.9311 - acc: 0.539 - ETA: 1s - loss: 6.9126 - acc: 0.541 - ETA: 1s - loss: 6.9432 - acc: 0.540 - ETA: 1s - loss: 6.9579 - acc: 0.539 - ETA: 1s - loss: 6.9234 - acc: 0.542 - ETA: 1s - loss: 6.9375 - acc: 0.541 - ETA: 1s - loss: 6.9050 - acc: 0.544 - ETA: 0s - loss: 6.8930 - acc: 0.545 - ETA: 0s - loss: 6.9259 - acc: 0.543 - ETA: 0s - loss: 6.9367 - acc: 0.542 - ETA: 0s - loss: 6.9390 - acc: 0.542 - ETA: 0s - loss: 6.9122 - acc: 0.543 - ETA: 0s - loss: 6.9153 - acc: 0.543 - ETA: 0s - loss: 6.9361 - acc: 0.542 - ETA: 0s - loss: 6.9418 - acc: 0.542 - ETA: 0s - loss: 6.9781 - acc: 0.540 - ETA: 0s - loss: 6.9681 - acc: 0.540 - ETA: 0s - loss: 6.9926 - acc: 0.539 - ETA: 0s - loss: 7.0008 - acc: 0.539 - ETA: 0s - loss: 7.0149 - acc: 0.537 - ETA: 0s - loss: 6.9972 - acc: 0.539 - ETA: 0s - loss: 6.9663 - acc: 0.541 - ETA: 0s - loss: 6.9518 - acc: 0.542 - ETA: 0s - loss: 6.9390 - acc: 0.543 - ETA: 0s - loss: 6.9249 - acc: 0.544 - 3s 431us/step - loss: 6.9151 - acc: 0.5448 - val_loss: 7.6885 - val_acc: 0.4311\n",
      "\n",
      "Epoch 00009: val_loss improved from 7.79825 to 7.68847, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.4196 - acc: 0.750 - ETA: 2s - loss: 6.8491 - acc: 0.562 - ETA: 2s - loss: 6.4060 - acc: 0.589 - ETA: 2s - loss: 6.7233 - acc: 0.566 - ETA: 2s - loss: 6.7451 - acc: 0.559 - ETA: 2s - loss: 7.0161 - acc: 0.545 - ETA: 2s - loss: 6.9806 - acc: 0.548 - ETA: 2s - loss: 6.7623 - acc: 0.562 - ETA: 2s - loss: 6.6569 - acc: 0.568 - ETA: 2s - loss: 6.5492 - acc: 0.573 - ETA: 2s - loss: 6.5166 - acc: 0.573 - ETA: 2s - loss: 6.6044 - acc: 0.568 - ETA: 1s - loss: 6.6684 - acc: 0.561 - ETA: 1s - loss: 6.6973 - acc: 0.558 - ETA: 1s - loss: 6.6776 - acc: 0.559 - ETA: 1s - loss: 6.7136 - acc: 0.558 - ETA: 1s - loss: 6.6857 - acc: 0.560 - ETA: 1s - loss: 6.7124 - acc: 0.558 - ETA: 1s - loss: 6.7177 - acc: 0.557 - ETA: 1s - loss: 6.7600 - acc: 0.553 - ETA: 1s - loss: 6.7383 - acc: 0.555 - ETA: 1s - loss: 6.7961 - acc: 0.552 - ETA: 1s - loss: 6.7888 - acc: 0.552 - ETA: 1s - loss: 6.7517 - acc: 0.555 - ETA: 1s - loss: 6.7504 - acc: 0.554 - ETA: 1s - loss: 6.7478 - acc: 0.555 - ETA: 1s - loss: 6.7008 - acc: 0.558 - ETA: 1s - loss: 6.6542 - acc: 0.559 - ETA: 0s - loss: 6.6484 - acc: 0.560 - ETA: 0s - loss: 6.6512 - acc: 0.560 - ETA: 0s - loss: 6.6471 - acc: 0.560 - ETA: 0s - loss: 6.6642 - acc: 0.558 - ETA: 0s - loss: 6.6712 - acc: 0.558 - ETA: 0s - loss: 6.6821 - acc: 0.558 - ETA: 0s - loss: 6.6890 - acc: 0.557 - ETA: 0s - loss: 6.6859 - acc: 0.557 - ETA: 0s - loss: 6.6946 - acc: 0.557 - ETA: 0s - loss: 6.6928 - acc: 0.557 - ETA: 0s - loss: 6.6788 - acc: 0.558 - ETA: 0s - loss: 6.6591 - acc: 0.559 - ETA: 0s - loss: 6.6367 - acc: 0.560 - ETA: 0s - loss: 6.6417 - acc: 0.560 - ETA: 0s - loss: 6.6202 - acc: 0.561 - ETA: 0s - loss: 6.6070 - acc: 0.562 - 3s 376us/step - loss: 6.5889 - acc: 0.5626 - val_loss: 7.3762 - val_acc: 0.4419\n",
      "\n",
      "Epoch 00010: val_loss improved from 7.68847 to 7.37616, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 6.4539 - acc: 0.600 - ETA: 2s - loss: 6.1987 - acc: 0.605 - ETA: 2s - loss: 6.3300 - acc: 0.593 - ETA: 2s - loss: 6.3518 - acc: 0.593 - ETA: 2s - loss: 6.3627 - acc: 0.595 - ETA: 2s - loss: 6.4315 - acc: 0.590 - ETA: 1s - loss: 6.5022 - acc: 0.584 - ETA: 1s - loss: 6.5142 - acc: 0.580 - ETA: 1s - loss: 6.5349 - acc: 0.578 - ETA: 1s - loss: 6.7358 - acc: 0.567 - ETA: 1s - loss: 6.6928 - acc: 0.568 - ETA: 1s - loss: 6.5415 - acc: 0.577 - ETA: 1s - loss: 6.4836 - acc: 0.582 - ETA: 1s - loss: 6.4738 - acc: 0.584 - ETA: 1s - loss: 6.3899 - acc: 0.588 - ETA: 1s - loss: 6.3354 - acc: 0.591 - ETA: 1s - loss: 6.3352 - acc: 0.591 - ETA: 1s - loss: 6.3756 - acc: 0.589 - ETA: 1s - loss: 6.3276 - acc: 0.590 - ETA: 1s - loss: 6.2958 - acc: 0.593 - ETA: 1s - loss: 6.2933 - acc: 0.593 - ETA: 1s - loss: 6.2925 - acc: 0.593 - ETA: 1s - loss: 6.3137 - acc: 0.590 - ETA: 1s - loss: 6.3279 - acc: 0.589 - ETA: 1s - loss: 6.3378 - acc: 0.589 - ETA: 1s - loss: 6.3815 - acc: 0.585 - ETA: 0s - loss: 6.3809 - acc: 0.585 - ETA: 0s - loss: 6.3895 - acc: 0.584 - ETA: 0s - loss: 6.3911 - acc: 0.584 - ETA: 0s - loss: 6.4068 - acc: 0.582 - ETA: 0s - loss: 6.4289 - acc: 0.580 - ETA: 0s - loss: 6.4319 - acc: 0.580 - ETA: 0s - loss: 6.4101 - acc: 0.582 - ETA: 0s - loss: 6.4121 - acc: 0.582 - ETA: 0s - loss: 6.4156 - acc: 0.582 - ETA: 0s - loss: 6.4036 - acc: 0.582 - ETA: 0s - loss: 6.3999 - acc: 0.582 - ETA: 0s - loss: 6.4016 - acc: 0.582 - ETA: 0s - loss: 6.4254 - acc: 0.580 - ETA: 0s - loss: 6.4733 - acc: 0.577 - ETA: 0s - loss: 6.4561 - acc: 0.577 - ETA: 0s - loss: 6.4553 - acc: 0.577 - ETA: 0s - loss: 6.4398 - acc: 0.578 - ETA: 0s - loss: 6.4390 - acc: 0.578 - ETA: 0s - loss: 6.4186 - acc: 0.579 - ETA: 0s - loss: 6.4043 - acc: 0.580 - 3s 387us/step - loss: 6.4131 - acc: 0.5799 - val_loss: 7.1987 - val_acc: 0.4599\n",
      "\n",
      "Epoch 00011: val_loss improved from 7.37616 to 7.19867, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.0320 - acc: 0.750 - ETA: 2s - loss: 6.0044 - acc: 0.616 - ETA: 2s - loss: 6.6058 - acc: 0.573 - ETA: 2s - loss: 6.6485 - acc: 0.568 - ETA: 2s - loss: 6.6857 - acc: 0.568 - ETA: 2s - loss: 6.6902 - acc: 0.569 - ETA: 1s - loss: 6.5471 - acc: 0.579 - ETA: 1s - loss: 6.5099 - acc: 0.582 - ETA: 1s - loss: 6.6010 - acc: 0.575 - ETA: 1s - loss: 6.5555 - acc: 0.577 - ETA: 1s - loss: 6.4971 - acc: 0.581 - ETA: 1s - loss: 6.4777 - acc: 0.581 - ETA: 1s - loss: 6.3911 - acc: 0.588 - ETA: 1s - loss: 6.3746 - acc: 0.588 - ETA: 1s - loss: 6.4595 - acc: 0.583 - ETA: 1s - loss: 6.3943 - acc: 0.586 - ETA: 1s - loss: 6.4340 - acc: 0.583 - ETA: 1s - loss: 6.4378 - acc: 0.582 - ETA: 1s - loss: 6.4170 - acc: 0.583 - ETA: 1s - loss: 6.4589 - acc: 0.580 - ETA: 1s - loss: 6.4973 - acc: 0.578 - ETA: 1s - loss: 6.4334 - acc: 0.582 - ETA: 1s - loss: 6.4300 - acc: 0.583 - ETA: 1s - loss: 6.4132 - acc: 0.583 - ETA: 1s - loss: 6.4370 - acc: 0.582 - ETA: 1s - loss: 6.4163 - acc: 0.584 - ETA: 1s - loss: 6.3876 - acc: 0.585 - ETA: 1s - loss: 6.3906 - acc: 0.585 - ETA: 0s - loss: 6.4047 - acc: 0.583 - ETA: 0s - loss: 6.4075 - acc: 0.583 - ETA: 0s - loss: 6.4099 - acc: 0.584 - ETA: 0s - loss: 6.3637 - acc: 0.587 - ETA: 0s - loss: 6.4094 - acc: 0.584 - ETA: 0s - loss: 6.3991 - acc: 0.585 - ETA: 0s - loss: 6.3639 - acc: 0.587 - ETA: 0s - loss: 6.3242 - acc: 0.590 - ETA: 0s - loss: 6.3200 - acc: 0.590 - ETA: 0s - loss: 6.3045 - acc: 0.591 - ETA: 0s - loss: 6.3293 - acc: 0.590 - ETA: 0s - loss: 6.3209 - acc: 0.590 - ETA: 0s - loss: 6.3272 - acc: 0.590 - ETA: 0s - loss: 6.3334 - acc: 0.590 - ETA: 0s - loss: 6.3092 - acc: 0.592 - ETA: 0s - loss: 6.2980 - acc: 0.593 - ETA: 0s - loss: 6.2747 - acc: 0.594 - ETA: 0s - loss: 6.3137 - acc: 0.592 - 3s 387us/step - loss: 6.3073 - acc: 0.5925 - val_loss: 7.1734 - val_acc: 0.4743\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: val_loss improved from 7.19867 to 7.17345, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 5.6511 - acc: 0.650 - ETA: 2s - loss: 4.9931 - acc: 0.683 - ETA: 2s - loss: 5.8202 - acc: 0.635 - ETA: 2s - loss: 5.9032 - acc: 0.628 - ETA: 2s - loss: 5.9406 - acc: 0.627 - ETA: 2s - loss: 5.8663 - acc: 0.628 - ETA: 2s - loss: 5.9494 - acc: 0.622 - ETA: 1s - loss: 5.9552 - acc: 0.623 - ETA: 1s - loss: 5.9888 - acc: 0.620 - ETA: 1s - loss: 5.9650 - acc: 0.620 - ETA: 1s - loss: 5.9713 - acc: 0.620 - ETA: 1s - loss: 5.9641 - acc: 0.621 - ETA: 1s - loss: 5.9765 - acc: 0.620 - ETA: 1s - loss: 6.0107 - acc: 0.619 - ETA: 1s - loss: 6.1089 - acc: 0.613 - ETA: 1s - loss: 6.1070 - acc: 0.613 - ETA: 1s - loss: 6.1141 - acc: 0.613 - ETA: 1s - loss: 6.0948 - acc: 0.614 - ETA: 1s - loss: 6.0837 - acc: 0.614 - ETA: 1s - loss: 6.0760 - acc: 0.614 - ETA: 1s - loss: 6.0555 - acc: 0.615 - ETA: 1s - loss: 6.0156 - acc: 0.617 - ETA: 1s - loss: 6.0493 - acc: 0.615 - ETA: 1s - loss: 6.0924 - acc: 0.612 - ETA: 1s - loss: 6.1347 - acc: 0.610 - ETA: 1s - loss: 6.1465 - acc: 0.609 - ETA: 1s - loss: 6.1064 - acc: 0.611 - ETA: 0s - loss: 6.0867 - acc: 0.612 - ETA: 0s - loss: 6.0498 - acc: 0.613 - ETA: 0s - loss: 6.0493 - acc: 0.613 - ETA: 0s - loss: 6.0637 - acc: 0.611 - ETA: 0s - loss: 6.0851 - acc: 0.610 - ETA: 0s - loss: 6.1039 - acc: 0.608 - ETA: 0s - loss: 6.1214 - acc: 0.606 - ETA: 0s - loss: 6.1391 - acc: 0.604 - ETA: 0s - loss: 6.1671 - acc: 0.602 - ETA: 0s - loss: 6.1481 - acc: 0.602 - ETA: 0s - loss: 6.1865 - acc: 0.600 - ETA: 0s - loss: 6.1683 - acc: 0.601 - ETA: 0s - loss: 6.1755 - acc: 0.600 - ETA: 0s - loss: 6.1916 - acc: 0.600 - ETA: 0s - loss: 6.1898 - acc: 0.599 - ETA: 0s - loss: 6.2024 - acc: 0.598 - ETA: 0s - loss: 6.2258 - acc: 0.596 - ETA: 0s - loss: 6.2372 - acc: 0.595 - 3s 385us/step - loss: 6.2261 - acc: 0.5964 - val_loss: 7.1321 - val_acc: 0.4766\n",
      "\n",
      "Epoch 00013: val_loss improved from 7.17345 to 7.13210, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 5.7122 - acc: 0.600 - ETA: 2s - loss: 6.2050 - acc: 0.587 - ETA: 2s - loss: 6.1457 - acc: 0.600 - ETA: 2s - loss: 6.0771 - acc: 0.609 - ETA: 2s - loss: 6.1443 - acc: 0.607 - ETA: 2s - loss: 6.3206 - acc: 0.594 - ETA: 2s - loss: 6.2025 - acc: 0.600 - ETA: 2s - loss: 6.2591 - acc: 0.594 - ETA: 2s - loss: 6.2116 - acc: 0.597 - ETA: 2s - loss: 6.1362 - acc: 0.602 - ETA: 2s - loss: 6.1744 - acc: 0.599 - ETA: 2s - loss: 6.2003 - acc: 0.598 - ETA: 2s - loss: 6.2147 - acc: 0.596 - ETA: 2s - loss: 6.2920 - acc: 0.592 - ETA: 2s - loss: 6.2774 - acc: 0.593 - ETA: 1s - loss: 6.2784 - acc: 0.592 - ETA: 1s - loss: 6.3098 - acc: 0.591 - ETA: 1s - loss: 6.3081 - acc: 0.590 - ETA: 1s - loss: 6.2794 - acc: 0.592 - ETA: 1s - loss: 6.2075 - acc: 0.597 - ETA: 1s - loss: 6.2030 - acc: 0.597 - ETA: 1s - loss: 6.1711 - acc: 0.598 - ETA: 1s - loss: 6.1255 - acc: 0.600 - ETA: 1s - loss: 6.0848 - acc: 0.603 - ETA: 1s - loss: 6.0878 - acc: 0.604 - ETA: 1s - loss: 6.0834 - acc: 0.604 - ETA: 1s - loss: 6.0740 - acc: 0.605 - ETA: 1s - loss: 6.0662 - acc: 0.606 - ETA: 1s - loss: 6.0607 - acc: 0.606 - ETA: 1s - loss: 6.0555 - acc: 0.607 - ETA: 0s - loss: 6.0999 - acc: 0.604 - ETA: 0s - loss: 6.1017 - acc: 0.604 - ETA: 0s - loss: 6.0489 - acc: 0.608 - ETA: 0s - loss: 6.0575 - acc: 0.607 - ETA: 0s - loss: 6.0531 - acc: 0.607 - ETA: 0s - loss: 6.0643 - acc: 0.607 - ETA: 0s - loss: 6.0749 - acc: 0.606 - ETA: 0s - loss: 6.0573 - acc: 0.607 - ETA: 0s - loss: 6.0863 - acc: 0.605 - ETA: 0s - loss: 6.0950 - acc: 0.604 - ETA: 0s - loss: 6.0737 - acc: 0.606 - ETA: 0s - loss: 6.0636 - acc: 0.606 - ETA: 0s - loss: 6.0539 - acc: 0.607 - ETA: 0s - loss: 6.0428 - acc: 0.607 - ETA: 0s - loss: 6.0463 - acc: 0.607 - ETA: 0s - loss: 6.0553 - acc: 0.606 - 3s 387us/step - loss: 6.0675 - acc: 0.6058 - val_loss: 6.9567 - val_acc: 0.4802\n",
      "\n",
      "Epoch 00014: val_loss improved from 7.13210 to 6.95668, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.3775 - acc: 0.700 - ETA: 2s - loss: 6.0758 - acc: 0.610 - ETA: 2s - loss: 6.4681 - acc: 0.590 - ETA: 2s - loss: 6.2234 - acc: 0.604 - ETA: 2s - loss: 5.9947 - acc: 0.616 - ETA: 2s - loss: 5.9162 - acc: 0.619 - ETA: 2s - loss: 5.6757 - acc: 0.635 - ETA: 2s - loss: 5.5787 - acc: 0.641 - ETA: 2s - loss: 5.5739 - acc: 0.641 - ETA: 2s - loss: 5.4651 - acc: 0.645 - ETA: 2s - loss: 5.4269 - acc: 0.649 - ETA: 2s - loss: 5.5378 - acc: 0.642 - ETA: 2s - loss: 5.6241 - acc: 0.637 - ETA: 1s - loss: 5.6910 - acc: 0.632 - ETA: 1s - loss: 5.7075 - acc: 0.631 - ETA: 1s - loss: 5.7137 - acc: 0.629 - ETA: 1s - loss: 5.7514 - acc: 0.626 - ETA: 1s - loss: 5.8025 - acc: 0.622 - ETA: 1s - loss: 5.7447 - acc: 0.625 - ETA: 1s - loss: 5.7987 - acc: 0.621 - ETA: 1s - loss: 5.7685 - acc: 0.623 - ETA: 1s - loss: 5.7600 - acc: 0.623 - ETA: 1s - loss: 5.7650 - acc: 0.622 - ETA: 1s - loss: 5.7466 - acc: 0.623 - ETA: 1s - loss: 5.7237 - acc: 0.624 - ETA: 1s - loss: 5.7294 - acc: 0.624 - ETA: 1s - loss: 5.7367 - acc: 0.623 - ETA: 1s - loss: 5.8099 - acc: 0.619 - ETA: 1s - loss: 5.8183 - acc: 0.619 - ETA: 0s - loss: 5.8354 - acc: 0.618 - ETA: 0s - loss: 5.8506 - acc: 0.617 - ETA: 0s - loss: 5.8593 - acc: 0.617 - ETA: 0s - loss: 5.8553 - acc: 0.617 - ETA: 0s - loss: 5.8678 - acc: 0.616 - ETA: 0s - loss: 5.8695 - acc: 0.616 - ETA: 0s - loss: 5.8578 - acc: 0.617 - ETA: 0s - loss: 5.8735 - acc: 0.615 - ETA: 0s - loss: 5.8767 - acc: 0.615 - ETA: 0s - loss: 5.9028 - acc: 0.613 - ETA: 0s - loss: 5.9091 - acc: 0.613 - ETA: 0s - loss: 5.9378 - acc: 0.611 - ETA: 0s - loss: 5.9425 - acc: 0.610 - ETA: 0s - loss: 5.9269 - acc: 0.611 - ETA: 0s - loss: 5.9473 - acc: 0.609 - ETA: 0s - loss: 5.9279 - acc: 0.610 - 3s 384us/step - loss: 5.9444 - acc: 0.6100 - val_loss: 6.8452 - val_acc: 0.4862\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.95668 to 6.84523, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.8910 - acc: 0.650 - ETA: 2s - loss: 5.3138 - acc: 0.644 - ETA: 2s - loss: 5.3341 - acc: 0.650 - ETA: 1s - loss: 5.5599 - acc: 0.630 - ETA: 1s - loss: 5.5483 - acc: 0.632 - ETA: 1s - loss: 5.5583 - acc: 0.635 - ETA: 1s - loss: 5.6101 - acc: 0.629 - ETA: 1s - loss: 5.5619 - acc: 0.632 - ETA: 1s - loss: 5.6295 - acc: 0.626 - ETA: 1s - loss: 5.5979 - acc: 0.628 - ETA: 1s - loss: 5.6424 - acc: 0.624 - ETA: 1s - loss: 5.5665 - acc: 0.629 - ETA: 1s - loss: 5.5903 - acc: 0.626 - ETA: 1s - loss: 5.6795 - acc: 0.621 - ETA: 1s - loss: 5.7218 - acc: 0.617 - ETA: 1s - loss: 5.7363 - acc: 0.618 - ETA: 1s - loss: 5.7544 - acc: 0.617 - ETA: 1s - loss: 5.7400 - acc: 0.619 - ETA: 1s - loss: 5.7371 - acc: 0.619 - ETA: 1s - loss: 5.7082 - acc: 0.621 - ETA: 0s - loss: 5.7400 - acc: 0.619 - ETA: 0s - loss: 5.7897 - acc: 0.616 - ETA: 0s - loss: 5.8146 - acc: 0.615 - ETA: 0s - loss: 5.7915 - acc: 0.616 - ETA: 0s - loss: 5.7758 - acc: 0.617 - ETA: 0s - loss: 5.7879 - acc: 0.617 - ETA: 0s - loss: 5.7442 - acc: 0.619 - ETA: 0s - loss: 5.7304 - acc: 0.619 - ETA: 0s - loss: 5.7157 - acc: 0.619 - ETA: 0s - loss: 5.7002 - acc: 0.620 - ETA: 0s - loss: 5.6889 - acc: 0.621 - ETA: 0s - loss: 5.6517 - acc: 0.623 - ETA: 0s - loss: 5.6249 - acc: 0.625 - ETA: 0s - loss: 5.6081 - acc: 0.626 - ETA: 0s - loss: 5.5962 - acc: 0.627 - ETA: 0s - loss: 5.6103 - acc: 0.626 - ETA: 0s - loss: 5.5990 - acc: 0.627 - 2s 312us/step - loss: 5.6043 - acc: 0.6269 - val_loss: 6.6082 - val_acc: 0.5030\n",
      "\n",
      "Epoch 00016: val_loss improved from 6.84523 to 6.60825, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 7.2533 - acc: 0.550 - ETA: 1s - loss: 5.6740 - acc: 0.636 - ETA: 1s - loss: 5.9135 - acc: 0.623 - ETA: 1s - loss: 5.5130 - acc: 0.645 - ETA: 1s - loss: 5.5050 - acc: 0.646 - ETA: 1s - loss: 5.4643 - acc: 0.649 - ETA: 1s - loss: 5.6619 - acc: 0.636 - ETA: 1s - loss: 5.5695 - acc: 0.643 - ETA: 1s - loss: 5.5321 - acc: 0.646 - ETA: 1s - loss: 5.4794 - acc: 0.647 - ETA: 1s - loss: 5.5570 - acc: 0.642 - ETA: 1s - loss: 5.5527 - acc: 0.642 - ETA: 1s - loss: 5.4873 - acc: 0.647 - ETA: 1s - loss: 5.4585 - acc: 0.647 - ETA: 1s - loss: 5.4163 - acc: 0.648 - ETA: 1s - loss: 5.4584 - acc: 0.645 - ETA: 0s - loss: 5.4555 - acc: 0.646 - ETA: 0s - loss: 5.4741 - acc: 0.644 - ETA: 0s - loss: 5.4946 - acc: 0.644 - ETA: 0s - loss: 5.4736 - acc: 0.645 - ETA: 0s - loss: 5.4967 - acc: 0.644 - ETA: 0s - loss: 5.5131 - acc: 0.642 - ETA: 0s - loss: 5.5216 - acc: 0.641 - ETA: 0s - loss: 5.5222 - acc: 0.641 - ETA: 0s - loss: 5.5098 - acc: 0.641 - ETA: 0s - loss: 5.4933 - acc: 0.642 - ETA: 0s - loss: 5.4854 - acc: 0.642 - ETA: 0s - loss: 5.4490 - acc: 0.644 - ETA: 0s - loss: 5.4446 - acc: 0.644 - ETA: 0s - loss: 5.4233 - acc: 0.646 - ETA: 0s - loss: 5.4286 - acc: 0.645 - ETA: 0s - loss: 5.4265 - acc: 0.645 - ETA: 0s - loss: 5.4397 - acc: 0.644 - ETA: 0s - loss: 5.4312 - acc: 0.644 - ETA: 0s - loss: 5.4464 - acc: 0.642 - ETA: 0s - loss: 5.4605 - acc: 0.641 - 2s 311us/step - loss: 5.4510 - acc: 0.6421 - val_loss: 6.5011 - val_acc: 0.4970\n",
      "\n",
      "Epoch 00017: val_loss improved from 6.60825 to 6.50115, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 4.0296 - acc: 0.750 - ETA: 1s - loss: 5.4374 - acc: 0.650 - ETA: 1s - loss: 5.1839 - acc: 0.663 - ETA: 1s - loss: 5.0815 - acc: 0.664 - ETA: 1s - loss: 5.2331 - acc: 0.655 - ETA: 1s - loss: 5.1432 - acc: 0.661 - ETA: 1s - loss: 5.1287 - acc: 0.662 - ETA: 1s - loss: 5.1419 - acc: 0.662 - ETA: 1s - loss: 5.2012 - acc: 0.660 - ETA: 1s - loss: 5.2519 - acc: 0.655 - ETA: 1s - loss: 5.2666 - acc: 0.655 - ETA: 1s - loss: 5.3411 - acc: 0.651 - ETA: 1s - loss: 5.3483 - acc: 0.650 - ETA: 1s - loss: 5.3759 - acc: 0.649 - ETA: 1s - loss: 5.3949 - acc: 0.648 - ETA: 1s - loss: 5.4023 - acc: 0.648 - ETA: 1s - loss: 5.3255 - acc: 0.653 - ETA: 1s - loss: 5.2834 - acc: 0.656 - ETA: 1s - loss: 5.2617 - acc: 0.657 - ETA: 1s - loss: 5.2515 - acc: 0.657 - ETA: 1s - loss: 5.2580 - acc: 0.657 - ETA: 1s - loss: 5.2947 - acc: 0.655 - ETA: 1s - loss: 5.2318 - acc: 0.658 - ETA: 1s - loss: 5.2480 - acc: 0.657 - ETA: 1s - loss: 5.2372 - acc: 0.658 - ETA: 1s - loss: 5.2248 - acc: 0.659 - ETA: 1s - loss: 5.2822 - acc: 0.656 - ETA: 1s - loss: 5.2469 - acc: 0.658 - ETA: 1s - loss: 5.2556 - acc: 0.657 - ETA: 0s - loss: 5.2536 - acc: 0.657 - ETA: 0s - loss: 5.2860 - acc: 0.654 - ETA: 0s - loss: 5.2842 - acc: 0.655 - ETA: 0s - loss: 5.2935 - acc: 0.654 - ETA: 0s - loss: 5.3228 - acc: 0.653 - ETA: 0s - loss: 5.3484 - acc: 0.651 - ETA: 0s - loss: 5.3468 - acc: 0.651 - ETA: 0s - loss: 5.3472 - acc: 0.651 - ETA: 0s - loss: 5.3636 - acc: 0.650 - ETA: 0s - loss: 5.3748 - acc: 0.649 - ETA: 0s - loss: 5.3740 - acc: 0.649 - ETA: 0s - loss: 5.3505 - acc: 0.651 - ETA: 0s - loss: 5.3432 - acc: 0.651 - ETA: 0s - loss: 5.3518 - acc: 0.651 - ETA: 0s - loss: 5.3295 - acc: 0.652 - ETA: 0s - loss: 5.3302 - acc: 0.652 - ETA: 0s - loss: 5.3588 - acc: 0.650 - ETA: 0s - loss: 5.3536 - acc: 0.650 - ETA: 0s - loss: 5.3491 - acc: 0.650 - ETA: 0s - loss: 5.3572 - acc: 0.650 - ETA: 0s - loss: 5.3564 - acc: 0.650 - ETA: 0s - loss: 5.3489 - acc: 0.650 - ETA: 0s - loss: 5.3401 - acc: 0.651 - ETA: 0s - loss: 5.3299 - acc: 0.652 - ETA: 0s - loss: 5.3187 - acc: 0.653 - 3s 483us/step - loss: 5.3116 - acc: 0.6534 - val_loss: 6.3288 - val_acc: 0.5186\n",
      "\n",
      "Epoch 00018: val_loss improved from 6.50115 to 6.32884, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.3194 - acc: 0.750 - ETA: 2s - loss: 5.4623 - acc: 0.638 - ETA: 2s - loss: 5.8477 - acc: 0.618 - ETA: 2s - loss: 5.4058 - acc: 0.647 - ETA: 2s - loss: 5.2921 - acc: 0.657 - ETA: 2s - loss: 5.2390 - acc: 0.660 - ETA: 2s - loss: 5.2816 - acc: 0.660 - ETA: 2s - loss: 5.2616 - acc: 0.662 - ETA: 2s - loss: 5.2567 - acc: 0.660 - ETA: 2s - loss: 5.2840 - acc: 0.659 - ETA: 2s - loss: 5.3570 - acc: 0.654 - ETA: 2s - loss: 5.3015 - acc: 0.659 - ETA: 2s - loss: 5.2285 - acc: 0.664 - ETA: 2s - loss: 5.2255 - acc: 0.664 - ETA: 2s - loss: 5.2614 - acc: 0.661 - ETA: 2s - loss: 5.3441 - acc: 0.656 - ETA: 2s - loss: 5.3727 - acc: 0.654 - ETA: 2s - loss: 5.3443 - acc: 0.656 - ETA: 2s - loss: 5.2985 - acc: 0.659 - ETA: 2s - loss: 5.2688 - acc: 0.661 - ETA: 2s - loss: 5.2927 - acc: 0.659 - ETA: 2s - loss: 5.2777 - acc: 0.659 - ETA: 2s - loss: 5.2805 - acc: 0.659 - ETA: 2s - loss: 5.3006 - acc: 0.658 - ETA: 2s - loss: 5.2854 - acc: 0.659 - ETA: 2s - loss: 5.2842 - acc: 0.658 - ETA: 2s - loss: 5.2918 - acc: 0.658 - ETA: 2s - loss: 5.2462 - acc: 0.660 - ETA: 2s - loss: 5.2246 - acc: 0.662 - ETA: 1s - loss: 5.2448 - acc: 0.660 - ETA: 1s - loss: 5.2579 - acc: 0.660 - ETA: 1s - loss: 5.2680 - acc: 0.659 - ETA: 1s - loss: 5.3050 - acc: 0.656 - ETA: 1s - loss: 5.3071 - acc: 0.657 - ETA: 1s - loss: 5.3198 - acc: 0.656 - ETA: 1s - loss: 5.3184 - acc: 0.656 - ETA: 1s - loss: 5.2993 - acc: 0.657 - ETA: 1s - loss: 5.2873 - acc: 0.658 - ETA: 1s - loss: 5.2757 - acc: 0.659 - ETA: 1s - loss: 5.2988 - acc: 0.657 - ETA: 1s - loss: 5.2979 - acc: 0.656 - ETA: 1s - loss: 5.3079 - acc: 0.656 - ETA: 1s - loss: 5.2714 - acc: 0.658 - ETA: 1s - loss: 5.2826 - acc: 0.657 - ETA: 0s - loss: 5.2857 - acc: 0.657 - ETA: 0s - loss: 5.2991 - acc: 0.656 - ETA: 0s - loss: 5.2706 - acc: 0.657 - ETA: 0s - loss: 5.2577 - acc: 0.658 - ETA: 0s - loss: 5.2308 - acc: 0.660 - ETA: 0s - loss: 5.2193 - acc: 0.661 - ETA: 0s - loss: 5.2054 - acc: 0.662 - ETA: 0s - loss: 5.2116 - acc: 0.661 - ETA: 0s - loss: 5.2142 - acc: 0.661 - ETA: 0s - loss: 5.2377 - acc: 0.659 - ETA: 0s - loss: 5.2561 - acc: 0.658 - ETA: 0s - loss: 5.2445 - acc: 0.659 - ETA: 0s - loss: 5.2307 - acc: 0.660 - ETA: 0s - loss: 5.2505 - acc: 0.659 - ETA: 0s - loss: 5.2402 - acc: 0.659 - 3s 509us/step - loss: 5.2333 - acc: 0.6602 - val_loss: 6.3353 - val_acc: 0.5186\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 6.32884\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 5.6442 - acc: 0.650 - ETA: 2s - loss: 5.7958 - acc: 0.627 - ETA: 2s - loss: 5.0940 - acc: 0.655 - ETA: 2s - loss: 5.1398 - acc: 0.656 - ETA: 2s - loss: 5.0459 - acc: 0.659 - ETA: 2s - loss: 5.1001 - acc: 0.661 - ETA: 2s - loss: 5.0695 - acc: 0.665 - ETA: 2s - loss: 5.2104 - acc: 0.657 - ETA: 2s - loss: 5.1738 - acc: 0.661 - ETA: 1s - loss: 5.1564 - acc: 0.662 - ETA: 1s - loss: 5.1008 - acc: 0.663 - ETA: 1s - loss: 5.1406 - acc: 0.660 - ETA: 1s - loss: 5.1125 - acc: 0.662 - ETA: 1s - loss: 5.0663 - acc: 0.665 - ETA: 1s - loss: 5.0446 - acc: 0.666 - ETA: 1s - loss: 5.0261 - acc: 0.667 - ETA: 1s - loss: 5.0659 - acc: 0.665 - ETA: 1s - loss: 5.0680 - acc: 0.665 - ETA: 1s - loss: 5.1138 - acc: 0.663 - ETA: 1s - loss: 5.1141 - acc: 0.663 - ETA: 1s - loss: 5.1249 - acc: 0.662 - ETA: 1s - loss: 5.0978 - acc: 0.663 - ETA: 1s - loss: 5.0946 - acc: 0.664 - ETA: 1s - loss: 5.0808 - acc: 0.665 - ETA: 1s - loss: 5.0812 - acc: 0.665 - ETA: 1s - loss: 5.0953 - acc: 0.665 - ETA: 0s - loss: 5.0757 - acc: 0.666 - ETA: 0s - loss: 5.0903 - acc: 0.665 - ETA: 0s - loss: 5.0785 - acc: 0.666 - ETA: 0s - loss: 5.1017 - acc: 0.665 - ETA: 0s - loss: 5.0663 - acc: 0.667 - ETA: 0s - loss: 5.0799 - acc: 0.666 - ETA: 0s - loss: 5.0842 - acc: 0.666 - ETA: 0s - loss: 5.0983 - acc: 0.665 - ETA: 0s - loss: 5.0833 - acc: 0.666 - ETA: 0s - loss: 5.0630 - acc: 0.667 - ETA: 0s - loss: 5.0545 - acc: 0.668 - ETA: 0s - loss: 5.0551 - acc: 0.668 - ETA: 0s - loss: 5.0698 - acc: 0.667 - ETA: 0s - loss: 5.0831 - acc: 0.667 - ETA: 0s - loss: 5.0834 - acc: 0.667 - ETA: 0s - loss: 5.0988 - acc: 0.666 - ETA: 0s - loss: 5.1195 - acc: 0.665 - ETA: 0s - loss: 5.1084 - acc: 0.666 - ETA: 0s - loss: 5.0944 - acc: 0.667 - 3s 392us/step - loss: 5.0998 - acc: 0.6671 - val_loss: 6.2216 - val_acc: 0.5281\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: val_loss improved from 6.32884 to 6.22157, saving model to saved_models/weights.best.VGG16.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad1c5c5e10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 53.2297%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogVGG19Data.npz')\n",
    "train_VGG19 = bottleneck_features['train']\n",
    "valid_VGG19 = bottleneck_features['valid']\n",
    "test_VGG19 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 133)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               17822     \n",
      "=================================================================\n",
      "Total params: 86,051\n",
      "Trainable params: 86,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "VGG19_model = Sequential()\n",
    "VGG19_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG19_model.add(Dense(133, activation='relu'))\n",
    "VGG19_model.add(Dropout(0.5))\n",
    "VGG19_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG19_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "VGG19_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/3\n",
      "6680/6680 [==============================] - ETA: 5:20 - loss: 14.3037 - acc: 0.03 - ETA: 1:05 - loss: 14.5264 - acc: 0.02 - ETA: 41s - loss: 14.2982 - acc: 0.0167 - ETA: 28s - loss: 13.6244 - acc: 0.016 - ETA: 21s - loss: 12.8970 - acc: 0.018 - ETA: 16s - loss: 11.8057 - acc: 0.015 - ETA: 13s - loss: 10.7339 - acc: 0.016 - ETA: 11s - loss: 9.8720 - acc: 0.016 - ETA: 9s - loss: 9.2328 - acc: 0.0185 - ETA: 8s - loss: 8.7250 - acc: 0.022 - ETA: 7s - loss: 8.3260 - acc: 0.023 - ETA: 6s - loss: 7.9949 - acc: 0.026 - ETA: 6s - loss: 7.7723 - acc: 0.026 - ETA: 5s - loss: 7.5249 - acc: 0.026 - ETA: 5s - loss: 7.3629 - acc: 0.026 - ETA: 5s - loss: 7.1783 - acc: 0.029 - ETA: 4s - loss: 7.0216 - acc: 0.030 - ETA: 4s - loss: 6.8893 - acc: 0.030 - ETA: 4s - loss: 6.7603 - acc: 0.031 - ETA: 3s - loss: 6.6522 - acc: 0.032 - ETA: 3s - loss: 6.5510 - acc: 0.036 - ETA: 3s - loss: 6.4432 - acc: 0.038 - ETA: 2s - loss: 6.3514 - acc: 0.038 - ETA: 2s - loss: 6.2738 - acc: 0.039 - ETA: 2s - loss: 6.1984 - acc: 0.040 - ETA: 2s - loss: 6.1343 - acc: 0.041 - ETA: 2s - loss: 6.0904 - acc: 0.040 - ETA: 2s - loss: 6.0238 - acc: 0.042 - ETA: 1s - loss: 5.9672 - acc: 0.042 - ETA: 1s - loss: 5.9000 - acc: 0.044 - ETA: 1s - loss: 5.8360 - acc: 0.047 - ETA: 1s - loss: 5.7961 - acc: 0.048 - ETA: 1s - loss: 5.7666 - acc: 0.047 - ETA: 1s - loss: 5.7227 - acc: 0.049 - ETA: 1s - loss: 5.6849 - acc: 0.050 - ETA: 0s - loss: 5.6445 - acc: 0.051 - ETA: 0s - loss: 5.5945 - acc: 0.054 - ETA: 0s - loss: 5.5499 - acc: 0.057 - ETA: 0s - loss: 5.5147 - acc: 0.057 - ETA: 0s - loss: 5.4808 - acc: 0.059 - ETA: 0s - loss: 5.4425 - acc: 0.060 - ETA: 0s - loss: 5.3959 - acc: 0.063 - ETA: 0s - loss: 5.3532 - acc: 0.066 - 4s 599us/step - loss: 5.3418 - acc: 0.0663 - val_loss: 3.6729 - val_acc: 0.1916\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.67292, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 2/3\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.5838 - acc: 0.133 - ETA: 2s - loss: 4.0335 - acc: 0.127 - ETA: 2s - loss: 4.0128 - acc: 0.133 - ETA: 2s - loss: 3.9815 - acc: 0.137 - ETA: 2s - loss: 4.0184 - acc: 0.134 - ETA: 2s - loss: 4.0164 - acc: 0.125 - ETA: 2s - loss: 3.9820 - acc: 0.134 - ETA: 1s - loss: 3.9677 - acc: 0.143 - ETA: 1s - loss: 3.9508 - acc: 0.146 - ETA: 1s - loss: 3.9487 - acc: 0.149 - ETA: 1s - loss: 3.9359 - acc: 0.152 - ETA: 1s - loss: 3.9333 - acc: 0.152 - ETA: 1s - loss: 3.9124 - acc: 0.156 - ETA: 1s - loss: 3.9161 - acc: 0.155 - ETA: 1s - loss: 3.9080 - acc: 0.154 - ETA: 1s - loss: 3.8950 - acc: 0.158 - ETA: 1s - loss: 3.8885 - acc: 0.159 - ETA: 1s - loss: 3.8806 - acc: 0.160 - ETA: 1s - loss: 3.8765 - acc: 0.161 - ETA: 1s - loss: 3.8666 - acc: 0.163 - ETA: 1s - loss: 3.8653 - acc: 0.162 - ETA: 1s - loss: 3.8481 - acc: 0.164 - ETA: 1s - loss: 3.8400 - acc: 0.165 - ETA: 1s - loss: 3.8336 - acc: 0.165 - ETA: 1s - loss: 3.8261 - acc: 0.165 - ETA: 1s - loss: 3.8301 - acc: 0.167 - ETA: 1s - loss: 3.8329 - acc: 0.166 - ETA: 1s - loss: 3.8271 - acc: 0.167 - ETA: 1s - loss: 3.8117 - acc: 0.170 - ETA: 1s - loss: 3.8099 - acc: 0.172 - ETA: 1s - loss: 3.8085 - acc: 0.171 - ETA: 1s - loss: 3.8042 - acc: 0.173 - ETA: 1s - loss: 3.7934 - acc: 0.174 - ETA: 1s - loss: 3.7753 - acc: 0.178 - ETA: 1s - loss: 3.7690 - acc: 0.178 - ETA: 0s - loss: 3.7650 - acc: 0.179 - ETA: 0s - loss: 3.7528 - acc: 0.181 - ETA: 0s - loss: 3.7497 - acc: 0.182 - ETA: 0s - loss: 3.7493 - acc: 0.182 - ETA: 0s - loss: 3.7418 - acc: 0.183 - ETA: 0s - loss: 3.7369 - acc: 0.184 - ETA: 0s - loss: 3.7321 - acc: 0.184 - ETA: 0s - loss: 3.7228 - acc: 0.185 - ETA: 0s - loss: 3.7162 - acc: 0.185 - ETA: 0s - loss: 3.7119 - acc: 0.185 - ETA: 0s - loss: 3.7118 - acc: 0.185 - ETA: 0s - loss: 3.7080 - acc: 0.186 - ETA: 0s - loss: 3.7040 - acc: 0.187 - ETA: 0s - loss: 3.7000 - acc: 0.187 - ETA: 0s - loss: 3.6928 - acc: 0.189 - ETA: 0s - loss: 3.6947 - acc: 0.188 - ETA: 0s - loss: 3.6871 - acc: 0.190 - ETA: 0s - loss: 3.6810 - acc: 0.190 - ETA: 0s - loss: 3.6807 - acc: 0.191 - ETA: 0s - loss: 3.6736 - acc: 0.192 - 3s 487us/step - loss: 3.6656 - acc: 0.1928 - val_loss: 2.4540 - val_acc: 0.4000\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.67292 to 2.45402, saving model to saved_models/weights.best.VGG19.hdf5\n",
      "Epoch 3/3\n",
      "6680/6680 [==============================] - ETA: 2s - loss: 3.1545 - acc: 0.233 - ETA: 4s - loss: 3.3888 - acc: 0.244 - ETA: 4s - loss: 3.1940 - acc: 0.238 - ETA: 4s - loss: 3.1857 - acc: 0.255 - ETA: 3s - loss: 3.1510 - acc: 0.258 - ETA: 3s - loss: 3.1203 - acc: 0.273 - ETA: 3s - loss: 3.1823 - acc: 0.264 - ETA: 3s - loss: 3.1814 - acc: 0.259 - ETA: 3s - loss: 3.1341 - acc: 0.272 - ETA: 3s - loss: 3.1355 - acc: 0.276 - ETA: 3s - loss: 3.1254 - acc: 0.274 - ETA: 3s - loss: 3.1081 - acc: 0.273 - ETA: 3s - loss: 3.1100 - acc: 0.272 - ETA: 3s - loss: 3.0970 - acc: 0.270 - ETA: 2s - loss: 3.1184 - acc: 0.271 - ETA: 2s - loss: 3.1252 - acc: 0.271 - ETA: 2s - loss: 3.1260 - acc: 0.273 - ETA: 2s - loss: 3.1210 - acc: 0.275 - ETA: 2s - loss: 3.1120 - acc: 0.276 - ETA: 2s - loss: 3.1305 - acc: 0.271 - ETA: 2s - loss: 3.1215 - acc: 0.274 - ETA: 2s - loss: 3.1350 - acc: 0.270 - ETA: 2s - loss: 3.1206 - acc: 0.271 - ETA: 2s - loss: 3.1106 - acc: 0.274 - ETA: 2s - loss: 3.1166 - acc: 0.275 - ETA: 2s - loss: 3.1115 - acc: 0.274 - ETA: 1s - loss: 3.1062 - acc: 0.274 - ETA: 1s - loss: 3.1041 - acc: 0.274 - ETA: 1s - loss: 3.1078 - acc: 0.275 - ETA: 1s - loss: 3.1090 - acc: 0.277 - ETA: 1s - loss: 3.1230 - acc: 0.276 - ETA: 1s - loss: 3.1254 - acc: 0.276 - ETA: 1s - loss: 3.1220 - acc: 0.274 - ETA: 1s - loss: 3.1182 - acc: 0.274 - ETA: 1s - loss: 3.1088 - acc: 0.275 - ETA: 1s - loss: 3.1005 - acc: 0.278 - ETA: 1s - loss: 3.0996 - acc: 0.280 - ETA: 1s - loss: 3.0849 - acc: 0.281 - ETA: 1s - loss: 3.0775 - acc: 0.282 - ETA: 0s - loss: 3.0687 - acc: 0.282 - ETA: 0s - loss: 3.0715 - acc: 0.284 - ETA: 0s - loss: 3.0590 - acc: 0.286 - ETA: 0s - loss: 3.0559 - acc: 0.287 - ETA: 0s - loss: 3.0510 - acc: 0.287 - ETA: 0s - loss: 3.0419 - acc: 0.289 - ETA: 0s - loss: 3.0412 - acc: 0.288 - ETA: 0s - loss: 3.0349 - acc: 0.289 - ETA: 0s - loss: 3.0283 - acc: 0.291 - ETA: 0s - loss: 3.0322 - acc: 0.291 - ETA: 0s - loss: 3.0309 - acc: 0.291 - ETA: 0s - loss: 3.0315 - acc: 0.291 - ETA: 0s - loss: 3.0294 - acc: 0.291 - ETA: 0s - loss: 3.0259 - acc: 0.291 - ETA: 0s - loss: 3.0247 - acc: 0.291 - ETA: 0s - loss: 3.0172 - acc: 0.293 - ETA: 0s - loss: 3.0026 - acc: 0.296 - ETA: 0s - loss: 3.0039 - acc: 0.296 - 3s 498us/step - loss: 3.0037 - acc: 0.2972 - val_loss: 1.9068 - val_acc: 0.4874\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.45402 to 1.90678, saving model to saved_models/weights.best.VGG19.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ad1a0bf160>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG19_model.fit(train_VGG19, train_targets, \n",
    "          validation_data=(valid_VGG19, valid_targets),\n",
    "          epochs=3, batch_size=30, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "VGG19_model.load_weights('saved_models/weights.best.VGG19.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 47.9665%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "### TODO: Train the model.\n",
    "# get index of predicted dog breed for each image in test set\n",
    "VGG19_predictions = [np.argmax(VGG19_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG19]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG19_predictions)==np.argmax(test_targets, axis=1))/len(VGG19_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "# It is some think rong with the the lfw folder, I dont get all the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG19_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG19(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG19_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "def plott(imge):\n",
    "    img = cv2.imread(imge,0)\n",
    "    plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "    plt.xticks([]), plt.yticks([]) \n",
    "\n",
    "    \n",
    "    \n",
    "def breed_detector(img):\n",
    "    if face_detector(img) == True:\n",
    "        humen = VGG19_predict_breed(img)\n",
    "        print(plott(img))\n",
    "        print('It is a humen that look like a:', humen)\n",
    "        \n",
    "        \n",
    "    elif dog_detector(img) == True:\n",
    "        dog = VGG19_predict_breed(img)\n",
    "        print(plott(img))\n",
    "        print('It is a dog and the breed is:', dog)\n",
    "        \n",
    "    else:\n",
    "        print(plott(img))\n",
    "        print('No human and dog was detected')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "it is a humen that look like a: German_shepherd_dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAADuCAYAAACzpitfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsvUmPJUl23X/8zXOMOVdlZVe1uglxWHHNhRb6tvoGAgRoIS1ECKJEkASbza5isqsqM2N+L948/Bfx/1kcv2EeQ3WTDTTSgEBEvOdubsO1c88dzLzY7Xb6XD6Xz+Vz+WMqtT90Az6Xz+Vz+Vx+3+UzsH0un8vn8kdXPgPb5/K5fC5/dOUzsH0un8vn8kdXPgPb5/K5fC5/dOUzsH0un8vn8kdXPgPb5/K5fC5/dOUzsH0un8vn8kdXPgPb5/K5fC5/dKXxlIuPj4937969+702gJ0Pu91O6/Va0+lUy+VSm81Gu90u/Wy3W223W0lSURTa7XYqikK1Wk3NZlObzUbr9Tp9V6vV0g+fbTabVA+fUTf1Uke9Xlej0Uht496iKNJza7WaNpuNNpuNWq2WOp2OJJXazv08x/vk13hf2u12qn+9Xms+n5fupU981mw21e/3tVqttFqt0thUFe6nbfV6Xf1+X+12W/V6vfTdZrPR9fW1zs7OtFgs7sxJ3Lny0P8PycFTy339fMx1jDvfIx/IxFPb4vMZS+zjY9r+0DX1ej3V3Ww21Wq1SrLt9fAT7x+NRup2u+k6ZM8Lc839m81G8/lc6/U6fefyED/zcfV2xTWSGyfatdvttFgstF6vHxy4JwHbu3fv9Nd//ddPuaWybLfbBArr9Vrr9Vr/8i//ov/9v/+3Tk9PNZvNNJ/P00Kdz+eaTCbabDZqt9saDAbqdDparVbqdru6urrSx48f08Tu7+9Lkrrdrrrdrrbbrc7Pz3V1daWLiwsNBoMEDuPxWK1WS/1+Xz/++KN2u52eP3+uwWCQBvPi4kL1el3NZlPNZjMBzGKx0Gw209dff613795ptVppOp1qtVqlSd9sNum65XKZAMiBFkAZDAY6Pj7Ws2fPNBwONR6P9Y//+I+6uLhIz+x0Omo2myqKQq1WS+/evdM333yj9+/f6+TkpATAUciLolCv11Or1UqfP3v2TH/1V3+lN2/eJHCTpPl8rh9//FH/7b/9N/2X//JfdHp6qvV6neYNJUSJApwDPq7LCbR/77/j3744a7Vaam+8LrdAYx3NZlO9Xk/tdjv16fr6Osldrl1+vytSrvHvvI8+Byjj3PUOQLF+L61WS91uV6vVSkVR6Be/+IWePXum7Xab1hR9rNfr6Ye6Go2G/uzP/kz/6T/9Jw2Hw5J8t1qtNK6sUdorSZPJRL/61a/04cOHJNOLxSLJ9Xq9Tp+t12stFgtNp9O0pn1trFarRGZosysX7/vf/M3fZOczlicB2++rsAB8wGA9vV5PFxcX6Vq+o3Owk3a7rfV6rdlspqIotFwuNZ/PVavVtN1uNZ1OSxqj0WioVqtpOp1qu93q6upKktTr9dRsNjWdTtPAw9wuLi6yArVarTSZTNTtdjWbzVSv19XpdBKDu2+xRo3J57C9er2u6XSq09NTXV9faz6fJ4Gbz+cJSFarlRqNRgL5g4MD/fDDD5VazwEutqFWq6nRaKjRaCSAZezOzs70T//0T5rNZneYjDOUCFS5MfDrY8m124GhqsAq6/V6qX+RocTvvH6+h7nF5/p1cRydoTiQuiWQG4f4bAfn+8bFrRWeu9lsdHx8rL29vVLfqJM2xef2+319/fXXGo1G6RqshOl0mvrjwLrZbFQURQJ+B0pvq4MSbWGNAISAr/el0WiUFKT//Vh2Lv2BgA0a6+C23W41GAz07t07LZfLhPJoHwBut9tpNpulhYYJu16v1el0VK/XNZ/P9enTJ9Xr9TQBznaur681mUxSvZJ0fX2dhKbZbJbAVbphL5K0t7en8Xic2rJarTQajTQcDlNb46J2ocxp8+12q9lsltgh2nw6nSZTod/vJ5NwvV6r3W6rKAqNx2N9//33evXq1R3zKQKJL2wAgeuazWaaG8ZqvV7ru+++0/v371O/vD8OYBHk4kL0+x5bHgLB2K/Y9whSufFwEwsG7e4L/x0BLo5pzgSMbeSa1WpVqus+ExbzMJqDAFer1dLz58+TZZJrowOQdMPWXr9+rRcvXqQ5r9frarVa2m63iWnBwNrtdmJ+3h5/jj8PJR8/j+PurJ6xiybter0u1feY8u8ObADR1dWVNptN8mPR6GfPnqler2swGOhf/uVftFwuNZvNtFgsEtJPJhN1Op3ErBBGBr7T6ZR8dZI0Ho/T/fyWbgAN5uNCMx6PtV6vVa/X1ev1tNvt1O/3tVgs0qRLNwIyGAzU7/fv0HU3yeKkuCbiWtd8UPZer5fu3d/f13a71ffff5+Abb1e6+PHj/rw4UPlxEdggQEjSAg9JjLm/cXFhf7+7/9eHz9+vGOaRfYWQQ4z/yFZoEQg8c/9+vi//x2Zg7NPX4hR0Tjg+OeMkT87ts3bUwUqVf62CL45cI6//e9er6fnz5+rKAodHR2lOcwpOPcjStJgMNCf/Mmf6Pj4OLkfWD+Yo5KSTHNvq9VKrKuKHa/Xay2XyxLLw6Lyden+QeYnsjQUqrfhMeUPwtjQjufn55JuBsu1wcHBgXq9ng4PDxPAffz4UcvlMrE4TNf1ep3AcT6fq9fraTQaqdVq6eLiIgGfJM1mM11cXCT21W63JSlpBCYU8w+zj0nClB0MBlosFume/f39xHCiJqpiHQDgdrstmdeYhM4glsul2u22hsOhLi8vk5DQxvl8nhisF57tIEahfbVaLY0DbLHRaGg+n+vbb7/VP//zP2s2m5XYTAS03DOZZ1/YOfDz9kS/UtTkOVDLAY5f4+wrAqGDujNt2hKBLbKq2Nd6vX5n3h9ilzkwy90bwaNWq6nf7+v169dJZnKA5vc64L9+/Vq//OUvdXx8nG0nACbdygr3Mm5xLLC+AK9ut5usJ9w6jJEHqXKlyj/72PLvDmxFUajRaGg0Guny8lIXFxdpwfT7fXW7XTUaDbVaLb18+VLD4VBff/21Tk5O9MMPP+jTp086OTlJpiSTulgskn/MzSlMyk6no+12q/l8nnxxAOJ6vU5O5OVyqel0KukWEBaLhRqNhi4vL7XZbHR0dJSegX9LKoOVC7dPUjRBa7Wa9vb29OrVK/X7/RQUkG4AdTKZ6PLyUovFQicnJ8mxzULE77bdbkvaumqxR0CRboR4s9no4uJCx8fHarVaOj091d/+7d/q48ePJYCkP/6cKiH073Lg8pAJFksEsQh0OdPHQZRF5cBVFVzIAdl97YqRZu7LKZVolvozcvOWqxdlSEAAZR/7x29vZ6fT0c9//nM9f/48BducETnAx+fyDFi/9xtQOzs7kyRdXFwkC6ff75f80LA46ncCUjXu9xGFWP4gjA1T89mzZ1osFjo7O9N8Ptf5+blarZba7ba63W6y658/f66joyO9e/dO0+lUFxcXOj8/Tz/j8Vjj8TgxNCYQ/wBA5T45/qc9w+FQy+VSJycn6bPtdptMPmdks9ks+fRevHihwWCQJkuqjvo5Y+EZz549S5oTxykRVFI9ptOpiqLQ5eWlVqtVMsOJKC0WizuLOpYqtkGgZTweq9lspnH7zW9+o//7f/9vKQgjlSNUkQ1VAU0O0HJt4+/cIr+P2eT8XRQ38bnWWU8Ve7qPEUZg8vSbqn7lTOoIbnznzv5cXZKSdYFM4yqISjTHjPb39/Xu3Tt1u907DIxnUaekZDYSSV8ul8kU9bmv1Woaj8c6Pz9P1x8eHqa15JFaxoqMAZSzu22cIDw1/eYPAmzSjVAdHBwkjfPDDz9oMpmklA4YVbvdTj+EoV+8eKGXL1+mIATBANI5SA05Pz9PCxOB8dw0GN9oNNJ2u9XHjx81nU4TC4Q6F0WhbrebNAxm8GAwULfbLS1i9xMwITEnD0HodDr65S9/qbdv3yaTczqd6vz8XGdnZ7q4uNDl5aXm87kajYam06kmk0lJUDFDMZfv80n52NNWSUmA6cvl5aW+++67FMp3350vsBwb8ZIzq+5brFWfVzGYHKhGkKCfVeaj11MFxNFp7/3mM2ewOX+ePzOyuxxouhkcxw3G1mq1JCkx9cf4NFHWMHwUclQG1IuPrdVqlUxelKCboNfX1zo/P1e73dbx8bH6/b6KokgK2nNLKQQoogKCGebm4THlDwZsTM7BwUFiab/+9a8T62JAnVYzsO12W51OJ/nmer2e+v1+KYcHwFutVsn8nM1murq60mQySQGARqOR8uD6/X4JmKTbPKlWq6XZbFbK8RkOhxoMBmq1WprP5yUw80WV+9ntdknAvvvuu8Q6Ly8vNZ1OS0EKTM7JZKLlcpnGDvAtipt0F/LrInjkAM5BFtOfaBjCzkJ3NhiBKidoMVoWr4nfu0xE8zXHarzOHFuLoOELJpqJXnf0j+aUgn8WGauzuyqTsqqNVf6weK104xt++fKl9vb2UrvjmFW1Gfb0D//wD9rtdil4gAuEZ0e/mpu1rVYrAR3rbblc6vz8XOv1Wi9fvkxWBfXwXKwSAM/TpVzu3LyNfXxM+YMBm3RLlQeDgb788kuNx+OUX5ZjOzn/SAQ9j+5g0o5Go5IAUq/nx/lz3Fcm3TVhPT+ISOlsNru3HoAa0CYy/H/+z/9JwAsIUwgi8He/31ev10tBC9hnvV4v9cVLDuj8b4SuXq9rsVikfh0cHGg0GiUfpS+2GP31eiMz9Xtzn/lC9M9yshLbX2V2e2CE4uZdBGUPiuSUwn0l16eqEoEtgqy334HXx6Xb7erLL7/U8+fPJamkgJxRopAjO2w2m1qtVvruu+/SnL958yZZJmQqeD8AHs9gQJFTN23o9XqSlHap5PzO3H92dqZms6lOp3NHbgBB6Ta49xRz9A8KbNLtwms2m8nmx4b3a/gdAc9R3LVfFPqYfOn/R4rsZkR8Jt/DdDqdTvK9RUZGHQCBO/13u50mk4lOT0+Tv8Iz0RuNhg4PD7VarVKw4vz8XNvtVp1OR/1+X41GIwVRcqDmTuMcoPkYEXzZbrcaDoelHQiSSqk5DvI+P7H++IyH/s6BwmOBroqRSnlTNKaG+GcRTGI7vF5/dhWwAbJeb24+Ynsik+v1enr79q1ev36dQMgXu8syoOb9RDl2Oh11Oh2Nx2N99913Ojw8TGYtiq4oipQvur+/n+Z+uVzq6upK4/E4y6oJAOL7JbXK1xSghknra9kVpo9FTDt5qPzBgY0BIVrTarW0XC51cXGRwK3RaCQWFu/zv6NmcGYmle12jyL55LivImoZAGy1WqlWq+n169fa29tLAx6dn5IqQW+z2STfWARx9qnW63UdHR3pyy+/1Pv371O7ut1uyX9BVDcyjvsAwX/XajWdnZ0lMJ1Op/rhhx90fX1dWnQ4muMzqj7LLeCHBDMu8scIcuyT10VhTlx5uTxVmXM54Kwyj3OMwhVVbBMl509zplUUhfr9vn72s5+VmFqsi/sc1DyXj509y+VSl5eXms1m+ou/+AsVRVFKgYKdrdfrlP9J23HzeAoQFslisdDe3l5yiaBweT4m69XVlRaLhYbDYQJo+gQIekI+0daHUkS8/N6A7T4ToUqreqnVaup2u8mHtdlsdHl5qfF4nByemF/45HwCYyZ01MwMkrMiBy1KzvmPYx+2hfY8Pj4ugYszMiYsOt7drCVVI14HI3v79q2++eYbLZdL9ft9jUYjSTdRLXwSPIuk5NwcsBBzkVMUConGRVHo9PRUHz58SJucpVtQq5rvh0ww7vGxj9/dV6q+z8nUY1hejiW5UqtiYff1NQc08SfKX1Wf3JpgR04O1KpANtax2Wx0enqq3/72t5rP5ylt6cWLF+kABcAeYEKeATICaKvVKillf34kDdEtgxJnOyJszecEguCgSY4m2w4fU54MbM5wonDQSM/FYhHvdrs7n0cBdKYCaNExoqWwO5yXgByAiEOde2NGM5PO8zyy4xNDX32QiSJ1u129fv1aR0dHabJyybkeAIljhcBMJpP0WVEUKUl3MBjoL//yLzUcDvVP//RPOjg4SMEGssLZIXB8fKzhcKjJZJLGhxIBLpqsCHy/39fh4WHqj/eJ9uH3iE53Sm5x5RZg1X2PBbnHAGnOdPTPogLzPlYp4pyFEK+rYnj33VfFEFH2X375pV6+fHnHAomgydy6sqfexWKhT58+pa17gFen09FoNEp+3l6vp3q9ruvr69LuG/xxJycnOjs7K1kZrA1kxdmxk4P1eq3JZJKegxXkzMwjvNRLgnpVzmGuPAnYeBDCHc225XKZwMkFaLFY3DlZANpJXSxI6DOUmMRaIoQstui8BOz8JAPq4nsXIIrTZKkM0C5IHqkdjUZ6/fp1yvniO8/tcVDL/Xhf6Adjh+9uf3+/tK+VQr3z+Vz7+/t69epV2jDvQQTqjELvWrUoCi0Wi/QMj3jlsvFpqyuOWBzEuDYHNIxxbl4obhoiI7m6cuZoVJ6wAal83E+u3ZHNed2u/LzOqrbzdzQfq1gl8ttqtfTFF1+kfcAuTxTWWpxrl/doPfBc5Ey63V3j0VHWEKQB/ysReHy8sDDAzmVcurWa/NAKP7TA5dGBEFIjlcnSY8qTgK2qYoRhuVwmFkUB8Nz+xx6fz+eJoUi3TlYGs9PppH2antbg+8cYCJA9aj//P5pgtC/mHTmIODXe7W7C41999VWKhtKe6Ex3LeaTxzN9DyqOeR+7xWKh77//Xu/fv9e3336bEhi9TYTYl8ulrq+v9eHDh9JiYoEghARoqKsobo/PIcKLUmFMovnpZlsEjhwYRJDIMa0q05G+PmS2uezcB2y5OhzgPcqe82FVsc2c9eFt5zn+Wa5frqTb7ba++OILvXnzRtKtMnOXRQSv+BlAF9vAdZ1OR8PhMKVQxXqcJV1fX6f7e72eut2uFouFiqJIR3WxFiSV/HCsd6L+OSbnPmzGE1dNv9+/44d8qDwZ2HKmpGsJGs9es93uJhrDPs9arZbOZlosFrq6ukq7DDCznAWwIHk+TAJ2GM0JLwyUa+Fcsmn0CbhmBHS326329vb09u3b5OvA3+DRKYAR35lrWu8DPrJoNhXF7cb2//pf/6suLi7S+W4+B74gv/322wR0FNfeaLt2u11KtCTJWJJev36t7XabNLMzSvrlQhhNTEpkL4xnvC5nflYJbpVp6MUXpIONtzEyxc3m9rijnLnI548BuKg8I+DcV+I1jUZDX3zxhd6+fSvpVo6Royrrw+vKRUVjqdVqaScP80ZkfDqd6scff0yBKV9r0u0+a5LUSRFxi4rdM/jkOBqs1+uV3DbIrgMdLJBUripWXFWeBGwODO5MRtCx24mmMRhowNlslkyd7fYmIDCdTrVer5PjGnPSJ8TZHs+ms9GMzPnJnJH59g7uib/5231+rVZLv/jFL/T111+ndpPM6mao+w0i46FQb7PZLJ0W7ON8fX2tf/7nf74T2Mj9Hf8HHKsAAYEHYE9OTrRer3V+fq7nz5/rN7/5jf7u7/4uzSNzzNjFEhd/BBX//Cklx7Qfuj7XlugX9ra4me51UGJ/Hwuufl3ObK5SDOQQfvPNN+lsPhQZys3dPTnzGFDzfDTqYCw2m5uthr/+9a91fX2dDi/1M/kGg0FidvF7CMY//uM/6vT0NO055bnOEmNwjTWy2+1KATfcU5vNJrmeiJz6fD6mPJmxucPdNRqOTjpMR+kciYFQ3Eajoc1mk44LwkRi8GAXzWYzJY66FnQHduy4O/1dkOPnkda7eQbw0IZXr17pT//0TzUajRL7ZJLct+btcGHzZ0gq+Sk9UuY+SNobTWVnFfdpsuiP4TPaUqvd7O07OzvT+/fv9T//5/9Uv9/X5eVlKT8u9oG5iGy5isVUfe9jFd0H8e9YbxxrEr2Lokjb6HJt4DOXIa+n6hlVbDMHUFXstGpsXK7ZPzwajUpnBOJnRvlHH5V0Oze4ILx9kALkabe7OSX6P/7H/6iXL1+mNekpIlzv1g7yyknDbHSXbo/DYu3gv6PtbtX5PR49deuJtlTJwX3lyVFR0JgBg62whYIoZdR6HMRI53a7XXIM9nq9pF0wYT3QgADSyWgeURgAn9Scb4P/nYEwcbvdLk0ywr+/v69f/OIXOjg4SNFMN0EdbNxJ63VHAXNhdADzCQZYIjjn/F9V4+HmtpcIVvP5XBcXF3eUhUe4Isv1+1EKVaXKrGMu/Dd/P4WptVot7e3tJWbjSivXjpwJzJi7zPh9/jv+netjNG8f6oN0k+rz/PnzO6yGqKV0s14i8OSeW8XY6d/BwYEODg6Sq4j0Hv/hfD5XwC6f7O9GyfvJu87+fE3jc/M8Qm8nz3aWnVNE95UnA5trbB8oWAuoHxuMr8rr8ZwvgOahsK4Doy++qLViVnZsj9fF3+7j2+12iUF+9dVXevnypSSlIAV1Q6Wlu+kCke1wjZsWflJCXEgOKv6dL75Yt7fDSwROH4tctneOveTA1P097j7wwM5DdTwEaM5Qc4Xr4+GHPi7RDI0/jFH83v+P7fFnY7XknpsDuPgdAaX9/f2U9I18AGrkKQ6HwxK4PXYco0xMp1P9wz/8Q0rupl3xWgcW72NR3ORPsmNou705cp/5x7JxEgBjyykfDx5g9T1klVSV3ylBlxQLB6lcyaFsvV5PCaegOpFC96Xxt0ceXSg8wuKRTtrH/1ULg6ihaz53Zj5//lxv375Vs9lMJ/m6T8DBOddv/Iv8z33QcRys9xUPMtD/3GQ7k4ugEschB3LUzb1Pof9xIeWYppszOTb7kAkanxefxfssPFhRBSSxHn/mYxZSrMeVJPV5iUwvRiylG1nkjLTxeJyUpjvfAQaeGdvk4x7n3dtBUMkz+u/7oZ7IrvHTciAEZihrI/rVmBPOOHRwc6bIPBIQY3wfC3K/884DBghG9tjFgK+Nwn3dbjeBJWxiPp9rPB4n5hc1OKDodfmkuz8O847PfPsHIMWgHh0d6Wc/+5mGw2HSMLAC/AZ+ZHYOTLyNvqCh4w7YuX55n6oUBwJH/TmGEU3zeI2zrRxr8wzxKkYVhT4uCkkp6ONBHv+dM6seYmt8zyJwdpFL2/D2OntnAXrf73u2F5ex+ByK9yk3hr1eT0dHRwnIPD/SI4qu+B9a7DzLg1qsWTasIz9uCVWZflXkoN1up9NxSBvCHCXndL1eq9/vS7pV1tLtCR6eK0phvd73/Fz5vWypitrxKffF/9vtdmny1uubc57W63XpvZ1VWoj/nT4TxWHAfA8bwBl9K51OR2/fvtWLFy+S2QgQRTrt/Y+CHH18PN81WZXPMAqa98v7K5XZWA4UovZmbBDqKnPLgSb3N3VyT2SCsc7onPY2xaTl+0AiJ0cRXGPQxX/7EeyMezSTH2IHuXmpAvnYPr+eth4dHWk4HGo2m5VeYzedTtM5g51OJ+WAItcoWy/OzH3sXKGTnQAxiErFx8MBMkccut2uhsOhTk5OStfww/tyJSUFxKkemNw+7kRj/ZUBcTzvK7/XTfD3PTTHQnKl1WppNBqVXt672+0Sk4NN5YQ/Z6q6sLlG4H8EyIUOX8erV6/SseMk41JyLyeuMnfiOBCAcJYI4/JFEn0QEUgQXAe0CKwP/c/vuE0nttmvdb9aXAwOmrFduTQIZwb+6rWYNpMbhxwz9N8ebIpRZZ+zCGwx79Dvi3XkPs9d43XRX/cXc4DqbrdLia1YCKQWeX4XwIx/C2d8XBc8B4DY7XaJYZEs7wrO58jr8Eir18uJIbQJN0tOcZHGQWI+jDrOFWPkCetPJU7/bqd7PBpp//8TbYmykiLC/64p3AwEGOLCwcRw00+6DXjktDQH+ZFd7cdvw9bYBCzd3abkhbPSpHLyrr8kNreIqswnLzyzasJz0b3cc6oAOQeKtMPBjWfdV1dcyN5+D0gh5M1ms8RsI3vw9uUWoz8zjof/X8UsI9Pyv/153vYIKDmWkQPUorh5gdHe3p4mk0kCNnaU+MkbmHm4f7xtvi1qu92WAnlcjwwPBoM0xoy5KxjfkkgeZ3zxMuM7Ho91enoq6fao79yG9d1ul3Ye8OZ5V6iAW5wzX1+PLT8J2HIL7CGm8thCp9nY7tuNcoLoAubnwLt/AnCqcqy7idput/XixQsdHx8n4XJG53VHDR1BhglBI3oCorPOqvQCBwFK9Ic4mPi4VN2XMyWrFnGuVLHTHCg7A3YAzAUmcrLj5hFj76a0z0uuDznmzrMim/dSBeh+r/8d2bK7RJzV5Z4v3YDO8fFxYmsEB2A3mJq9Xq+UKCup9AZ2Hzd+e1v8e47Xpz6yAQBN/vYx4MfXBSkezrR4L4ff4xkAvP/Xd37wHOY45xt9SvmdGFsVI/hdSlHc2v9MzHA4VKvVShOIBnf/ALSVgUTbVzn4IwjB5A4PD/X69WvV6/U7Jw1wL3XSVibJTR6e5cIdTdcI1M5e+Cz+7fdW0fPIEOLijNfE/6MgU9xfw+cumDmTNIKOs1pnOyykmHrCGLu/jHHzsYrjwt9VfaT+3BzEMY0Rz/g8r8/HIfe8OIZFUaRTnnFvIN8oZvZgoiAwIwEW91HxP6dnDAaDO3mdRVHozZs36dV9tI362FbFOnJFHNcOf9frdXW7XXW7XV1fX6dtkFzjYMVapX/uevBAGms7mvCPKT8Z2BgAwrtVk/lTCnW6YCPcTpPdtJRuoyru6PdEP+nucTVon+32Zi/omzdvNBgMSgPsEwnVhkl60qEDWxwH2hXBNEbh/Jn+f1wMEXR8/HPszBetj8djGJovfG+DJ2zG9vl90YHvviAfM2dgsX+xDklZYMsxp9xYMPeUHJD7vXGMI1j7dw5wcfy8L4zF/v5+eqdGdJXw3o6iKFK6B697RMZ9LzEF5UtKlrs8WEscw5+LUvtYVI2bM/J2u63BYKD9/X3NZjPN5/MEbqxb32rlmQn4+nLj9lPx5Cedxwb78Ma7PX/fonlMQ5kMN0F9YvzlI3yOVvNoEu1B26MF+Z5o52azUa/X06tXr9LhkavVKi1AF1zXjrTPJ9yZlwswRwphKnvb7zPj/PPNz/wRAAAgAElEQVSHwCjHEOL/OfYm3c39yy3KXNscLHPMLbbFI2W+Q4PFm8sH83qjL8+fF8fIv3fTncUUx6gK2Py6nFLIjXturuLY8lm/309EgRNqdrubvDzOTyuKIkVFkcvlcpmO78a/xjx6rlutVkt7souiSKdz5LYCVo2JyzjzxrmBo9FIV1dXOjs7S+ayvzUtulSclQJ6rEmpfOo0fY+M+aHyZGBDAP3hUGgG3QfEGYSnL9xXOF4HsKTDDOp6vU7JshGscgmztNEpvv+0Wi09f/5cL1++VKvVKqVx0Ec/vQAHrJtPTtdpq/vVrq+v03azqm07tLWKckfNfx/4VZUcG4rPr7omAliuTV5HzuT2drjAx774dbnf8dlV9UTGJZV3r8QI+33t4v/Hsog4H94ePm+322m3gVsa8/lcV1dXd5gqawzfcTw6i7WCD+z6+rpkUdRqNR0eHmo4HCYlHNucG1/GiPHCL3h0dKTxeKxPnz6lE3d6vV4KeOAn5zQb/IbgSI6sxITenzL2Tz5ocjabJQehDyCT5vtEnUnxPoOHkHe3K2+ozy3eGC3zNI64qOL1bv4AUEdHR3r79m0SLrSNpHSPBwCkMpWmr95OAM3PqiKNhTf4VDGE3MKm3jhW8Rr/LjLBKLRRUHLAFc3ih9rLczyFQConZMYFXwVWFI/EOdN1wKxiUTlWF/uXA/KohHPjGcfBvwMgoyy63CDn2+02nVa72+3SOz/Y9E57cH3M5/O0iZwCGO52u2TlxO1+jCE7BBzQq8aferiOe/b397W/v6/NZqPvv/9ev/3tb9N9REQ5zYd+c7gFJreTBdZvURTpvSd8jqka5eK+8iRg22w2mkwmCaAAOISAh3Oyh4PIY4qbKzgio3OawvNzDmWEJhckcAeodLOh+Msvv9Tx8bEkJSDipAj6hCC4T8+FN9aNmQCodbtddTqddL07yXNmSw5kYong8FB9uftyrCJXZ+77+z6PLCoHWPGaHBA7qyBIVNU3B5WocHLmjH8WASwHcrniDCzHQNxVQR+4jmd///33pXHiHbPeRo+Euukp3fqWWROdTidlFRB8cEXDiToAlvt+3fXC/wAu1/jJHQTYYGbkZ+52u0SCUPSMZVEUJdD2scQC4jQgct7iXD9UnszYsMvJg/G3Uec0Iu/2fAqNLIpCo9FIe3t7ad8cxf1mntDpoW3aSl0OQG5Ct9ttvXnzRm/evEn75sgb2t/fT+yAdueiNhHA0XCLxSId/1MUhfb399PkA3gR8KnL681N/n1sKwKE3+fXel3x2nhNfH5si7ezCgwjg4psi7mLQMTYEuDxQJWDXgQ3L5E5ebvcaR5lxRWgP8fb6++A8G2FtVotHb0llf1GgBBuGxYwvluAwevzOXUFy9+eGYASjcE1SQlskUvGlmP6pRvg7PV6KaJK5B/GyJpGOR8cHKS0j3/913/VdDpNQbm41RF3zGw2u3NSEPPNMzFH42lBjylPPmhyvb49UZPGxozk1WqVBNCdto8tRVGk41sGg4E+ffqUBjTHAiMj80ijf+e7BTBBv/rqK/V6vWT7X11dlXx7MFLqdu3jwQB3fPIOAd7qPhgMEltDiD0KFPvAGOSKa9NcqWJcub9zoBbBrgrg/PvHsroqpsiYxjPDEHzPSWQR+3N8YbiiuU8x8HcEXPoD+/ETlLnOXSR+gAOL35Uvf2NO+Qux3bJga5Sk5GuK7YqFkz+KonyGoL/HwNdJvV5PbzujzWxip46iuAlSHB0daTQalQ6W9MRcktS3223pHbec0caYkZDreaTNZjOZ07Q1KmxM2Lgr6LHlScDGiRzxaCIyk5mAyKAeMkXjQoL18B7ETqejwWCgVqul6XRaOqcfrUObcFC6GSmV0zSkm1fYffXVV+llxByRfHZ2ppcvXyZBA8i9H9B3TG7/HJCbTCZJc3lKDMIdQ+aU+8zL+8zGyPRyxQGtSlhy4FfVjlw9ubn2+/172utpI16Pz2sELPeBufuBeyNwOiPLAZufzSeVt+fFhY2l4OYVEXI3GZETronPg10B1hyZjdPcxyMGrAASDnCkTgdQT2ehjS9fvtQvf/lLHR8fp+tpsx80ySsZHZxpO0GJuPuGN2qx1ev7779PL3mJu0cwOT17gfrdD8k8/JtGRV3DSLf5ME43abQvINgJiX841qXyhnA0EKh/cXGhWq1WilaSO+chcgITPAP7nB9/PyapJJyxVhRFer3fycmJptNpyX+wWCwSsLlZtNncvIYv+l8A0MlkcieJVyq/5CKObWQPlCrTscrkdLCN9/t1/vuxjLHqnrgQc6DL+Lg5GlM7EGo3+71f/j9g4/dKKrFhVzreppwl4c/IuQlgObEAmrhmfI14alFUKuRnAqqu+AA771dR3KRqkJfm+ytRDrxPJKe46vW6Xr9+rW+++UbD4bAUlHEF4iDMvJIpQGqG7zrwfnHa9Gg00osXL/Tp0yf9+OOPuri4SLsqPNqJdeeKwpUn/rUIfg+VJ5uiIDV2th/MCDgxyKvVSpPJROPxuORI93SKSDWdDTSbzRRJ9KQ/numsCYYIRQZAyAWSlI4t9zdNzWazFK7+9OlTess6A+0aBUGlj9H3QWSHMUIQPFPcz3O7D3D8fwchQCve60IY/6Z99zE1+phrVxVgVbHNaOpWFWdfjK3fH81ub7+DCWMb88QcRLmf+3KMmeciS7QtmmHUFRkU+ymdabDZOx6k4GyTZ7urwvsOiO/t7anRaCT5xmRjDREokFQ6vZa2swf64OAgBeZyQJZTes5gsagItMVUEEBvOByq3W7r4OAgvfz86upK5+fnaU9szi3D3DCunAzSarX061//+l6ZojwJ2KbTqf76r/+69CouBg0g8xdOeA4aAtBsNtXv99NpHTTa94cCXNPpVL/5zW/Sa+VqtZvNu/jaeJEqBVMUIKzVaumVgGy6bTQa+uqrr3R4eJh8amiV8Xisly9fpu0o9Mc1ynZ7+45Tj3gxoeTYeRIjR5878CMkOaaTYydeouB5ZDYCU1zYUYF48XsfAqVYHnNfjmVGc5cfNz99TFio7tCWlFwhbh4CbA7aVW31MYNJAiiYamzzA6DcZHLGw/VFUaRgGwodNw0mI3IkqSRDPg673S69UczZLOYsMoWF4GsC8Go0GhoOh9rf3y8FwHKgljPZ6buzT9oWwc0PjWANNRoNHRwcaDAYpLU3nU51eXmZTFpAjnZ5sA0y9djyJGCbz+cpXwUmwm9/G7u/7NhfwOtvuXHb3YWLyeVwSU4ShYY7+/HIKAvGw8ikWcA0V6uV+v1+etXceDzWycmJPnz4oMvLSzUajeR7YEJYSL5TAf+FCyQTy6TSJo5ORov7e1EjeEWBuq9Uma0RDKq+8/8Z+7jgc6zvvvbkzORYqr6LJiDCHQMFLFKCMR7Ekm63qbmfzX1vcTdJjgEjh+44R67d/M35f9zfFVONImt2E5R2ROBgPDyYQpscBOizby6PrHQ8Hut//I//oU+fPumbb77Ru3fvdHBwkNZUzo8VGTpAiB/u8PBQm81GJycnCewBNHctOZgyPm5W4+LhmKarqytdXFxoPB6n9URi8mPKk4BtNBrpP//n/5yYljsdY2SUQfE0CGc70FV3LKKtMNmurq6SDe+0lAJguj+D/9Eyw+FQktK7Tl+/fq1Op5Mo8fn5ua6urrRe37wCcG9vr2RWwwyIMPnnnttGXz2rmjbg/2Oi476+Kob2EAh4uc9EjNdU5WXlWN9D7fDFmmsLJRfNzYFpjrEy9x608gUPAHh0Otdvl8fYxhzo80zf58icFkVRYhhev7fPo/TO8JB/6uUIITf5nMmy3mBFyKjvdqFvAFu73U7PXq/X+vTpk87Pz/U3f/M3Ojo60s9//nP98pe/1Ndff538Yuxs8I3zcX5pL5YTfaN+toF5KgnvbfBgA+PpjHo+n+vTp0/J3+1mqm/neqg8Cdh4OzWD5wwGofIsYgcqJsMFi/8ja6AuKK5TfITYHadMgEfXCBIUxW36yd7eng4ODjSfz3V5eZloMOxvf38/BSUAof39/dJiwpwAVD2fiIl0LcyxMA5sOf9OBA9fYPzOma65eqrMwpxpmitVzO4+9pb7PwKZs7GH2uJzSsE84fqYbxZ9NXFhVslblUKGgRAhdIbu6UfS7dvbPK8s9gFzmTpI/XAG5grAgZIUELL2PcEbtuWBCsDTmRtguFqtdHV1pffv3+t//a//pWfPnunt27f66quv9PbtW71580bHx8clhooyjJFgTGRcTP1+Xz/++KM+ffqk6+vrOyYp8p8LzsxmM3369ClZaY4TkVk/VJ5siv7qV79KDctFe3wyGEwv9/3vpih+A6f20YnvERYHViYWM7XZbKZtJLVaTRcXF4mlMUHktXU6nZRjgw8PIXFmCZPzrG1PKdntdslkcm3sUbqqBR59HfeNn3TXyX/fNVX35YCU34xvbr68rhwQxme74sk9L2fOMp9uovlLtf0t4hE8+Tvu9Mgxw9hu5hpwI2fR90FynVR+Kxn1+HOxOGLADFn3cwO94CPGJywpWU3SbT7lbrdL8ud9AZApgNN2u03ZB7/5zW/Ubre1t7enL7/8Um/evNGzZ8/07NkzvXnzJq0NXwv0o1arJXO22+3q8PBQZ2dn+vjxoz58+KDT09PSKcBemFO2kfEqvypZemx58paq8/PzezU3n1V9Tskxj8jmvC4HPQbVExAdeCSlyWTSedGER2LwD5LecXh4mDQfNBhN6Sx1t9slP4sLrpsdTtfRdvgQ7mMuPj7UWcXknMHlBCDHtHLPifdGcI3zXcUufR5jG3Kmdq54/ZhusG8ijlL5hdONRiNr3sdUkgg6HnTxdnvaAwoVOcF1AhvifpcTlCVtY3w8iupjj3WCEz36GYfDobrdri4uLtJ+UBS2pBJYeDqKu4pga27VuGVBPdfX1/r48aP+3//7f+p2uzo4ONAXX3yhb775Rt98802KqJL2QYYD1gxEo9/v68WLF6WI5unpaWKcvtbJ+6RvPocR5B9bnpzukdPY/llc6FWLyqNelOgrAAyg/tTvIOPRGF/0RVGU/HIereFIItekHPZHHhFRVfwUCAKTEs9ix/fmpjHfRw3nbXTBqhqrONn+O3fNfddVfeZ9eUy9VZ9VadaHNK6DmV9LBJQFAtCguFBO5BsiC+4WiWZNlQz75zwDQANE3TTd7W6CUp6eEIHZ2ZKbcRR3u/jpuYxps9nUwcFBys2EQTorI1LqEVA+d2bn0crcPLr5jMUyn891dnamb7/9Vn//93+vFy9e6PXr13rx4kUyH7mO6CVmJyeQYNKT/kE6VI7VViluyn1K0ctPPmgyx6ik2/2Uu90umXQMMCHjmOUdO4Mw4nB0P5t3NrK72HF31vJMd9hC09vtto6Pj1OaBxOCAAOgTJgDG88EwBA8Z5L+0lsXqpwp5OPgZmCVyRpLTG24j2l5XbkxzCmuHFOPYB3rzfXxoTpjXSxcP+vfWZV096CC3W5XArVcu2Pbognruw4YX6L9UtlX5iae/w3Dj2Pj0UDMNG9Ds9nU4eGhDg4OdH5+no7pguXRHh8/JwCkOrnZ79vBPJARk3MBG4B2Mpno/fv3Ojs70/v371PaVUwSjgrE3VbOXJFxz2uN5b65eqj85IMm40NZyO4kfP/+vb7//ns1Gg29efNGz58/T4mBnu7hHWYw/V0FXBNNhxzD8O8obhqgwWgv4MWLaqHEi8VCo9GolB6AsACInoqAgEZAIokxHmsemUmctMji3LTKTTLjkxsbny9fOFVz6X+7OfpQeQy7e8x3brLxfII18To362Au7g6IwB6VaK4tkXUxv+7n3e1ucsv6/X7yoTqg0XZ3j3geFi4SEm39Wfzf7Xb19u3btH0K94kr5miqOQkgMRhfMm2P/k1P5PXfgCdWjAP+en1zFBlvpM8pf8x23D/+Zjba6TuO/Ch+xtjbGdf/feXJpmik9XzuJuFyudTV1ZV++9vf6scff0yRE3/zexRK31zrp+D6JnRfZDkBpS18hrZgkj1xEsBEYA8ODpKmwqTkjUB+zLg7TL3wLPfLMHFMqu9VzbGD+7SSfw8j9EUQr433RNO0isFUPTN3Ta69sf4cg5PybzHnc0+85TtAg7/jdiSPUMdn5RipK5aYK0c7aCNM3KOD6/U6vQOXY71xdwB6sBl/7yfySAqQpHTElbtWSGjd39/X+fl5YnNuLuZSTZjrnAlMcV9iBDPf8wog5kx82s/zrq6udHp6moiB52zGNI1a7fYcOtYGa/4+mfo3BTbocKSc/A9qbzYb7e/vp0S8o6OjUv4bfzsA+CC4P4BO0YacpuI7fvtC8M28UfNut1v1er2U5gHbrNVudjlwjWdGw9b8ec4UfEfCZDJJCgHhZbIiKOUSJCMg8byqyff/q8DF/3Yhz42n9zHW+VD9jF3ORM21G/PLI54eQKA+fJ+8ydzzCEmmdse21+/mVizRBGXhuxvDFfJ2u02O8Xq9XkrlIZcRpul7hQFi1oynDgHc3W5Xr169Sm97oh2Mnytt75ebzs5+GCfA3wGN+9yX6RvikWcUvqdfcR1A6Em2zhTdJHZy4Vswo4XgLp1otTxUnnwem5/c6Q9z2323uzmvjVfYdTqdtI2KQfPriYpAt+mwp1K4YzaCmg9KzuRwLec5P9Tb6/VKh/JtNpt0phX+AQcudlq4OcA1ruW322063YBrvD05tkWbI3jlmFD8/j5TMH7nY5b7Pl7Lbx/jGHV8qP74f5Vrweticbpywl/rJpezWUl3TDxn0/6cKivAAbAKDNfrdQpakDMJoAJEzvC5ZjabJTlwcODZBAxevnyZlH7sh4993BMKoHrqE3Ja5W/jfwdq/5vi/jiCHtKNS+dnP/uZlstl2nvN6TY+/8wpY+cWFd8B7lG+Hgtq0k84toiEVZDcTa/cYnWE90Y69fRX5NFpz9CPgBWZQhTQSHt9wHySpNuTTjFBeZkGzlE0NcDFS2tJznTzuyrrHabmpoO3n7Y7C47f+T05APBx4rn+jNim3HeMUSwPAWTu7yqGmTMLpVv/anRkkwvohw3Cytgr6akH1O9BH+5xE82ZgM8j1/q2KTeffVsTrGg+n6c1gA/Z91Ziinr6h59ZGMcOa6HdbpfeQAWwwUzpC32L68yBwFlj9GHF8YdtuiJmLDA1PRBWFDf+wBcvXujg4CAl6nKiB9cTZMH6cXeKEw5AjO98Dv9NGFuz2dSLFy+yghqjnAxSNGciPXaTkWucsbmNH5lajtnkmI6zvlqtVjJRGFRMX5yiR0dHajabpbC3dKOZRqNRKQMeTR3NntjnCFj8zoGJ1+Ng5iBI/d73+0oODP050bx+qK7HfEbdueKLFGF2lwOgxgkvkhJI+JY+N2v8zemudKkfYHEQi4sJ0IrARa4aoIKZ6AtcUsrE937E1BP2NEdwkW7W2XA4TP0qipsdLH7UlwO4kwzAL7I1+ooSoA7GhPGkzSgYTpa+vLzU2dlZGv/ot6vVarq8vNQvfvEL7e3tpYT4yWSSTvOA3bHmc7LLHPh65zMY72PKk31srn34rEqgI6hVLUIPUfPjIXquZ7AjO6QtkfFEtuaUPIbhicLC1vb29koaMmpz2rDb3aanSGUgRaDddIsAF8fDF4j3MTK1aEpRImuLf/tzcuwu164cw4sA7UrnIeGLisiVDOk4zWZTvV4vOedZgAATp8P4mCBHnprhW4o85YIF7vPobQME3G3irJuxh/FjefBcxgTw8c9gas5K/X9OwABsSEPy03edcRJEob/uNnFZi35nfscUEGQcxvjdd9/p6uoqmdj+Njo3ka+vr3VycpLYNcrAn08d+OSinPl8eh/r9ZvTf09OTu6VLcpPOho8fua/aUhcSPeBHMIXdw8wIESk3IzMMcb4TO51oPOz4PgMYOI5LCjud+3mGptnuaPYhd/Z4H3A7v3JgVpViRrfwS+Okc9JfFZuDnPsOPfM+H2uVPU1AjMLjcBAZD3kTbkZ5myLV7xdX1/fYR3+PJ9HzEP67CYrz/KAhrONaLoDMiSEO3OK7htX4DGQ9cUXX+j58+epzZ67J92+zzaa116vjzOKgnXmARnWlAdsMC03m42+/fZb/fjjj6l+B/W49hh73gSP7xFXAqzTA3BRRliPjAd1N5vNdErOY8qT89geY6JUmUk5sIsA5QKTc07nmMt9UVoHGOnuOy7ZRgLtJ+Lm51zh5EQYWGBMEoCIMCDMfBdZgRcHsJh24P2PvgVnf16X/+Y6nxMf6zgH8Tuv130eOTMkN+dxEdwHjjjX+/1+8i8xJixclE+MNPJcgIm5i/1zwJLu+s8oBCe4xhk54+OAxfhw8KpHLImyw3IAGN6x4W6ORuPm2Kx3795pMBjo8vIy1eNA6+zMgToe6cN3gIKkO2AK04WhSUry/9133+nTp0939lTTHmfbgD3Mks9brZb29vbSWiLgsFgsShvsmWtnd7BqSclv929miua0e/w+B2j87YuxiikwIT74Xl8uS9m/d8YXkxlda6MdptNpSbvh88GRut3enuaBIECrPQGy1+uVknBJ9YBRsKcxgnQu4haZW7wm7mCI90XG5XX5WMexywGbX0tfolKKz/fPI6BVMUrmG+ZBsMbTgtx0Yn5hNH58NcrO+xiZTgRc5ta3IdF3Z/nuOvEF6cdoAXA+T7Tf8yGJDu52Ox0fH+tnP/uZBoNBSSk6OCGz/r3PS5xTnun/029AjhQP5qDVauni4kK//e1vS2cLOqACcn6+IGsMC4atVO12O+3Dpo6rq6v0FquoqKJlxtpiT/djypMZWwSVKvMlMie+98+ZXKiqAwC+Aq6P+WxxwXsonIlzqh2Tdb1NUF/fJuXABnX2heKJxH4v9cL+XDDdzPZ2R3PMx8idwnHMfcF4nVGR+Lx43+P8OcOtUmKMa87kjff5PFUBpnTrzyqKm0icvykM84kFEc0oZ9Lu96GvrpRyrJffLj/U69/BaqIsR3MUcJSUTGMi7ZJKe4rdhBsMBnr37p329vZK9fheZ/eh5RSEywl9od9xDll/zoaXy2WK5nKitAP/ZrNJ/kEfH74joMLzfA10Oh3t7e0lIKvVbk7ZieuSttMv7u31elkCUFV+so8tCn40QX3SPDrqQIU2cycwHfIF4SDogh1ZjV/PoLtWdhDJDZALsSfrbrfbFAFDg8EoMI3cZKVN7hOq1Wqlg/a4hvHi+Tn2E5mrA3kVOEoqjTufR7CK455jU16qQMoXAP3wZ+XGO7JnqexX8Tl2hcfC8mCTR9I9WIBScUYQgwkO6L7AnG1RAAzmm32jca4wozwy6Wza29xoNDQajdJpzzwbWeJFRm6KYknEur3fLie0mzpJl+GkjvF4rOvrax0fH+v6+lpnZ2d3fMO4IZwN+1r3I/7pJ+zSQc9ZOMcw+Vg7GXK25qbqQ+XJwObHpnjHvDHuu9hut+llKuT4RMcn+82c4vrCcrCi5EAgTnIOLNG6DpK5evGBEOmivQjk9fV1Sr7FsU1uEr4eANJNGz9dNNd275ODOGPp30XwieCSMzVd+dxnenrbcsDn3/tvih+pQ7t83AEnPmehuslJUrcHDFxG6IcfvOCOdNoVTU9ndpFtOcvxl/ZQnysqigcpONoIto1vywkB5povem+bM0n2MgPgzCvtclD2OXEl7wES5o/z5VC4nE8oKfnAXEkzzm5+SyqdiO2b4qMpz9zGCG6n09HFxUUpl9XdDJ7D6Hl/D5Unn8cGwnqjHTRogDsGyWnh3aARvBhcAMDNL8/H4Voml0GMiysuwMjsPLExsj9JaZHxZnhfGLR1sVikI2toM74RF97VapWEEydyzj8WwcOF1YHSFUaVYFNf1ORxrKLg+v/+bL8/lqrP0Myw3hyDi0rFlQ3z5P4W6Za5+f5Fn0c/ap62M98sXO+fszaAxesDgHh2NIlhfyxYjhei0A/6Czg5CZCUFrAfOe/BAXYSME7OmlyGqiwRnoVpzpqczWa6vr7WeDxOY8rRQ16fy51bT5jtzggJhvR6vaS0YHSMH7miktJa8hOLCeD4+009LeWh8uT3iuLA9El2p2zVZw50zkDc54B973XnOhLte0rVIvPFjNC4dvRUks1mk7a8OHD5omPx+EmqbL+iDkxV6WaC1+u1Li8vS5FT748HNnImWgSf2GcHCmfPfF7F6qJpGuc7t2iqQJN7JJWCKn59FdBKKsmOL/SYwO1+Ua/L2ZaPawRz2urPY958TiQllhDHjboi8/brMNvchPQ+YA2MRiPt7+8ncGy1WmmrlrtyIA/Oav353h+XG/d5YSFRN+/wnc1maVuhJzi7iU4BgJxBsY54hu9LBQwBfuaKRGTWzmq1SqYsfriYk/fY8uSdB69fv74Teo6g5ezNI1g54URAqMe3rDgl5x7P0YEdRYr60GL1CXMQdb8NC4BjaeIEMmn8jj4ctCOgd319nXLjYH1u2vqR0M4qo68E8zb2776+0qYcuEVTU7rrvL2v/lxxP5GzJK/TF6vPSavV0mg00sHBQemAR68jsj/3rznTcX8cPiVcBZ5r6AzRZTOyaPf7uJLxTewOCj7ODkCdTkfD4VCTySSZY5KSH81TTYqi0GKxSGsNoKYvPt4OotHswyXC82q1WgK1xWKhXq+nwWCQZNHHxSPCsGQCIvgaUUK0w01ZZNm3UbFW8Ms5Rvjhsn7EWM4yqSpPBrbnz5+XNIODkD84B2YUhMIbSWg/Rq98ITNoUcDcqZijz/Fv2hd9WCwiN399jyKaBoerpMTuPIFUKmtrgHE+n6vf76vX6yU/hvvw4n5TBxX3jeX6FK+J4BR9apgOcdH6+Pgzqv6PLCV+H+91pYF8ePTPF4ezEm+397kobt/wxAJkQfuCz807vieXU2djbvpxL8qX3Db8VOzIabVaaaHSLu8rYE/yqnTDgC4vL0uvqazVamknjLNY1oO7gGijr+IM+uMAACAASURBVAsnHg6CyBpuJdwjvHZyMpkkUIpj7mYnY4MM9/v9tAMjyirRVl/37q4aDofq9XrpmURTWVMehHtseRKw1Wo1jUajOxQ1h6I5s4Pffh9Cvd1uS1s1clSb+50R+G/XKvGZvnB5bhQ6X/jONv1e2gWg4gNAoP3se3faFkWho6Mjrddr9fv95NfwJE3X7jmzkwXn//uY5ha/t9u/j332elyx5EAvAmXu2jjPPh/+fAQc88ODLAAATnhXQrFNLPyoMBkLlKazacAzmsrOmN3XhK/LWQQmlHRrouXGEaCF7ex2u2QSwjj9Ouk2AEPUnedFMzPOlzO2nDunXq8nhXp0dJSissgjz/Doq9ez291uI6TPnF3I9668vL5c6hTv+v348WMKrjWbzdKJQE4aHlOeDGx+Cqhr7ftKzh8TTUJ8Kh75wh6X8n4oBMXNnTiR8bk5YM1pafrL57A17vNdCoAbi5BneWi+Xq8nZzpvv2q1WikQMR6PJd2+fSu21cHKzdKiuDXJI6DlzDgfJ8Y1N5855hXvhYF4W3OMzu9jHLmO8fc+4JvDjMdUcxmJc+S+Mgdw6vVoOO3xz1ypODOjbbAenods8myAxzPyPbcxArybse5v2263KdIOQwIUPKjg/XW5RxnnlBvjtdvdRI9//vOfa7PZ6OLiQrvdLjE4SSmv1HPwfD4Bt91ul44lc7cHP4CRp6vQT/zXnU5Hz549k6RkGmPaEkjI5SHeV54cPMhVnvusamBzJiOT5H42n0jpNlKZY1gegq4CUZ/s+NzI1Ny88gXJ4LovQ7qNiLn/ACGjH97W1WqVGN5sNtN0Ok3azF9RFkElBhN8TAEMH3fPhYrmqY8Pn+UWn38WmR6LyTeG59oX+8Bc8NvNd1c2HgVjgXnE3K/DhPHnSLcJse5jRE5ikIVnu8/VjyRnLhkfZJXnuOnpjAnZ9TQn+o6CA7RjwRrg3SHIms+9t52xog0oAlfYq9VKe3t7Ojo60tnZWTJB/SVGPq8e1fW2I+vD4TCdAuLtc6vFTylx2WOdDQaDNKdXV1epP/77IQJVGrdHX1lRHkLRaDJI+eRPj+ZEH4H72KIZ7AuXhZED1aiZaQeA5MnC0q2z3f13vpEYk8LHwLWUB1S8uC8JoWZhEmaH5Xk7o3nlPz6mzsSiKV7FsJ1B+bjm2Jrf42CRu8b/9/bwOXX4s1gIjLW7JnzuPaId++E/gAHy4WMUxwxWRp3O9mivg6LPN/VfX1+nPa/0zdlcNKcajZsjwP1QR++rdMOeotJzkxXm7D7BONcoTn9nx2QySS83xpS/urpKSegO3k4afHzr9Xpia76flud6kGi5XKa58MAOARNXULhmfLxyWFJVfmdgq2JmVSUKN585/XdnsGtmd/KzKDxC6KZQDsi4l8/cpPH2oK1dW+Bfobi2dvDygATtzrFBgByfDbQ/+qni+DoI+8Q7w/K+VLFc72sUGF8w3hZXGtz30Pw/VHecv6Io0tHsfkouf7sC8zF3ZsHicCDzlI5ojka2yXdujvr4Ee2m78gpz/RgQmTQ9Xo9BY+Q4Y8fPyYfM+8GcX+gz5cnImOmEpzybX700U176XYrIPucz8/P9eHDBx0dHWk0Gunq6iol67r8OuPzdct8AL7uY3QfnXS7G4Mf5sTZHxFj970+Vta8/M7Adl/xRcr/VYWB8Mioa0k3Sx3tI3C5f8UFwpkXz3NgcAbh3/ODkEd2F6m6Axltj8zK++WgTT3+Eoy4+L2fsT/U6VoyF9nL+dVy8+EmR3x2joHdx95yLN3rk8qMC3YRGZenIjhriaYrC9376ww9mtquNB38nXn7XLgvTVKpbe6ycEXqW7A2m03apeCABHtBLjlJ2oEFUEUe3eVRr9cT88c/6ESAPsOeDg4ONJlMJCn5i0mWjZFf74ubppiwjKkr90ajkdI3drtdSl1hvAmQAHRxXyhj+e8KbA9RQ9f2VffRYAbcHYUuvEw497hAS/kFymKrMsGiUOdMJf5GcFxQImPze/z/KtPbAxfeVxbJZDIp+cm8T1XtZHF7tjzPdpCPzNYFJwdYVQBIeYxJmpMDn4MIhDBa6RY4PG3BFQ5KwtkXWt9BJxdJ94UY5cUVoCtSz3OjTKfTxLyYN5/nzWajyWSS+tTpdFIia7PZ1Gg0Km0v9Hl3BVmv3+zx9JNB3HxE+aLkAHdkl8wGdsW8fv1akkoyyPxwf26M+Ww+n+v09DQBkwMtY8Xz5/N5OnLfx9/TPzhYkzYxB08pv7fz2O7TxFWmjYOasyDMDvd/OWBErZnrOBrTGUfOXKVdgFZ8ZmyTA26MRvrev/v8Ty4gztgc6MgP8tcE+oJzYI/g4z4jrq/yLzEO9Dky3WjG+tz5WPtnOabmf0fF5uPK8TycyUY/XLmwzxFl54ck+mKM4ODgFQMjUUH4XLm57gs7pl7wXlrOGqM9tBMG5ZHNVquVFjL5b8iQuz+QCcw4LBuiqO5zLYrizjHfyBC+3Xq9noIqjNN4PNZ4PE6+Y/qJkozmobsEptNp6rPvQ63VbvI+r6+vU5DM16XPSww8kMfn4PdYgPvJ57Hlvqv67CF24IvJExTdDPV73GSMz6Lj/r2bre6f4j4GjlMU3CxwOhzNPQcM6vLJJqpEoQ4HMDQfE8cCmM/nur6+Lm05AwR8kfE/7fJnsYDcwc138ToHzgiWkW25kvCFHZmeX+9g4N/zOX0gvcMZglROwyD44mZbHGfvE3XTllxKRAz0OIOsCtCwLYnvAKHZbJay+9kahTzTBmSDPC0/w8+Td8nh8iOE/LQM5IPiDIviyh3mRDtWq5UuLy/TD/41lwG/1+fJzUvG27dUca2z6iqrCcDkmZ6753Py2PI7MbYcI6nS6rkB4gct5AfoAQC+GJwZ5RhHZGSxnT5QMTpFiXa8s6sILg4w3IvJg9AwLl6Xa3un/97uV69epSRVTlKNvqJoAufYB+Dm4xj7Gxe+F/881pFTEF5vNOly8+Pz5s5yvmORsdvDo6RRg/uY+FvPpPJOEO9bNIdpb06J+xgDjh4w4H7fueJywCJ3s9aj42dnZ7q8vNT+/n4peOXjz7NdofHjih95jeuCF9MAprwD9OrqSpPJpNRuxsLngt+YvvwPEMOyyNlkbNvtdukUXZcXgI3+uky5bD6l/GRgc2BiMP2z+D0dIhoDM+KVd36Mi6dROBNxhuMO9ZwQ5hZdlYnFD+ajp2P44qEPnpSaax/fRxMBKo9w+1YRZ0HU54vm/Pz8ziKKqTCMGaBJ+xBWX8gOVA7MHnzw8YrAmlMeVeAYF4mPv5uZ7mOElfT7fXU6nfSeV9gPrGe73d45FcZljnbFqJ4zWQc2Vwi0k3HxqCv3eFI26SLIKG2kHbAQBx1JpbfGcxJKbqx8bnGFSLcuGW+3K2JkhPWG22G9Xms2m6X15kzWAdEJBa+SdNdJ3LSOHCJDjF273S5t16K9rrBc3hxrfJ0+pvykgybjwnDfDAPmYMWWIbLs+c29LoCtVivZ8C5w3kmKa5cIcNwXUwO8uJam+DHJvs2HCQKUfQGwuCTdAT7efuW7D0jIdA0eF/lwOExj0u129a//+q86OTkpvWzXWa0vSh+b2Ecfx5yfCSFyVhxdBS6ULoAUH5vcvPlzaHcEF8bI909GE55xRwE5KLvp7e3w5GdnB57WQx9QdtHM9+s8co1TX7oBCSKavsBh9YAZQSLMbwd8X1MeSPExc9+pB7hIs4hkgrbRr2azmQ5noD3xdGjG0MeSdUK7OFaJufPIMO3jue6n5Lnu/4x7pqOF8pjy5DfBX11dJfDyd276gERgY7EDZJENOCLztztNcwDHd9zvg+/18b9ryFgv1zDRFAfZ3AL1t21L5ciVa0Xe3EMUik2/3EveTlw8e3t7ajQa6agj33YT2ZMzZ+lujl7ulA03F2NxcMqNvY+htyPW54rG/+Y6B+TIDl0WWLT+Kj4CDe5n4zmMrS8e5tjHir6xEKW7J+fGRUpbI7OGfVCH+7toL+YYqUKA02azKTn8oznm7BBGKKm0xzS2i7XnhASFyvsG8JdxL4mxRVGUDtqs1+tJ0cCeATSUM+sckI7zG10pjL2vxd3u9tBRX8dV1kBVeRKwzedz/e3f/m3qgEdpPC3BNSq/Y/FrnLnBlPb29tLGZzetcosrmjpRANF6PlgUByzXllBnFzaPUKKZPJDgz6NvZFGPx2PNZrPkayBKRJ28INcXmy/i6XSaTmKYTqeazWalMfMF6gzCF7EvdFhAHM+caUt/fJ69v/F3NCF8LiJg+HlnUSOz6GJGPvexDSlqfNI7XOG43ykefuCBihgoiHLifcjJNdf4i2V8z2tRFGl7FGacbxdDXiJb4rlYE7yJKyp1xgO5BZBII9lub6KNpKb4HNMGj6JyT7PZ1N7eXolF+7tdWRO0wV0gkdk7JkSm7mzNv4uy81B5ErAtl0t9+PAhPdQZUgQwp65O192Phkm6XC5TUqEk7e3t6T/8h/+QWEzsVHSU50wO/y5nLnkSq7cdSo+fjQlyJ7uDjwMhBRBEAThrxemKae7anhOGEUoEuFar6fnz59psNjo5OdHHjx9Vq9XSIpCUTkahL3EbUTx1xLWm+4KihqUOZ3jOthhL/o+gVuUfcR8NhXGEIblfK2euxoRcbw/sO7YRMI9AGpmBy7DPqz+LsXQ2535KPnOmGWU3jgl+LDfZpFv25b5E5h4TkjGDPXmUHWU6m82S7Pm4e5sAR9jkbndz+u1gMNBgMEjzxlmFmLo8O0Y/wQD6QXIwVpm/v4G6Y3APU/6x5ScHD5yNoQVZHG6WEnXxIIEfDIj29KjVbDbTq1ev9OzZszuTn/u/yqntLAwB9Pwynh2FPCYjetDAzdXI8FzgofH00f2BAAXCxTiu12uNRqPUBmcUMDrOtWo0bl7AQb1+rIszE2+npASGLDJnOZEx0Xd3tLtpmvN5OEuSyi9OcWCIyiAmdbolgOnmpo+f8oEioi8wBvxNjAMMKsqJz3WOwbvM8wxXdKR9eJ3uC6TN7v/zaGfsL0dZ+RlmyCApLu7PkpTMv3a7ncYcsxPgYaM745EDblcgLv8wv729vRLbpLB+o1y4r4z/eQ2A+6ORixxjBhcea4ZKP/GdB4ATEwDj8rcE8dvNl5xp6gsMAWVyoknJ72hKRvbmAOPP8efTLjdTcuaH98GBzK91M8YBLJ7sGk2Y7fb2fPgc83BnMmaMP5N6yMh3PxEa3IUsLlrud1DDNHRzjd9EKn08neV531m4MWLM85lvF2p8moA5PySx9nq9kr8K8HKW58zbZYeFCJuJKRg5kzpaJT53Ua4ii/X/XXkgPz42zq53u9uAE2DID/d43awZxgSZwUTkXt9A721HzijRN+sm6sHBQQKkRqOR/MQokwhQ4ABzg5WGGRvxgDXjLidf31Wmf678JB8bqM+guaDQsWiy5I7XjqBSFLennwIKsCP3A0Tqn/O7+XX+Pxrcw9JVJk4MdLA4o7+lChQBf8xPBCmaO55P5Fn1XMffvITDzXoitRHEok+Ne/D11Gq15GtxE4lnuemTSzSW7jIgZIKxib5PZ3FuisLWME/wSfX7fbXb7fQW8Fy0nHvdfGZRxSBE9OXSJp8/fvs8u8np10aG5sot5yPz9BxnR+5CkJTmxRNw1+vbE6bdb+rmmQdIGo1GUqrxVJAog6xNwNHlR1ICIk4sceXiCj+uOfpO3+gHQOcg6vPj8hPdA48tTwK21Wqljx8/Zr9jQHxRxhA9DY6MK+eYBd09T4hO8jsCT2Rv0t2jxaM/LYKamxBOi/1avvN++TOl2wMDOZE0Mr24sLgH/weL2McF4ev3+zo8PEwm2nQ61XQ6LTFK6vOxAlAAbf+Oz1y4chrdx9HNb6mc58cCdzMP4aUfHhzhOQCZ7y7wiDH985/I1nKKzl0k7n6I8hfvYxzjNX5fvId6nckzXoyRyxb9dxMRS4gAlBOBKM+Mgbdps9mU0qyk25N4HdycNaNUXCZ9nHe725ebu5LPKVTpFjABQE++dj8qz/NxiWMcraSHypMPmiS64gPjPoPYSQecaIb6dfF/tnoURaHhcFgSbF+E0aEb/Wou7M5qoiPXC+10MEKAnL24YDso0zbMcdrm2tXNLRaxdOuDQDs78NLuZvPm7T6S0mkMHz9+TFnl3OfRKQccNLmH6d2H6MJa5bT1LTOAkz8rKhxnSe7D9HnwiJtf43Lhvjh3NFOHM3E3AyODjOYNCzfOaZxfD8D4nEa5dgBGJlDSsPPVapXegVEU5VQLDyoxdsyrR+cZTz/kkkI74zmCrpj9OgezOH4RTDGZnWH5d8gGMui7Frie+fTDPKMLwZ8Zc+vuK08CtkajocPDw9LDqih8FJCcb00qv0rPtdhqtUrvMCWJ0BmT1x3NxNgG9+m49okC7u10FkeJETDq9t8IAuDhzMDby7W5KBLg5iZWBDh8GrQR3wXCD/uhLe4HcyaBeQCjkG61NO1309F9Pf43wOYMLJr9Dn6YN9G8xD9ErpT/zh3E6MBGW3KmEe3xuYqLNbJPSs5PmJNhxtS3cwHAzFW/39dqtdJ0Ok2uAUkl3xUy74512pf7zJXgdrtNmQbMqQMHQIl56mwQdwJzBcOMSoD1EU192kTfUdYAmhMRl3snQMibr0fqm06nd/yEVeXJUVFnOTn0dM3G/9E35h3hswgam83NES8IA5uBXbgiyEQNy4AwadB1Z1IONC60Plletw94ru+03aNP/gwv8XnUv16vk+8LgPIFzPh51Gy1WqXruK/RaKT6EGQWAr4bTkx1fxf1YfbA6iiMHb46P+0WP1lRFMkJjq+ODHf+JwDgpghWAewNUIsM1hlvdCk4i3L/FQzCI/DuI/J5d4CLysj/jso9Wh/u1+Ot9hz1zZizvxJQjyYigBDXG0wNOXB26+kt+G2dDZIsTNDPlQFrhDEDnCWVWFNuzHzd5+53putY4TslHDCdZFSto1z5nc5j8wWeAwI3R7im6n4vHgkiGXV/f/+OADsYRk3qLM3v2e1u30MazdP7itPrHGA5C/DF4ozLf/vC8Un39BfaxqJA82LmEG6nbWh5EosRjO12W3qhMw5lnk09tJeFD9Awhq6A8LX4PsHon8FHSh3O6qJZ7k5prvW9tYxPBG6PBDvouCwiH9EP5ykKOYWVU85uruWUNu2Iph7jyu4RTyxer9caj8eq1Wp3XlPnlg7BIm+nJ4lTUFrL5TLttfXXSDrIuRsiynQVkLiS83WBUkTpeGZDnC9nie4q8fdTOHPDisnhRa482ccWO/oQE/H/faG7NpJUEk7XUPHdm9wbhSk+z0HVgQtWEf0MuX46aEXzM5qqPkEuKFVjFZWCU34P70vlQw3dlIyghGA5W3MtjOnBW5B8cZAXhVkLi4gmDKxiMBio2+2q2+0m5gVTo0+4D9wUh1USKaZ97B0m3QeWEcHNxz0XmIp+KF94LBD64pF8byP9jazBZcMVmQOgm/w8n0XvvjHmw312yCV7SUmJcLlweWVsoqwyx8gNUWbax1xLSi4PV4LROoiFa6NMutvF24SJ3W63k8skrgsHcd+rynP47N+MsflijCzNGxopOtf7/15yzlfvqC+QHKg5mMXPHKgQsiiwORD0OmP7IzNwlunpLz4+PCNGQllktMkjXG6CwLw8WuiA4kzBmSLjRtvcDPSkTtf8Dn4ANj4kABW2wP8wAx9DwNADJfj2WAzOZDwjPT5HuvtSFwcSBzkHtugH8gVTxQAcyHluVHhRebkp5cXNOMaLnQCYkEQsiWISMHPzlAAEcxflkHqZS+p2WYkg3+12tdvtEoP3kpNdn1eAMHe9g6fLWa/XSyzSgz/4Bklox6ogMry/v6/9/f1shD5XfrIpmgOnaJbdx1Ck8gF4PklR4/Hb7W0HJb73iYnto37MHOlWMCMY8rsK2KjPndbuS3FHaQ74o7MUAUFLRc3XaDSSz265XKazybbbbWnRI6wO0jEyR0ifLP5Op5NAxYMWLDT8c55a4+kg+HBGo1FqF21jTIqiSOfY+zg7i8Rf5/Prp726MvB+OYi7TDhbdQD138xTfG6cZ9ob3QgejHDlFOXPmRYmt6QURGDXANup1uvbgxM6nY729/dTexiXnLXibhDmAQDx9eUssShuo9HuP4tWhD/DTy3x8XHF4tejjDGHB4NBSTlLSoDuu5M4aPXly5f6kz/5E/X7ff33//7f76ynXPm9+diqTNT7wE7SHYDiM65F0Gazmfr9ftrzGJlbzvyNZogvdu6PwBnb7ELAd94390n5s3N9lcoHXFaxQ+l2SxUgsdvtNB6Pk7DDkAaDQTIzfAF7cSBAmwNqfu7+arVSt9st7ZjwI6Y8KdtNnF6vp+FweOdt7YyVMzVPN6AO/EqMA4DpwOTfYYLlgC0qIOpCKQBkDtYoEW+fm0fOsnkebAhF4qYrpShutyP5j8sLjnxXhsz1YrFQt9tNx24TQfW+uyuHgADzy2sAeeO8dJvfdn5+rpOTE+12u5QALd2c/nt9fZ3WZbQonFV5FB5FQSEY4EeYF8XticG0BTOT9Bc3Q4fDoZ49e6bXr18nH3sVw47lyeexxf+jhvKFH6/Lsbz7zFM+n81m6S04DlD+vzvgvV1VJqSzJulu9Os+E9QZ0mNLznSgH7l6Irvd7XbphA98W5xSure3V4qIOhhH/6L3DyYRFxqRMnZ+kKIQXykH8/DIJc9xMI3msBdndSxuTOsY2kfD53xL9827AynXOrvyrHwHNs+tii4E5pC0mKgAYx9dwXLsO/3GRMWt0Gq1dH5+rh9++KHEml69epXSZDx1xnMluV9SSfEAUg6EzNdoNEoveJlOpyqKQhcXFyl4BGvH70ph3FBanr+I7GFR8L4DGCSmJ+PN8UsA7cuXL/Xy5Uv1ej2Nx2NdXl4mUvOY8pMYWwSh3OKPC9PZV6wrCqcvftga2pbP/V5fOBGMfCJjO739DjKx7bk+x35H5ugRt+ifybE+f058tnSzzebq6qp0mAAnK+x2O+3v75cCCWhQWIMvzjhX7kvzFIx4Ikg05biHAII7891U5P+qYA11+RllPqa01c/+clM+zqPLUvQBxXmiHnyHAL3LjJuR0l3W7eCdy7OKjM73VHuE0tnNycmJzs7OEiBwZBAJvpjuThp4Fu3x/EOXU0zD4+NjSbd+Tb4jovnjjz+mIAQn5/b7/Tsyy/O9j56DCOh5IjrMH7BkTLvdrr755hu9efMmKVk+fyyoST8R2BwI3LFN56qYTA4coqA7qDhIwhgwuyIwRFORiWWw3c9UVaLvq4pBVgFsHJ94n4NnjgXE+/x/P8wT31ej0Si9zero6Cgl5ZIqs1wuk3/LTUQWLJ/54iuKoiSA7hCPJhvtd18e9Xof3YRzwHEzMsqEa/4IlJFZcz+/AVJniTAyzjIjVy+n1GgDoODyyLO9v5i70ffloBhPWwGscSt4vpc7+/v9vr7++mu9efMmPYf5j/Liz2RLFX49xsF9l55ATMAFZlWr3RxJz3tHeZGzW2eRUPAZz0B2ATafL0DN96f2ej3tdjt9+PBB0+lUp6endwJijylPBjZf+Ai4D6YPcCxVQJHTvF5wahMl8dC/lPet+EK9rw05UPLfEURzrM0nOBclikwtglqs3+tx1kqyKwoF7Y7gFEWho6OjFB2bTCZarVbq9Xo6PDwsZe/HdgF0u93tW8Zd00a/pzvnI9C7eZ1jqa78fEz43MfFQSkCZpVfi2x7j1T7d5iP/rkDHM+L7Dkqnni956C5/LjJiBksKZ2m7CkzLPS9vT09f/5c+/v7+vLLL/Xll1+q0+kkAMLvBtumn5jrHnklEul9oL0eSHPHvaSSfBXFbWCKugFXxprDGPr9fgJgFKybrUVRpPzU4XCYxpC2/OpXv0r1sfuIMf4323lAiYvYP5ee5n+qqoOy2Ww0m800mUzUbreTk7rqXtrmTINJknRHiJksZyX89oVH3V4X9UXHcy4wEBdFbrz8flgKJ/By0ioA4ItoMpno5ORE9Xpdg8Eg7UckurZcLjUcDjUajUovRcmxFTdjcxo5jrenRcTvcz7E3Di4RuYeZzC0JYKmdJftMT4ERKLLwOcp10539PtYRGB2doJiKIqiBJreTgCDxTkYDDQcDu/sk9xutxqNRvrzP//z5Keq1Wqlt8JfXl5qtVrp8PDwjuLxvaOLxUJnZ2dar9eJDfm2Ke4DhAgY4c9F3mFVV1dXCWB9uxzmqnR7EMZms0lvv6K//X5f2+225GNkfBqNmzMGMYEJWsEesUAeU54MbDl2xOcRbKoEuorRcY2bIFw7nU51cXGhXq+XJjq3yKjDfSxulrpASmU/jvtfqtglgMPzolnEdTkmGM30qr6iUdlJwDHO7sdy04wym810enqaTnNA2Dj9YzweazKZaG9vL+USuWnv/fD6AS8fPwc7D+QgCw4kOUYcgTyOizO3CGjc58rV58vn3vOp+NzTPOhrVHY806O4MX/M2RpAAsi5onCT2E1lP1rb+8B1vmEekAIckQ0i4x48q9Vq6aUyKMX5fK7BYJAipNvtVrPZrLSv1w9F9fcd8MMOCXILaTNMjhcNwRzn87kuLy9LZw4ib/jr/AU12+1WZ2dn6cxHxpDthVXBmVz5vb0JPnddNBFy2tzZh1/r/iDpZoImk4mm02kCN/8+alTqpy1VA5IzHXNmKPWxkD1a5ZNbBd5xPNycc1Bz0CqKImlphMnbFH2Cy+VS4/E4JdP6uXlo4slkorOzM/X7fR0dHeno6CgdE+RjmUuEzDEl9196v3MMLjJhB/V4rQNILrveWaJH4zDJYND+P6aTz0Ws35/rc+LXulwhD67w3N/sCs3nje1otJP58QNc3VSmbQATn11dXZWc8tJtNLTb7SZQAySXy6Vev36tZrOp6+vrlMPopqhbOryseTwep90T0+m09AIdB0fkFKY4mUzSfC+XS11cezLj3gAAIABJREFUXCT5HgwGJdl3ICQ4Jqm0l/U+H7mXnxw8eEypcgg/VF8EBwSWMPnh4eEdwUPAcv6bCCrRtPLn5a7lGd4H174Ip9flizSCJ4vZF7WboESpon8oKoacAx76LpX39AFC4/FYFxcX6bijxWKho6MjDYfDbDJs1Zz5mPhY+u+qcfc25cAkAoazNQcs9/FWMTdPmsbJT3rFbne7bxhGFucN4MYk44fF7xFH6e62p9g+n6uTk5NSfl10lZBrGP19ZOXjnplOpzo6Okqb7N0fyYEC9Xpd5+fnif15EAI5wf/HNc1mM6WWAHgUzE4AjvEAQD9+/Kirq6vULz+soNVqJd+ib5OaTCbppUfOtp2dPxZ7fpIpKj3sQ3ssZbyvRCHD7q7qHIvDhT46mN28iKazA50vsFx436NMDqaP6fdD7ZduTQPfGlQ15lW+zrjthc+oezab6eLiQkdHR3r58qWOj4/vvC2Ldt0XHHLAisBWJYw5kzSyPWev8bmRtUWzj3ljAcfvMSv5jIVclUpCXX6SCaDpY+MJu9EKoW4AxH1SnnTsbHa32yUGA4tjXtlFcnl5qc1mU9r94W4TouIceooC5HBSLA5JybdWq92kfmw2NwcwTCaTFByICpwjwql/PB4n0xUfGVuiYGH9fr+UrMz9rszdZfCUiKj0OwBbzjflTITPq/wrbh5Gf4t/5lo9/p0DL2i9772MWpwSTY4qRsF39M/D/J6v5oswB0LRWe/XRVDLLdzIMKtKpOu+qH3M/JQPNz94z2lsq49b/D+y4FzJuSJoX47RxPGPY0FfeKbLUzQPuddfWVer1e6cbAEoeF3Ux6kpvsiiUvE28QxnnfQnmr9+QIEf5+1j6pHMePpJrVZL+Z4AJCYpc319fa2rqyvV6/W0Id3HJZ76gtl+eHio3W6ni4uLku/NZYMEcY+6o1z29/e1t7eXXCswOJexGIByOWJ9+TUPlSfvPKBi7xwTSKNyQQRfXA5+0d8RiwuZP9tPafAFy56zKuYUF0fU+L5ocqZi/KFPmCruC8sVv8+ZIJ/lfAg+PlEJVJnxTynz+VwXFxfJzDk+Pk47Cfx4HUqcXwcgV0D+ubcZGfFxjiYo9T6U8JoDP4DEj0FyOfHAAP7I+NJl2gBo+PYhX3iupHO5alI5343xcd+nm6MU3xZFH/zF0Q7ckWH6qSE85/z8XOPxWNPpNAWQvI2LxSL59rbbbZp3N4UbjUaKsCNrnk3Q7XY1GAxKgAzDJYDgfXS5ZvxQIJJK7+B9rFKn/E7BAxdQL5EF+e8cqCGMcbFGwENA/dVkPokkq7JQHIB8IfFM1/bRhPL++d+ufd1Z7AuqijrH9uRAMjKeqvGIjK/KPMyZaH4f48J7XWezmc7Pz7W3t6f9/f1k3sBU3DcYgdXb5HMa2aYfhUTx9gFWLPw4JrFv3M99PB9fJRHE6Jejje5nw2/mCtsBMs6J75+FZXlfXIY42JHrWMj8+Jl7DmA8H4UZxylHJmq1WomlffjwIQFXvV7XxcVFMoH9pGWCF5igML/RaKRms5mACxB3efITSWiPvyfXx9PfasX90Z/Ktfibn+Le+kkJuvzOmV1xMTmgPFRnBMkcI8mZUzFEzb1xsGIbXDh8oeYAzvvj9XqfHbDjs6IpwnUR0LzPuf7nyn3M08Hc/W0RGBm/1Wql8Xis09NT9ft9DYdDHRwcaDQalTZUO5v14kCdA7XIeCIjjuDNXLsDPf5moUWFiSLETxmDP1E5eToHwOJO7xj9dQvCwTLOe61WS+/1JCcOv5ebk/F9D3E8aGf8jP7A/IiG+1ZEEmLZ1zqbzRKwHRwcpNQLT+4mncRNW1c6Dv6wewIT7jeM4+Lr0fvFGEI4OJfu4OBA8/m8lKz7UPlJwJbzu1SVqgXgJot/xj3SrUaNYAdrQ8sgWLmjVO5jmL7o+N61rYMdExkXJEmOmCC0L5cE6v3MAZ2XXBsj4MY2x/sfUijcH0GE8Z1MJjo/P9enT580HA71/PlzvXjxQqPRKO0jdRaX60M0IXJjzv0R1HyMYioFbB3fVC6QQIQPho8vKwIb18ctWx5McMe/K1afd29bBEPSGzApYTEARpVCcJLgY+UpIqRx4B9zfyX1w+Con2P3AR5OAkHB+Trzl3/7MymRlQFqVTLoOXBEVAFknl0Ut8GHRuPmzWz+9vqHypNP0HU/RG4x5Yp3rAoU40JHUCPS73a7dAIBqI62cmqP8LqwRmHJMZ3ogI6fRTPRx4RrnUHEQEGOcvv4VbFf/o8+Cv/MAcYXgM+Dfx/NmOi451pnxvP5XMfHx9rf30/5b7mSA2R/hreR6+JpGu4G8HGJAOyHKzoAEkHHV8V3sIpYf5RBFhUAkZOVHEt3QEHxeXJ1zozPMWyKK27cBX7CLM/0M8+8PU4gfLsVoEwSN+ek4ePyIEb88Tn001b4n/Pd3GoD8P38Odrpc8M1PhZ+CvBjyk82RatAzBenC3FcUG6Hu1MXwSIi44LgLwmBMbDgGCT33/igIgQ5U43/AYHoLM/1NbIm+lT1gowIaLF+ro1tzDEhX0zeBhdkXwxeF0DsgH9fYVzYq3txcaHT01MdHx+X2FsucTWOlY+Ft9sjcFJeqcW2+Lwzz76hGzbQ6/VKLIDCyShSmam5uUn7nKm7s9z9ab6PF7PS38YeWW1krswbdfpOA7dKuM7bG9m2X0MfCGpxphumKW06Pz///9p7095IkuRa2zPJYnEtstjVmwbSFSD9/380UL9SQ4OunftWmfl+KDzBJw7Ng0z2zAXugA4QmcyI8MXc/Ngx8yXa1tbW4CIzzpC1x7QXbxMv4zd2VGxvbw+ut8cajNVtBti8wR7vCzaLbLN/eulPHTRJSkVtrZ5kINmdc+cgNH5LtuP/YWlMGFh5bF3n8/G2GPJKC+s1SQaBqmy3AyWETrM301t5KCMHdlpof6/kWDEG55esEQvvwDoWu+q7KrlOy+Vy2L3w6dOn9vnz5/b27dthOn9/f788ccIK6diRWbTLSFDLmCpMja1msCMAm3vy/K4qb3TDwJx1pe9gGZ7EgvlQDozHg5Rysr88K8oBDz7U064tfWV32C61DSfjyDq7t7fXLi8vW2v3b666vLwcnYl2fX09bLPjfxv7iuHnoZ+ttWFZjJd9kE/qH33uwyuRBwYTWeQC6qn0p44Gf+yaldrfk7mgNL08KwaAYjlACkA5LlKtfclyKoWnHINRxThcj/Pz83ZycjJsU6GdtJkO5RmX44GdQGqLly6z3bkEA7MMB6Uro1HJOetnEGch55cvX9rx8XF79+5dOz4+HoLQNlAZu+rJ33VOgCflJI8HlAe++5p72MuI8TKbT2Nrg2v95BlmNxmwZmnpNTgmxiegSAywYuHuG5L3ojrsgn6lW01itf/Jycmw9Ym4m1nw+fn5iPlSpvXD7qSXsiBnj2U+K8Pmvidf66JjqLPZ9yPFn5r+9HKPSlmzkbaEVczJeVdumJkI97C9ijPi7b7mnsB0AysATRDL+lQJlkZA9cuXL8OpC2ajnuXy8hS3zW0GrCpGk/Wy64HsXW7Vd5XlzfpkGIHBSfwQt5s4DIci7u/vD5utOUufvs/Fr65/9rv7Ax0y4yE/ywn5EgOye2qXNfvbDMj5YyhJAAFrvhzXg6WxGZ7tavwBhA7wu2/dd+4b8rexdh+SckxVBoHTaAH4zc3NYc0n8sV4Jch7DHr21rqXulnhw+7ubtvY2Bi5wQDryclJu7i4eNCfm5ubbXd398GpOlNpbWCzVcgp8Vz/Uwm/BxRTAOLrWBIvKFwsFgOQoej49ga9XDXuREdWCx4rgMgANscX44KarSEX2EEv4OwYGSkDyU4epEn7XUefCMHCywrAUh7k57z4xP1j7RVAd3Z2Nlp6ALixbqkXIzHAGGS9zMDy8kQBMvfKfRtV6pP9i34SC7OsZ7PZwKp4kxNl2UNAb3DJrq+v2++//97ev38/BPkdbDcYenx47Rr1NiutACvHUw/Q+A030/s8q1hvuuaML9rqSR7rLGVZRxKIYZwGtcVi0S4uLkYL690Ob7KvSEmV1gK2jY3vRxSbeSWqrwNeU2BG3m4ovzmwyqCtZoAMGiQHLqvyHHymzUmRGXx+m87JycngXtq6Os+cyUqGmr8ZYBOATNUZkA48A24oGK8+y9fjWU5Zpq8ZlBnIBIkzZods7HqnsiMTs6Y8O621NnJrLHsDnRfIkmcOHvQgQdO65fbOZt/ftbFYLIZTOFprgyG1+4vRvLm5aX/961/bb7/9NnJ7GdDIzUbP7at+43fLN/XW95l55X0sOeHEDWSYs/pOBjdYqV1Ql9vaeOLNLqxlYRbM9fl8PiwIZhzloQMu57G0NrBx5vmfAS9bTO55zO2zO+nV0b6OwnmZB/nbnfIzOcCcHBdL5YcdcAzQxcXF6Bm7Qa21wfpne2mz60pKt9Wgldfcnvl8PizFYNDZOj9G5xkcyfwoz0Bp5V0ul0MwHdCHJXCP2QogSZ8ZTF1vu1nIAPl7rycDJF1swKe1NirHxjMHTC6kTXfVi1VxkT5+/Ng+fPjQVqvVsDTBJ6aYBVlXkF3F5KwbTr6nCqPkZ2v3xxl5xUHK1m31xIT1C51Ob2Iqnmpm6xcz0XZeGuTVDjDFjY16S9ZU+tPHFllwUyCW9z41fwSNS9JaG5Ta9NegA7swG6AeDASzhiyb+6uJDq6zsvvTp0/tw4cPw/ntPVe8ChBbMQxUjmdVAEb+BHFZIMmME9+tXFl3UgKj5QU7Nitq7T5Q3tr4XQwscdnc3GyHh4ft7du3AwjSbjNWM6oEl9VqNayrMnCh9NfX10N8tbLkBgza5jZXIJ/AysmtPG/X0Gz07u6unZ6eto8fP7atra325s2b0SB3f6UbnPX2spfK4Bq8kyBYN/zdgGw2jfFNz8EhGLNIszcAPWOkuPbJMsmH+6mz13xubGwMpzsj1+VyOUzGuZzH0t9luYcbUFXA7k0VF3BKJociER/yRl2Dh6l1a20EWr6eLNG/eXbMnWHa7WUmZ2dn7cOHD+38/HykxG4/bIRYi8HFSzEMNimf+Xw+Ai8zME9OOLaXVhNQt0JVKfsoXVuuXV1dDTERX2NrDfV13SrZe7aQfgOsYT0AomXPMTpYewYo+uLnkLtPfUGuOflyc3Mz2lrFNbvSgAULlj9//txOTk7acrlsu7u7IwOWeuGU8Wr3fXow7hPHCrmWOm5ZkNfGxv3pIHg31cqB9CIAMsvEBs/eht3sZJ0bG/evKqw8FMAStg7Txzubz+ftt99+K2WZ6U8DWyJogph/9/cpgKuuwUCIreVAybrY1XPg3orf2jjomS6Kgc1sA7b24cOH4VSM7Egzvq9fv7YvX76M3KYeC6OdPinEM29e6e8yWhsPlHRFkEO1/CVTup8pVxa8Al6cAYYBIpaXcst8kCXXcKE5NJHYCoDkLT0wRdpkppnA7BM0SAZSb+Y2aHlCivrh0l5dXbXT09PhlFgP1mRM2SfWMe4xM0y9cFuos/Nwm51XjsHNze/vJN3b2xvceE67tdFjvNiQmRlvbNy/Acz6Z/nb0FKX9KS8Hc3ydz1ms/Eyj97kU6a/G2OrgKyiyb7uQefOzIHAgMbdaa0WfqYqduKyPfgdczBjSxcGK80eShYSGiA9adBaGxbtkrhuRmNWyvdc+Jj1yQFRKXTVJzmIMyWw2Z2jjvv7+wNr9MBbLO7f0YAcPauZrB6gnc/nbX9/f9jJ0FobubgsXmUhK5bfLpZjVuQLcLLMwgtnuccTHak/rbURu/n27Vs7PT1tJycnI9ZIX9g4uq1mep4lz3613hkw+HO+CYwJqJUXw8u2ce/oH9pm19IuK3KwR+Owh8ewJ0B8v91Qt9Nts7xSHinTqfR3AzYDDalXCbOpdDM8g9ba9xXTHJnC2hfHN9K9QeApWLtnOStHXVPBLEhAjXcd/u///u9wUmjVzlR0grYwMQaK3UnYQM46pQXzbzkQ0t1Gsb01x+0zuGW/uL+qSQOWujg8QJ9R/3RFk2XbHQSIWCEP22LtmPcueuDC3KiTAW2KLXIf9bQsDH4kDNoff/wxAqg8fy/DGeklVEbfZVsvrH9pHDLckjpso8/3nZ2ddnx8PCzE5Zrj1zYWxFPn8/tN9D61gzoB7jnr70/rs8Gx0m/f23Plp9Kfeq9oNcgs6NbG8RQrGmCW13gGCvrzzz+3g4OD4R2ZlQvnjsZdSGDwzFQVnzDTyvqzgJCtRH/729/ap0+f2mKxeOAacj8u5Wq1agcHB6PtRlUcrLLatNN18p+Viv9buz9W3KvckR3sN9fm0Vb3jQcT5S+Xy+GMLxsWBvj29nbb3d1t+/v7A5inkTBw8vtq9X3RNfL0WjjczozHWXYMKrtUyIMlQR7Ibiv35kQC+RDTOzk5aaenp8NeZeqa4YypPnVK48d3h0AcXsBAUU/ak4uIk+25/K2trXZ0dNTev3/frq+vhxibDZNd79lsNtor2lobvCa2l7U23lngvjW4GeAow0bTk15uh+X2D2Ns7gT+t0A8ELCmU8qT8QMUY3d3t/3yyy/tzZs3w+kDrFPKgKdZjwVh4LCCVDNllUUlnsThi58/fx7ianSqWVK6Cq19V779/f3S1e2lBLXKSPA/gxk5M/ixvLTHLLYK3NJvBjQDHPd4ozcTBSgjyun1bRVLs1JTH08M2J30tjm3zf3McgvCBN5lQDuq+KRdV9rskyoWi0U7OTlpf/zxxwDkXoJA39hIPNUDQF7Io+eOVrFbH/rgNlTsP9282Ww2MjgGwGp9GWOYMYHhIAaZs+8YO/evr1kWCXq9CbDnpLWPBjdImeYn40qq36OTSVtns1l78+bNAGpsR8mjk7nfx6y4o4z2lZuQn5WrBFv4/Plz+/jxY/v69Ws7OzsbOjoDo6bkJGaiXHdft1LnDFNr93ENBrkXo/qEC5fnuB2LKh2zSGVx/QEDGyjkuLW1NTAx8sl9geyX9C4P6m8596y3DaFnk806aWP2J+VcXl6OBhF1yYFkd482ki+HbebkgNm/n6UsUgVK7nuztZw5tV5SX+uPF8nacGdcijambhIjvbi4aK9fvx5e0lIZfI9lG6bb29shLMH9gP7R0dHoefKsvC1PQiQxcR4V451KawHbcvn9DTSuUOVHV5a5WuznZ7Akx8fH7aeffmpbW1vDxnJbbIPr69ev297e3oOgLXXoCSLdaCsJ+S8W398+f3Jy0r5+/Tq80dq7C3i+Ut50MSzD/G4wBMC90JVB7RiZy8Fy8ulBl3Ear+GrBrsNjcGNgwh5p2taY/ej5Wzm11obKW8qOYaQOhO8dkyUujAzlwFtjG0u63DfYpi8jYm6MCmAEbNrywDMd0FYDgl0lc7nIPU4sByRkdm2428GLRuRLM+xu9a+k4Fff/11MIosmOWeKkZmAgM7dn/yG15VZcDSmHm7WS5ZSlb3FE/HaW3GRkfnAOjdn0hvofO5vb3d3r5923788ce2u7vbFotFOz8/H/aP+Sx5XCHiOQyyLBP2k8KxkDIet1qthqUEvD3di0A5PI+OSwWmfXaBUDSDmOOMyNSBeB93bsbr+nq9T1puL240m8vNy8jJoIaye/0X6+Y8oGlLMg3ikcQZsey8ed4pgYB2ErDOGCKMgD7HLXWsdmtra4gd0fbV6v68NPqDmVpkcnFxMZw1d3FxUQIjMmQjd8aB3AeU68FYjRmzRpdXgab1wGBrMLOeJ+t2XkdHR+3s7GxYj0lyv1fJYMm97Aqgz4iVkdBx40G1Y6QCrZ7n9Vj6U5MHFaBVDM7JMZvNzc3hnZYHBwette9LI3w+ey4OxfXMV6G5XAOJ62slTT+eDuKEAVbR20rzPx2ZFjHlYAXx2V2AGO3Lwdna/fYUBlFr92dZpbX3DKCtqvNJUOLZdKMNvLSRVfjEzVw+dSdkwFKP3d3ddnh4OLx2bWdn5wGI9QatZc59LPeZz+ejo3u4Tr47OzsjZgvIm9X4aKHNzc12c3PT/va3v7WPHz+Otuo5TkW7vUXMdU7WZhk7TIP8Mx5m9sx1A7/bm2y3YjI25ukqY6x+/vnnQUafP39uX758GS19sR6ld2b26/LoH8YGbNj7pNPlhel537EZ2zqARnr2lqrKjbCb5MHia27Y0dFR+/nnn9vOzk47PT0dTRBUG6Jhd2x1qTq6WuaRFtK/MQhwOzM+w+fGxsbouOJc6Gq2ZpZBkPXy8rJ9/vx5WPeUkwNe22aLbXlbLlhL5zGbjWNcflEIbckAsfvEsTU+YcYwHN4SfnV11S4uLtrZ2dkQsMeN9Et7kxE/RVHdl3d3dyMQ4pOy7KIiP6/+94Ci7xl4W1tb7du3b+3Dhw/tw4cPwzIT6mhjysSIvQS3ZYrh2KCyNMTMLllhul+V++r8M3ncTMl6d3e3HR8fD2sEvdbSoQ/rtVmoQxuAPzHdNGBuJ31D7JQ1dRxWym9TzPGxtLYrmszAbqappWfUvOKb6yjgavX9Razn5+fDkg4rlNcIEfisFCJjSo4tZV0Xi8VwXhZC9Asq0l2mPkwE+KW7VhzKNDDe3t4O9SYvBlfGS1K5YXe5zMUDtAIxgJ/8sJgM7sooVeDGoGKpC7FO4oDOhxjcq1ev2sHBQTs6OhptiTITNGAblDK2k+6HgR85Mqgc3MddJKVR9cD6/Plze//+/Wg/ouViluLjl3yPmVp6ApWrnfpF3arYksHTsuPTOpEsLkEh3dflctn29vba0dFR+/DhwxAu4ADMXHlggtDz3Kp30aJ7jKEMwyyXy+GdpNQhZ/mTOD2W1gY2GFWuQyO5Mo4rpUuIO4D7wup8wAUBIRyvc6ETTbEzBpV02kH4s7OzdnZ2NgKodAOtTO5gby1xJ9sd8IDAfdrZ2Wm//vpr+/r160Dfnaojh8x4DGK4MJaHYzUeODA72J0HULWOMPvVbjl5+jgZl81nLlgFXG38PPBsuEjIl/soh8WiaTAN3uiP+5577OJ+/PixC2rcTx+yPs/rtawnGSdzOzI/UgLPFKN1iKUXbzLYIaM0FjZ01Ht3d7f9+OOPbTabDTFl+soeRoJtMkwOAEAv0kh7jGV9WIJiFz29PmZvn5LWnhW9vr4eXCIrlgcLgvFzTgj96uqqbW5uDkFzrIRdJ/x0Bjauh8GDge9B5nVRl5eXw8mhnoywC+hky+DV7rQb+p2Mju/plgKGxJk+ffo0uHQ5uHF5aW+CmSctEqByYzsDzTEz9wdA6gkDu7jUDffL8TnHilwe9anWRPkv41KV0ntZB4AJaOZsJf2Us8ZexGq3mrWJ+Yz7wm3b3t5uBwcHA7gleBlU3B7XsWJrtJk6WId5xslyQy4JhsjMZWSdfHTR9vZ2+/nnn9tyuWyfP38exqWNq4279Zt8Nza+n/V2dHQ0Whtoz8PA6FBQa60dHBw8iAEjG/o835cwldZmbNfX1202m43WG7myOVARXtLk1u73AvpoHjeee3GvMvCNZXIwmHPbYYDE7WCano3xynE+04V2HM5xmgSy1sYsIy01q9UZIFhM3Cuf2uGlGx4wBh67i+STltwMMBVxuVwOrjjAli4vM5C5U6FiJFmXTFMMoxdHsbttHQHYPHCYcMIQZn09gbJYLNrXr18HQ2om2asHS12qHRvZTrNRr01MF7O1+35FR1+/ft12dnZGMcRqka7HlNliytssPsNFlI+cj46OhnizjQxGhQm2KjywuXl/fLcn3lq73/mRDA99ZhG7Z/gz1OG2PCWtDWw5C+JrBiP75rZozGqyOt3KSvwKZQOlCUY71kV5nv4/Pz9vHz9+HLa95BloWecMLEPR08XmXY2kdOFgBF5GkRa7tfstKI5ltNZGbp1ly+wpZRj4s4MBJ9oNo0bhWO+Hcvtez8xSPnWpAsGV1XQ4wOwaMO3FEh27mc/nI8OCIvMeATNSG0XPtvVYEqyeiZyLi4vRwKnkasZweHg49G/Gt3jeTIpUgZqftftZhTTseuaMajKwqj9cP0DKS3y4tr29PZqd9FHnLN0x27Q+Uje7k9kPHhM5O8qpI24DelKN26ekP/2WqgQ0FJEGAmRmVVZuBMMUfmttECRHrJhZoaB0OMHI09PT9scffwyLKg2sycgsaAKTnpX0Gpxkm7BWziLzAHRnmv6TuB8w3tnZGcXA3Nk5QJLGo4TUhckPz5Sy5CJdeIOHFY38zYpae3g8Or9xvwck1p3lOHm2WU+XfN1HU7nthC9gFYQIYKToTrJIXFJ0EuBPL6MCt/l83g4ODkYb+w3KGXbwc/lbupCUiUeyv78/Ytb85f5mT5BVMc3UmyQayNJypc+oB/Jl2VWGW/i053FwcDBMXFlPbXSs74xFDL3b8FxAIz17HVtrbbRA1LNzaa0zrkDKADyf8/m8HR0dtYODg7a1tTViVd58jOBOTk7a+/fv29nZ2ajDst7+bgWiLtTZDImyGUSt3U9TOx+DLzIy63OypeOIHi8e9X2AEMDls8jMYgB83IHNzc1BYQxqVi4UKydDSJ6g8MBKIPIg9F5L2pAAmkty3D+47d++fRsMGPVkUsprAD1IkgWZ1XutWBUQJ2UfvHr1qh0eHg79mzLoycR9WMkp3fVqbJgFW195tooPux2Zn79bpsiFscs7SFnGwzZCSEqy++Vy2Y6Ojtovv/zywGtJGWCE7PFUwIZMkgn/Q1zRjY2NdnR0NChKvtwhqW9V0fwN5aKhuKl7e3vDeiW2wBAjocG8FenDhw/DCaam9hlkz10KGXsgJsXAyYkQ8qGOvcWqqVSAsv8HkLyqH7lhCb9+/dq+fv06xAptObHytMsTC9Qz+6WKYyb4etGoj1eqZlyzj20oHN8zE012b32p5OP8LNt8nnqaVcBqDCjEYIknOraW/YQhZT9kBWoVsFi+gIZdUodmnKoy3H9uTxXnzZBBa/cM1mCSMjVxYK3e9vb24FG09v3IJuqNB4bevnnzpv3Hf/z9j/OKAAAgAElEQVRHOz4+fsAOM/7s3/CU3rx58yDcYzkii3UY3FrA9urVq/aXv/zlgRvpwiuFrRiMG4/fT3B9Z2dnOFsLcIEme90S7xxgYaGVJtmiYznuUNiBZ8e4hxkjBnnFQlI57E6bkpsV5YDyZAFtgNns7e0NM365uLe1+2UiyaDSnUlZcN2zoDzrGdiKNWSqLGqCZ+Va5L0Jipl3VSb6g9HyjKzzo92eIa/qkyGEZBOWYYK88/HYSJll37g9+T/tc9kwXpfruKfbbZ1LLwTZ46pjXG9vb4cJLhgvoZfZbNb29vba27dvh7/d3d3Ri31ct/TIksgQA04DXKWeLlRpLWADrf2/OzFjClXMKC2YBwDsBTqLkjKjRefhmhFP8zHUqUyUD4h5D2WeiuGTMKxQpGr7Uy6VoK2AhjvYzId2s2yBOAksobXvgLe/vz9SRJ9Ltlwuh8Wzdg9QVMc609jkejVbfA8uLySuBrMZcrIlvnswUbZBMgHGkyY54NOKm9kwc4fsuR9m0Fob9kfi8qYuG9xgMG5zgn3FlCoGlXG/yshY9ult5KfLS1l73R4hDrfPQXqHRWaz2aBvuPyr1WpwT6nP4eFhOzg4GCb0cpa+MlBVrJvY3sHBwUjOPUNYeYG9tBawMQBbGwc/jcJ5f0WZSWYvaXmSHmORARdiAJ4poiNQWgvQ4ABAOx6V53B56UO11ov2JAs0g9vc3GwHBwcPFB+FMOujXgA7ssbl9qkibNPyTOz5+floUPhocWSd9cv1Zh6Mlr0BLwcVckiFQ17UM/vcepCyANhub28fxAfTvUsmkPUCpMjn4uJitOQndc7xK88Ie0Ip+zLlgRHzb5UM83vGfEk9LySXqngMco8Bg3u4DuCbXMxm348Nw4uZzWaDIdjY2BjCRF6wbOJhT8ArKNy/1HOxWLSdnZ0hn6ybjea6aW1gY1Dm79X3HPh0gmekTJV5xopHXI1y7+7uX1DMhuucWcw4Gx3iFfPucCt5tZCQdlnBKI9gPrNItnRslmbPY2vjDdjIhpidZxDtFhMoR/4si8n4H4Mh2VrOgKasEmSseHd3dw8W5+Z9gG9r46NxSGYrzhvwtMGCYVBPlNwMyczAeRrckJ2fvbu7Gx1g4D5P/QUMWf9ova+AycmejO+r/vyMWZrLsXeQbLgiD26/N/Lbzfazvt7adx19+/ZtW62+b3f0dkOWDh0cHAxGu2Jr1mOPdRgxMt/b2yvja9z33PR3eeeBXS+7G97vlUFkACeZHgP57u5uADRY4nL5fXby9PS0ffnyZbRYkPy8pQpXKlfsuyy7ZP4z03HZbAFjLVSuAXPwHUCDWWbglz9vE2rt/rQDBrflw6JaZqw8wUEbk63Z9cypdgYw9bf7nO4Rymb3nuTwQq7JQydYx0eZVl5bc7tzPXfX9TJIZ5lOllWCebbVgI8srbsVo0jAM7jxu2eMfW/Wk+cYN5ZZNZGDDF3/apuXZ6Z5xon7Xr9+3X766ad2eHg4LCuCge/u7o7WlqYMXBaej/vP7jIrHwy4WRfas056NrBZ8Bm3Mktzgw0wbiSD0kDnJSQMTN68DlDwVno620HVqa0XOWAAldlsNlpOABvzUUpe9kEsy+uMklIvFovRS188mwq9p1zqdXV11a6urobBRHkoEnUyk/PbrXIFN/2RLmi6Y9TbfZKuqQe3f7fcDQJ2gVFoD4YEOJIHaCp7xp/QD+4HnHNQJ0NNBsZvHpwscajii6nfFbDlmKlcK4c9qJPdOm8XTANdyacC6SkDYVk4n42NjQHEbCzSJd7cvD+o0797jWG6oIDkwcHBA13tgdg64Lb2zgO7f3Zt3GHeCmIwS2bmAWSF5RlOUmCTMidxMEWcyla5JlWy+8PyDoMYG+R9widl+Cz/jDO6XXa3fA4bU+S5VASgAVABNg5rvLi4GBb1ImNm91r7bvl4m5etca7zcrywsrRT7lbFTtx3Caw8g44kY3Z81MbQ9XQ+/qS8HOSWJ4YDfYIFE0qwm5rtwp33MTw5sHImMpmY6+axYmA0SKcsze6degM8PQH3mY3IVDJjznal0fZnusiML5dvI7RarYYTPSwDG52K8T41rQVssA83GCZmJUuf366pLTfB7+qoaaw+AczZbDZswPcMaVojC76aiQGwcCdhgCi9FZtV1NTF8YQKPP1/uj1eljGfz4cOdWdTBx9ACYM8Pz9vq9Wq/fjjj21jY2N4UxRuBZbdW1rsgpqxpZtmkEJWgFRvu1fVX4CaBzUyya14vicZVGv3hw947aGvV/X3b14nSJyRXQvptjofD1AMCW2qGGAaBLNenzbia/S7wyYApwdvrsur2p559tw3g1Ne6zFP7kuQ83V7O+5PjGm67a7P3t7eYKidt70912udmNvarmhO/1MoDaLyyRQ8S8KzgFoyLVNZ7p/NZiNhORDqxG8MDB/1DYDZnWztfqlHdUKtgZG6eZlEa2O33EzNablcDpuu7+7u2vHx8TC5QL4GHx9PzeJcDl2cz+ftw4cP7fr6erQNJoP8dkENxJWCOB7jGGEqV87WJVur2DnJs7jUL9sOGBBXTb3yc9Sbe+xqJhPjze2AcMqjYkaz2X38kzZVYIx8HevkupcOTcnPz3AtZWQvwa6f8/KY8Ph0vxuofG/qRTUOLHPqlTiAIan0zuPca9/ct/m/wzFPdUfXXsfGYEQQdkvzhSu4Eumm5JoVn09mymyQIz8L0OXAxjgJlIMRmaqmLsTtHOtKhbNAATI6K5NZkZXciufBcHNzMxzBzJpAWBeD/tu3bwOTdLB3uVy2jx8/DiDPG6NYuV3V3wyCfjPwVIzAebjvzSA8kGAeAKEZCSvUMyTh+tE/foeB65ztSaacLk7VT7jz1iEDlcHFoO7gvV1l9D6TGZiNf7pqtM8s5Snyp14JbL4/2RH3Vf1euZoGKufvMs3yfC994xN1KnAjnOTJlPQobBzybWuPpbUZmxmAGZktJSDmz9w/6M714E1Q47vjQkw/E2T3OxLMlgAyK4XPy0/q7w5Iuu3gNJ3gWBBtoZO9pKOKDXhtGnWl7Z4Vnc/vD1f0GkJAw+8iADxgDjnbmzNsaUErxoLb7/oZ3My87YbO5/NR36fbjbvr/x2vzT5IUEG2PTbnsEKy4YwLe7CY6c1m91uw8uQJXP/UEzMrA5m9DmLGBr7KaCSzslzMMB16qORS9Wvqpr8ne6XsZJsVSHK/3x1RLTd69epV29vbG41XyrBcTT56Y6lKax80eXFxMVIklLpa4Z7xGTfCQky67uQOXiwW7dOnT8MBgcn0cv9qWsNUCFu2HgVO9zLXgQHmtMUpKTrfe0Fc329DYFZpd5n2+rcEXrfRCujfDIBmYH6rFbK0DOkzjrkBIJK92eh5kKR74sGYLp/ZgPvI4ODEoGIjPevXDGIu3+3zTOjFxUW7urpqh4eHo3Z5oLvOZuiWMW3/9u37+yJms/sX3CQ4Vu4q+XkSxvdatv7f9bAO9P73bzZ6TtW4sUxsWNJ4Mm5YmJvhH7P6fLaqSy+tzdh88KCteM9K2EXw4DFjy8BndV9r3120i4uLtlgshs26WNKqs8k/lYeUvxnAzCRhZv6NelVucwV0ngG0u5Lg4/vTjc3Yl/NKVkL7bA1RPLNPAzgxML9DIV0W18sLgd0mvlumJANDz2WivewesYzzXve3PQfrD/FWs8B0t7xsx5NhxP1spA2Ezqtiv+nqEQKxLJIBJ0PJWJnJAPkm6GfZPYCqZN8z9lWyV9VaGyZpcq8vdef3/f399vbt22HyoGKb1s9109qne/zwww8jNwtKn53N9VRgAwWL/nZ2dkbBYj9vobGuJs+fqkCQ/yuf3HVJq8KAYkA6aNljE621Edh4wPF/BWxVyudpCywxJ288qMxAsr2O/fnTMUoPIrutlMOgp04OHfAbbUvGa7n1dISEm8xEguNzGQCnXu5zf6csgA1A9rILADoNImUBsAa11CV/dx0q0CJemgwa+aXnY3AwGNrL4Xn6MdliGrgkIxWo9dqZhsn9PJvNhkk6txFd4z6WJvXqkJixblp78sAsI2f/qkFGowneEh+7vLxst7e3bW9vr/3lL395wPTyO+Xgf7tzKhbo+vBbZYEAWdriOJ2ZjRW9cheqPZVWZithNUOWcrSVa62NjInrULEfMzADhK97CxPgnXE3yy3jpL3ZOesGnwbFrG8G4DM25UFNSoNp2TH40+qz1IPnDYa59i/7drn8Hgx3WCWT62nAcpv8e5WPdaZ3vwP8yVbz/wTWnrwyJYj5N8eVE9C8QwPds0tM2tj4vrgeYpTlVvXsGe1eetab4F2YrxnEeHEKAX5iYu4AH4zohqViOG+Da1JwBk0Ks7I+Zo5mLXnqh+uUsTF3Zp4Cwf0ZQ6isY4KU73Obqv2KSeEBMq+JS5CmHwlke6Bnez3IzDpbu1/eQl2ZDHG7zCwy7uO9rimjrIvXBVJ2JssWptfa/cJuL/h1+5icQc5uq/WZc8jcJ+SVgJSMySkBMAHchrHqC+fhepNPxtuS8TB+uNey9HMOxfjEkMViMeg2snM/+a1mgFH2tU/addl+JmW2Tlob2DwVDysAxHzKa77SzNsn+J+YAx3meMpUSjfNtHoqUUY1EeDYmak7z/k79fWGei/i9D2p6NmRdod79bfVTUBL8HdbqvidmbbXBDpmxgC2y2v5Ol+ea+3hm5Zae8jA7HqYKVaDNtvo9zj4PvrBM6qWHeyUlHHCZOAkDAMG4Nu3b6PtVamngOkUe6s8ktQVt811Tbc/86n0J40+3w1u3GcDT/sxehlPN4Bm35LvxsbGaJKNP85vyzFhQJtyTZ+S1ga2L1++PDii2taQv93d3ZHiZMcZHCom5DKTGveU0CmtYQ786g/Bu8PSHaB9Xs6Sb3HyswaSbIvjXslWrECVXHxvKk6PspupeYIBYGD5CDOtyRo4yZiFw8hjtVoN1xyHy36y22KWUrFVyrQbZ8Pq3y0XswL/xp+BIpejULZZaD5flem2Jlvt3U/7emMjr/s+yzUHfZaThtCGxadEW196LnfPW3A9yNPAbJY9m82GdyNkAmxTf+1xPDWtBWzX19ftv//7v0exJkAsLZ8F0+vUx35PK5BxA1LSag8SkvOplmxYoB70rbURc/F6OM+ipSLkIPX6pUo5kZlBLpkL97r9yT57i4XddisOwAZLY7CnsXE8hb2vyMETSLgnKK6tP20kmaXBHm3sclY25ZM6YKZgkFgu7w+VRD+qAxmyvVyj766vr9v+/v4D4E6W5ZTgZrlXRroHiAm2zp/r/rQxt3flvsI44U4ayH1PxZZcH49TL4o3iLq+eaJHlSoXdZ209nlsvDXKsQp3QMbduIf/s5E03vnlNTOt1sYKVTWaOmSswBvA7bL5mcoSu405GZCDyHUw4Np9am28/MPtcKwpqXkvVjIFXGkczExbu98Tyw4BWKgVvrX7F9j4RBEDhgHJS1mIueCmsUMi3cAq9gUD8Iwz+VYspWK41NHb+Sx3y9sydb0wmGzNo+7JpCgzGWmOkR4AVqDWY+nJYhPAElTIx8bCv6dMK2BLoPV95IHRS2/CQOrwk/NKtpaf66S1l3twDlMGKBGyFa9iZH4OQZmJ+Tk3HEs7FYOrgpG2VD4fLjsdl4pkt8Wg5vVMPYWtlImZOtfN99iFSnB3O1yGr5l9Jltzm9yXs9lsWAvoeGGyUBaVEnKw4TDL8tq3NBi5mDi3x3jyIlkmfZuAP8UwnJbL5chF8k6Nqg4pL2S1WCza5eVla60NzM2AVOmgU+qKPzNuXN1fAYF12/1PvWxoWnu4cLuSl+vg8qr7krX5WKqKrfmY8SomWNWnN96n0rNO0M1Abw7SDL639nARYWWRennmPTmwLUR3nGcEbclIU0I0cHkQGOwy6GyG6IWyVi4PTsvKTGI+n4/WWSW7BDh4LhW1l8wKV6vxZnfq58FKWzgJhb3Azs+vXHRgfWNjY3Tkj2NFZmg9tzL7B+AEYDg2PAEwBz/lbW5uDrHh7e3t0XtCkYefsS7C9jnY1GWxwJQy6YtqMsQ6VjG2vN9s3vXIlQncb5cy9cHGInViahwYuFxeJvoFNxQ5ui+Rl9/I5v7mmZ4er8Pc1t55YCW1dXDjuc/fYSxmEukGtFaDGiDlBto6JY22EJKhZce6kzIuCCCZpXngVh3cY30ZN/OzDCzf598MamZptK83gCyDVDZPGHgCxFvklsvv67cMatmvvHCGI2gADb9fNpkXuuOYqOvmevuEF7uFHjSevTPoW9a86dynD8NWU8buK8AEeVKHy8vL0Yx49n2PzVvnaEM125n6neEV+sopiUEu8el5Uulu8pvHisdltpPf2P+cy6WQ4XK5HB3zn/J2n2XZFShPpbUZ2xTttnVJYXhGrPeXCxkzj56loE7JJhwsrYSCQNMSOI5mFy3dhqxP0v4sMyclKDvZBXlhCKoZvux8t9kD3oMD2QLSXoNHO1G81u4nPHDjMi7F/YDZ/v7+YJHNHMxwEvjdj+lKuf7U24uIvfTIQIde2pASM2ytDevScCfJK1mW8za7YKByGCjsL5+twCwTQJ3GGr22ce0ZxwSmDHmk/nscJgPrMbjsR35DJngWPnfQz1Em7zfAI0k2X5VhQH0qa3s2Y8sKUHCCmwWQ9NZA4UbaqtviYmntJnrQ2kKl62l6n4LnuuvhwZsr7lOx8j5bVf+fbmda4exMuzUktyk7mliLlYxn3H+e+WR2yrOQKB7KZ2C1q+p1b97UDMOm7W5DtrFiDwYTZGDmDGD1kkEN0N7a2hrF/cwM05C6jtZF2lQtKN/b2ysD4vm/AczXaI+Njdl+DnB/ZhsqspEhiynQrUDNeSWg0Sa7oWnA+J9+8JIhM2FjyzoMLdPajK1ywdIiJ4BZmEmtSQwyHzzHAHNMptchLjdjLj0FzjonqCV7pJxkrVXczCktEs8ySD3QKotk5bYLZytqum9Qc37pSuNy5no8M9nVajX6zZMsDHLWvlEvs3PLugqsp5FI9yvLcljAz1eD2flwykdr90z27u5udL5gBR6t3b/7lfoRu53NZqMDQQE39wseg2cKkY13pcAmPcYMID3gsnytyxXA+P8p9pNgnMm6yh9xTx8HRbu5n3MQ/Tv6ZGa6sbHxwLtBXlVMtkprM7YKVFyYG20GxbVMCMFH36Csd3d3A8MA4CzsnhWzUFLpKyvntmRsBpAw+8r2Ztwp3VSXYxeN36ZALe/vydmzvz3328yMGUofTYRyGbz9jIECAwCozWaz0VYagxn17bEkG4fs23ShWZpi2fC89c2TL8vl+EReQJ+JBPSuYoHJPG5vb0eDjN0QyJ89kBkCsPGw8UQWVR8nU+p5QX6++ksdSBn38u3lkQYVmbr/zZodA8XAOJzgEA6650nKXEf6lPSst1RVNNsVfUqgDwDzZ1VprgNstkTZIV7r1rO+lPUUV8ZMzoO9iq/l80mrXb7v64E+5WSdKuBk64+XszhvxwtZrwawmaWxf4+3gPEyHfqUAbxc3q9JM1NzvxiQk31kO3OQus2u/2q1GuqO0auME3W0/HJHhCeVXHaCb3oa9Ck7bXju+vp6OCeQdVpeE1gBWnoZ2Q7LIdvZG4Nmd5lXFfutxrDLzTqmbnEfLyHyxAE6ybhkmYfzIH9vy7u5uWnb29vD+yZclx6LzPSsE3T9aYHyf2v1CRWtPQys0qjsMISMu2YATKtg9yAFUAnC+dh9TLeIugG6fqGxZcBxQmndXX7Plci2kCoAY3B7oNmQAO7MIJs1cX4dDMUsLQHFcY/Xr1+3xeL7OxiIoXj71cbGxsjNQrZ894Bxv9i9dn/wl/EqA2YCRO78yAA87ed/ln609n3w5YJg2J7X5SVgu+7U0SfjvnnzZhTSqAx9Gq/UiewXX6sYbtarx9Z613rMj/wrwmKjxjhMlo6OYkwrvaYMr8mjXovF/RlvBs6p9OzTPdKKeIFlz+pM/YZQMzhu5cwYVWttxFRgF6kYVWynYocImIHjPwvf+2MJnLu8BGkP1my//zK469mwdOXyWdpu1mq3E1CrTjvm02vDPIBQNq7PZrNhKYfl7jIZzNWATjn02Ij72HLFumddkxUiw52dnaENtIc6p/vqs9dyOUW6SKnnGIDb29vh5BqeMxs0o88x0PtegVIP9NKI9ryTBLhk1pVuVnU0Obm9vR3a6/DIxsb3rZf2zLJNACBAdn5+3ra3t4e+qcZxLz17VpQKTSkwAqqAzpVMl6NaFlGhvM9O64EHQrQSZ34GJP4MZgbV5XI5vEB5c3NzmAk08JFnMtWe1U55pVsPHactDD6UILc7wbaISbG+jJiaAT/B2EBF/gSFuZ8B74MgATxcU1t4y9GGivZ7Qgqjwu+U7/Y74EyqgJ76Ewv0vZUBJE/Xj/rTbvez+8bG5e7urp2eng6ysFfge60vmSrAq8A0dSnzZkz0xmeCW8qGe6owkO9xqOj8/Hx0Xtvd3V07ODhorbUhRulwkCe+eovhAbip9js9a+dBUlYzNgsIpeh1ZLoWKUAn3CJfQ2m9JoYyGJgOhhMAp2wsfoJmLhxOd40V3ixxqNruNnqwVR2TLI+6z2azEdMgX4NOnidHfriKXo7hLUQ2JhW42sXtLZIlFsfZ/dVhAP6/ilEaXBP4kJ0HDX1OnyYQW6a0pbX7Ba0sCHab0RMv8HUfGmizTTnYcceurq6GQZwyTn2wXmQ4omJlFbNNgDS4k5fz7rE7j9UpEPF4pg+2t7fb4eFhu7m5aWdnZ8PrJpHhx48fh1ibQyJ+MXXqZzLIp6Y/tUCXjkyQI9nqmRFkfgZLGubZsNVqNewxs6Wwm1TV1UKxMmZZ/EaaWig8m30/ncCDMJPB0opSlZXg7tk0lMZuC64w7MzWrrX7Aby3tzewSe+NrAaFB7tBJpXcrhkMmNX7djFofy4NqRixy8vdHVmWJwXsKbiv7Ia6b+g7l8dgx1VKRkWbc5mJ2Z0Hto05AGeZWw/zfv/W2kMgMuvNQV/pWD6XrqWfRZbJ1Cy7qozUD3SA0Ay7PTC6Jycng67Y6FYx0sx/yius0rNc0Z5f7t/86cpWFoLf7Z6kdTEdJUhLYBu62+uUrJtjdtnBHiQV9c/fEsyTUSKzqZSDcDabjWKHyaBwO/2HS7i7uzu4n8TBbDhcRi950HuKHgDk2nw+H72Dwi6lmYL/WrtnxF7sazaYHgF9PJvdx2AdsnAfGDjT+hucuQ+2m2vjvAfWsU4zf+qSW65ghszu2fjTfmRUhXUMrB7kOfay3RXQVWBQMbMeeHF/xZh4zobHnhXXr66u2mq1GkI2y+X9W8OWy+XopdRpVLO9T2Vtzwa2nrvl3zwgK9qMq8X6l2QLjouhMAjl7u6ubW1ttbdv37bVatVOTk5GbMf1sPL4dz7z9+q+nhv5mKyqZ1JOtM8D/tWrV0Ow3sBtJfJAYvEjEwSO+5llJBN2m5PNMQC3trZGA5g6EcOrgJNyDQbVEp+KvaWuuF+vr68fMDEbkJw4oL7VIEE/ASHPFs/n98c2JVuznMwgAd/lcjkczW55JzMx2Kc+uK4Vc+sRBO5PYmCG2Fvn6dion7NMHdu10Z3NZoOHgDvqWDReV2v3ByuwjhD2hs5mLDNl85T0rHVs2bmVpfBASvfTFYaB3NzcjM7/4jpuJq8tY9qeTcz7+/vDnkYsQ1VfUsbDei5qBZBuY7JSs5gMFpMqi1flQ/4MLk6WgNZvbW0NLiqW3GvUegMGi2r3xAmgyf7EdfDRSFwn/kdd7XqZOQFsBjvaWIGi5UVf3N3dtYuLi4FJ4UbmUUoOfVTuJwF/Biaz3Aaz1Wo1HEPuHTG02eDlNXKwaQaxgS/ba8NXAW/lTicTS6JRAVMai8qoV0SFvvafWXLuLXZskVja+fl529nZGR0b5TJ4sTJrEx1rT2OyDrl4FmOzYDOlEPktBWg2wcD1OxfN3Djc782bN21nZ6cdHByM3hV5dHQ0WpLgZGEk7a862Pf0AK4Sru/vMbWU4ZRraHeHgWP3tLXxLCGuXAKL2XL2Q08eDAgYGgBitgiYfP36deiTdC/NxhLQss0emK3VJ3twegQTRo4LMfAM9K9evRrFD3OtnHcwMMB83X9en4WucUAA5QCyhEo8DjxAK8Y8ZZCtwwluydQynwTMSjfdTk9E2UDYeHq5DXX3y17oAz63t7dHupu6BhD6cAbceevUP8wVrVwFfjdd5R4rsoWZLIl4Cet/PChms1m7urpqd3d3w2kMu7u77e7urp2dnbXT09P27t279sMPP7Tb29v26dOnkmkh9AROpxx0dhPpsEopDMJZZo9SpyXuAT+ytRUzsFWMJMuvZsOcsk12MVBIjEjKwRvv/QKYipVkMrPwrKYHqGe9Webi/Yd2q+gD76hgIGEo0M2Mv7Ft7/b2dogZAo6wOuoIK8OFNbNmzd0PP/ww2m3gmJ/LTYDrJdqaupNMlXySeSfjMZjlQZWUwXY5TxqZ2Dh/x0lZrA5AIRt7HmaF5AcpYccL/QTA9QxAlZ61V7Q3OKwsVszq+cwDhb25uRkFa4nhsNmYOBICPj09bZeXl21vb68dHh4OU8zJKA0UOdgSIJJJJZtIhbHQM5+nMDe7sMkebTRIWY+8v3IlSbaY6VajaFZ6+pUBntafwW9mVjFAyjYLo/1eae728EmdqQdl2AW1gaTNWW9+4xn0DIYFkM9mswHciOuxlo/vZk0bGxvDlqLVatXevXvXjo+Phy1B3gNtHbGRSHml7GzweobKRtRjjGfsemfcDhfd9UWulTF3X9mIIW9mmn0f9fPsqycSyJvF79TFev7UmdFnxdgyWNra/YuMK+AyKlNBp7Qel5eXQ/yM5QpnZ2eDW+aZJxZDbm5utp2dnbazs9MuLy9LAdgaOXDNd8cCk531rGDlWqVVzXsMGnZdU/lRSAft/Qz16KWMh1GXSu6Vu2Lldv/5PvrHp1MkUEwXytMAACAASURBVDr4Tzn0VzJd18fX7G639vCtZLQrDWkyyLzXwAxr8OSMWevd3d1gOA1IzvP4+Lj927/9Wzs+Ph6C6cnUKubvPsr/GdAYQP4qgCF5Ft0hBJJf3GN9dezTifydT3WfZ639LMw724487L4nIBs//mHANjVgXdn8zuDsob/ZhV9nh5Ac4KXBbBU6Oztr5+fn7eDgoO3t7bUvX76MlIByUWTP0OU0v+k4qWI4GSermGwOstbG4Jqd7Pq67J77XIFlKoYpf7owvuZP5OI6MfDTTWQjfAbk3bacGeR3FrBWsSHrhJkgFpxBBntyu8mT8jY3Nx8YB/JPN/H09HR4taQZIwDnF3+jR5ubm21/f7/99NNP7Zdffmnv3r17sCDaepT9m+ws2VwFjJXX4/invRbAG+/HIFbVJetpI2pm7PvMQikTnaoIQY57GLhPprm5uRkZ+nXSs3Ye8J2K+bpdGRIUHn89G9vad+VnajjdT159xqynaS+vjTs/Px+o6+7u7gN2yACxBfd3M4TscOqXDK2SD6nKo7rfIJR5V65KNQiy7Cp5QDt/ys5BgnwpP2fHkvXiUjAIza4co4EFPTXNZrMhvpfvO2UQW46WD7NtuePELMtMmQkQDCUbr808W7v3TtjRcXR01P7lX/6l/fDDD21vb29wY6vQQIKB+6HXF8m23Q+AgXWHuJZ1CjJQre3z/yTrRo5pX+NZrnnVAv1cGUwTiHyHrSersq+emtYGNh8ESaPyuxkPQIFlz5kp3E+Oaz47OxuWM5ydnQ3A1VobgSoK+fr167a3tzdsmiUGxywXycyDQVtZUBQrOz7voW3Z2VxPxldZWLelcmtcfmXpq7zye28iJfO18jsZ8HPdmdsK2HFfFeBeN3nGM9czekJntbo/lSPbBxuzx5DydjyOT4zmzc3N6GRYQMPveTg4OGiHh4ejNX09byZly/Uq9ME9nijhzx4D8qhcS7txqVcJmGbBTIhYbyvws/7TR7nDJdtVMUV7Kx631Lsaq1NpbVfUCkWlXMFqAHL4nhvLH42A6l9cXLSzs7PhdW+//PJLOz4+bm/evGl7e3sjpOc7Vujs7GwIjmYnGDS8oNT1tUVNIEqGmamKo6VMkrb36ufv64JCllWBsOuc7qUTz5hhu95eV+h7Wxtvq+qlqo96LiV5+ZoZCZ4Ayw4Wi8VghKsJjWyHyyD+xNo9g+arV6/a3t7eEPYAAD2BQkowcRtTRzAK7u/c2cA1v4CnMsTphfjZnOFnPNAf7msmZ6yTBjnH06oQTnohWacce9zv01wcz3yKF0R69nlsU4hcuTxToJDCwIKyx3GxWLSDg4O2Wq2GqXYYHg0HFK+vrwfrYTeDDkQZevEF2uQO6NWdZ9yhZgFO6fZUZVjhyTsVM/O0jF2v6rlUvHRpeu1zXtTPa+ZSB8zeevGRZPpV7CVZLOXa3aVfmYUzEJDM8Bzvy0B5LlPY2dkZlpggDxZBMzPveK3bgQwq3cj2JyPLurN2LGcIMzk8wD2VDlv3fB35MDHnnRS5pjPHSpblGHKOkxwzbhNLhrjGGsEp/KjSszbBV+6Nraln4ioq60rSGYAYEwecjd5aG+IdLOu4u7sbJgiOjo7aq1ev2tXVVfv69evgnjqw679q0FesolK8/N0ySIWrOtMzsD1gq+SDglSutdsxlbI9xKd6LnLljjs+48WUZkQZw+nlm+UmO6aebvft7e3Qt/Sjj0lKF4YyAAWYvEHNwGnwhLG5/3yfXXGzErO0yo1yvQx+1Mu7Nwz2Gc90In+ey9hsglclo6yLj78insm1XpvcjwY8M9KevtEOy5O2e/vWU9Oz1rH1lLcKfPJMfrqhBh4EyQZ3ptjPzs7a+/fv23w+H/ahbW9vt59++qnt7u6OlnjMZrORe+BJAitlzpDxbCqA76tct8oiWybkUcXe/D3r8ljivgxSU0/u8aD396eWl/d56YUtcrKDqfwT1Bw38vO0Y7H4fkwSOkL/MtDYjoM83H6ACheHvNPtz3a4DdnOlHmPmWJEMgCPjjLRwPMJaunNJLnwWPTC5dbaaHGtxwZue7JLX3M/eDzwPZmY65vglSDosWUddjsZt16cmx7HVHr2co9Eem9ErnzopOopJCcLzKcs+MBDL95jYzTHGqEwXkP0WJwlgaG6z3VPhfNzTqb8Lj/LSorv50l2B6yM6coxmJIFOm/fX8m/J5dsr5Nny3p9n0bDzCeBMMFvuVy2q6urgdET3IbpEBNz2ynr9evXQ5jCYYGnpASzrGPGE21EnLyZ3ovQeSbjUa6j9dGumd1uAwxGp/din9S//N/MsLq3Aq8q7/wtgbkCqpwgoW2pV1NpbWBzg9IKpBV0g/LZHrOj02FsdiW47nU61cC10qTrkOUlaLmT3Q4Hsbnf7U/rmla3AtWnMLPMx+DR2sOFqhWjnAI3ysA69urgsisw9G92x9IQUDcbRLuBUzJhcsB7DmEkbNuB0eegY1Y+jxm3bKpAt+veA1yDKHnbA8nZ5Eo3vMmetubi4mRJKf8EC8fsyC9ZpgmJ22Nd5/meK/iYLmeelZ6lt2EmBxD+QxmbK+jA/BTT6FmHHHDEbFAMn03vrTT85i01CIH8cEFypirB5zFBJcv0cz3AylQBX8rCdbfiM+By0FfWNF2dygjl9wTLKUvqOKrLqwLkHuQ9pvvYRFS2gz8PWE8gsF6NfZ8pw3fv3g0shiVGPjLL9cpYMmX1+tdHHtnI957x3lWMcLLznAyw0cpFuz5FGnnYTU3j2DNOlnkmx3ufEu8yOE6Ns2RzZoyu8z8M2Gaz2WjrjH/PilqAed8ULU1f2zMzCSBcd6c5f5ibO7Hq0CnXqhqwVUyrksOUjKrfK2ZlYKqUyfXifw8SK7Vja1lWAmwqEW699/VVjDXbmRMefi7BvGdoqJ/ZHqwNIMl9ha3dn8OPDHZ3d9vu7u5gFDkuizwXi/u3IVmvmHFlxhDgwItorb91a4ppJYuz/rt/sj7kjf7Der2OrZJfRTAcv0rCYebtlAA4xeSpq++jzIz1ci1j+JURfyw9a1Y0KXoOQt/fczlJZiRcM5ClYJN9AYwVgPZifXkt6z8FWJQ3BVTZ7iwzGVKW12OJyCnZRHVvlkMZuQne4JTK5zKYyEll9/25+Nr3Zpum4pK932kD536x4Bt24pgbTChnKZ0voOj6nJ2dtU+fPo1O0iDGa+DwItSs8xTQuy8eczXZDO72U5ckABlvTZlV/6fOuP/z98yjMsLOv9IvPquZ3aky0b1s21R61qwoBbuyPQbwGHW0oqdbySB2GRXoGCBs+QyMGXzsdbB/T1CtWGPVlkpGzsPf7V5Q33RL0sVMYKoscZWvZZX3ZxnZJ1MxlhwQlnG2t+ea9cC5uo89hN7NArCZBZjNABa4bLQv93IeHh622ez7UVmttdKt7A3kCsjMNjjEwWEV66vH0M3NzbCGDjbaWnsQp7N76Hqk/riMHij1GHX2bRrOqo9SJ5OJVvIiP58Cw/24+k+d8HkWsFXf87cpYOtR1immk66Ir+eAtbLwfyWQihH2QI56JGOZYneWRVL2fKZSxh6DTNlkmmKBlUUGBLL+bnO2p+q/HBAGzZ4Sc39PrwzS3Mu2n+vr63ZzczNaJOtPv+qN9W7eW+oYrFnU8fHxsLzErIm2WA8qMONef3oTfg7QNEL8zysDcX2n0hS45e8VIFXjteeK9ohN9jP/M9GXRtWG3ffM5/Nhoa7r1jOKVXrWsUU0ppem6GKPJvesIJ8ZQ6qsSlqvvMdl9ZjnY0HRivG4zm6Tgcyd7rIr16ECuqy/v1sG6VomuLoc39dzi6tyKwbp/zOW4jKT9VJvL2uYStTx27dv7eLiYmBtJBiRl3zgTnpDPKd0uP45Cw8AXV5ejjaa01aHS7J/LTPa53dR8HuybgY2rzR0PtYjy76KdaY+ul7JxAHdCjwzftabXEodsP4ZrCmPpVuz2ezBpv1kijmOnpKeDWxOFnhP4St3iZRxqxxk7vTs3F4eTglWDKKcHKgApmpr9Vu6whX9znwrYH2svLSIlpGBrQK1CpAzT9rxGMD13BNSxoB6A9MunuvVA33qh4UHLA4ODkY7TdjIjbvpHRIMYPYkA3i4QLxghB0xq9Vq9I5M8nS7aQu/MSvLNZ9M4n5FHnnAQ8p0qj/SoFuOFTiSn+OmHntVeiw/91lr7QFw+pReGBmvAjDwpZ4+5rn00rOAraKt1UAx2E0xkKlBYoBLpuO6tHb/9qReeszt7LWrAhIDmfOvBjlKX9WtB2YVoOU9/m5LWrmbT0lpKSvjk3lWgFa1vxqwuRyn6le3DZeF78vlsn358mUADr8zwwdIwrxgemaUX758aavVqu3t7Q33MlPqevrEGLNOGGHKHYbnva0kbw9KkLR+TxnBXt9VfZPPVUbDz3gcZViB1AtPmPmuVvfvQoAxk5+BH3nkwubHPIip9OzJg0w9muxrvpd7Mjl+YaXJzqgA0y5V5jnFiqZ+TybkPNdJPbk95bns3EoxzVgzEM2gy1niCmgrcPPArNi186wGaF7vycTtMvulPrbq5OtlG57hnM3GR2zxjAcPQPbXv/61bW5utuPj4/b27ds2n8+H2BrvHM1lFOSFrL38BHc4QZsXwtAfeb0nk+z7ynD0xlRORPkZ2uQjoSp9Q6eqfJmgAfSRAa/Uo++qc+2SfafL2fPAnpKetdyDlCBTxac80JJq8nxaBQaPlw9YoSvLXjEnD44pAZl9Wfk9yKes5VNSbw0Zn7TbqXIxqpQMN6/x5wW2CVgpx2qgmcFUg8XKmLKstrZ5o3WlQ+nOmX0ZWJfL72/Lur29bXt7e621NtqDmfsf+Xv16lX74Ycf2ocPH9rvv/8+nJ77888/D5MPDEreRrW/v/9gu1F1Kgfs0EaV/jVLs6z5/pi+JvAjN8enMq+KoVV97fo5P/dlrgPMMEzOQnsNHnU1mPH/FItcl7k96zw2GlqxmJ6FTkudeZEQUjKHqqMRnq36U5XC3xlAXhLheF7FAv9MSrZZKVxlraw8lbtYleH/Mx+Xl/3ZW2vk52xs/DuxGwfnDUjIN0/WTebtxbAMNE8eMGg4KOHr16/DIZAcze3TbO3i8Lm9vd3+8z//s71+/br99ttv7ffff2/fvn1rb968GZ75+PFje//+fTs8PGz//u//Pryrwf3kAW2Apy29CazsLxv07Mc0HM7HAGTdqMo2YXC9ydPnFQLqvKQ89Q12ijwBwMvLy3ILV/WX4aaUSf7+lPTsFya3Vq9S71UsmQfXclFnZcWcrwe162AGknn5txRo5pP3+3pvlhUZ9AAwQdrtqeTS2hj0DYAVkBkoXYeey+J8DGx2/+32VSDo8rI9KDeD1IcReG0ZoJXg13P9aJcZNWVxRt/19fXArI6Ojtrx8fFINwzIZg0//fRTWywW7b/+67/a+/fv22q1am/evGmLxaKdn5+36+vr4SAGZmHtShoc8rtZW/ZfsjU+q+1rsNZeLNeb4xNozdwrpkafYzAANL/RKuOn1WylgYr+p8w8eqjS+wT0x4hKL/2pdWxPuVYBUo9JVP/7RIsKsJxSaRJkckD7WJQqr+xQ6pIDjnsypWL1gM8AUV3jM0Gll08qg+MXVd0NdgwOlkPM5/NhEaxlVrWPvJj1cgLArNycyMGaJYNPgngVvmB3APGt29vbdnp62k5PT9urV6/a0dFR+9d//df266+/DvUGQFtrw/s0FovFsCD29evX7erqanhHx8XFRbu4uGjHx8ft3bt3w+ypjbaZfm83gfsof0tgSFlmGX7exqcyhiYhnkRBv82umVixrpiRubxKdzwBwHjvucj+3iMg1tf/K4wtU8VIqmRh+5M0NWBJOVOSrNB5WfEqat/aGDirdrmjc+bJ1j+f6bU7r1txs5yc9p6ycFb8KTfV13JwWEZbW1ujVf38bheyZ2U9MMh3Pr8/asqLLytXKZXaAxwWYFePGVAmEDY3N9vl5WU7Pz9vm5ubw4zn5eXlqP6spVoul+2PP/4YjkRiIJ+enrZPnz61y8vLtru7OwL46qBNs7Nksm5rZVzcr4455Uuoe3Inf5Ljpz7jbbVaDW/hqg6RAJyzbtUMpXXJ1xOc8l2ylYH2/zlGub4OuP3dlntQ8damt1o8JSU7ybJSGHx3x+eERQ8MecbUv1f/pNYoeJbjmcO0nAYff3owTMW2MiXYVsqS7ar6wRaf/z1gXM8pZux8WOTKb1VAuurXZPf+HdAh3sNA5O1ksMXLy8th58CXL1/azs7O8ELj+Xw+vDmttdaur6/b//zP/7Rv374N7zKACeKCAmC0KWN/tDFBJ/uu0l1knEDuGFXllvF/Amtr9zFS3PPl8v5Fyegqi4Up09dc12r2PCcCUw8sA8c00YPKe0qAtFwe89YyrT0r+thvU+BlBc2ObW3MrJLRudHVAKiCs1lGT9n4n2em4mlWLg80H1+8Wq1GSu7yrAz+7LmimSqwtCvgtnr2yW1BzgmyU7S/x3rJm9iYB5hds0p+2a7834M5g+Yca2XQ3NzcHOJffF+tVu3z589tf39/1D4G/O3tbfv48WM7OTkZAVVrbVj4u729PbxWL9+Sngtrsz3pbbgfbCANMAaqZEE2xOn2Oka2Wq2GY5loi9mYZVyB2ZQXlAaz6j9+c97WxYqZuR15T0VQptKzJw96BXiwtNbfa/hY3hUbc6ekL09KRXC+PeSv7s2lEFXHoUgMIq/knmpjMrN0T3qxOCfur6ysQc1tz/hMzwWs5OJlG57dJOVC1Cwz653fq7IB3Mpdy3amUXn16tXwP4dQrlb3RwCx9goWR/wM5sdrHMnLi38r2dEXWU+3rWJblmN1PWWJnAE1jloizkjduHd7e/sBcPbAKevgMEz2SwV4vm4gcj8xljwjbjkloHsZyFNBrbU1gW1KwTK543Mg9xr/mMWrhNpjf8kOexYFC9kTWgWC2XaWNaD4XrvjuiTLQjY9hjQVY/PvvX6pYmEVM7NbUbEy3DsGjA9G5M9uUA7IrG/WtbqejDDrxVoqvx7P9d/Y+L7xfblcDizLeQEOTF74FXTeK8qSEZaPZBu8SLc3HhK8EtQqkOS6jQhhj2/fvg0LiAEM+rs6TqliX77W+z3dTIMTf7kTxH+VzvKMQczluY96IPqU9CxXtGdp00VJQPP0eh4QmbMmCQYWdMXQegCbFqByefz5GJvMe+iA29vbkWvgGF8vn95Kb7cxlTKBurLs2WaezwFkefpeB3lzn6UHQlVutqcaOFZmt2nqu8tjBpTgf2v3B2E6IG4GR/39JidY3M7OzmhtmmcCATYmU7w8xTKsDFSCV+p67xk/iw4RE7T7PZvNhq1KKd+eF5O/9/5smP2/+7MComw717LPe+SkN/4qoz6Vnr3coxqQibJeqWwEz0CpP8k72Rz38HwGrCvKvK6QDKJT7Uym4/If6yCe78mvaoNBI+tviz+l0Hxarg7mOhg+JR8zNdfF/Wn5PYVhp6HsGSizFRjLbPZ9n2gyudbGMR3aCABeXFwMSz2urq4G99OymM3uX4fnkzksU8cRs0+t5wluacxJuMkGFpeR793M/kHmln2OId/vLWZc82RBr6+IeXoZDa4xz5il8n8a7BzHWWay/qcC3N8txobSVXGG7FBXPgPcrnxFQafAqjcgqjQ1eJ4qvIpmZ/sNBDmoe5MHziOte1U//4bypAWuYnEMdJYxmOX1XED/TQFSD+BdP/9fPWM9oP64+bCVDJ4z2JbL5bCViu+ECra3t9vOzs6wTg3QYZM2gJbvMUhXm8HqmfI0RrTRYOT6Ai60jXvsDvu53iQTbed7xscq/cv+dL9XE3XZz/Y83M8+pdllu93IJRl66lT1+ZT0LMbmznAl6OinAEM1SFLgVdnVbxnvqYTvct0O3/MYg+rVIRnlVHudesFmr9K3a9vLMwGA/slper81yYBiUOsxqSl3Yep3nrUb6TyT7frT+fi5fFFJzvJaPw0onrU9ODhob968GWYNATIAwu/8zFleEgYpZ31z76v7B2DOtV1sHM/Zzp5uVXpscKpiXMmeM+ad9/X0IQHLdarA0M/bOKdOVYasaudT0tqTBw7SzmbjmAVHH/v+ykVMQfj7FPOqmAnXplJ2RLLDKYvQ60Tf3+vI1h7OLGUbKzbL957LUQVU/dtqdR8c92sLMw+DwFQbKiXPflvnuCjnYddlirmSqnqarblurbXR8oxcTOsDDp1PsjADm11R18HM2kYCJsZseQb6/Zo+kr+7jVMG2zL19R478196CGbVKVfYcEUgcmdHysmGJ9tUgWIa254eVWltxua3PqEklQI6VZSyZ+Vbq4PGFSBNgdsU0lfsJMt6zFI8Zj0eY485eBLUejJr7f5cMlP61u4HceZZ1cVKwjKVKjDeY2ekZFspS+6pZv88qKqJoUqmzjeNhAGcwYW+0uaKnXpAMVniNW25sJiyAKDM8/b2djRr6zV2+fKYlOOUR5ATbj2dTyPHZ0UkMpnF2+C5bgYnGzSv7yOvKeM0NYbSC3sMYzI9+y1VOWiywlamFHRSzLRGLs/fUyAJbn6mAtzq+Sxnqnw/69T7rWJlpLT+LsOMGHklyHizeMZvpoCnch1auz8iyK9XrNyYKu+qXPK3AUxZOnkPaiVLf2ZCjgaglIdBrwJ9MzPvLsj8KkDyin4YDowQMPM6vzTa/s31zZnIZDVO2VcGyscMeCX7HvPzs76WbNtj088apBijHic9EOu1o5f+1F7Rigk8pfBU8hRaL4+0zlneOs9WYNoDr4ohPGZ1ciD59ykQsgvgMmEInrlMxtEzDj0ASgsMy8j1WraaPRnT1lTkdLUrptHafTzRLCEHQc+4tfbQzUwGbJkZxMgrwbFXV89atna/D9KvAKxYM7LJ/PI3+gFg67FmUuomeWa+2W+ZZ04AILM8R7DSH+tFNaYStF2HSl8dX6zWvD0lPRvYkiL2ULlK2bm9Qek0lV/F2p4SbJ9iao/VJ/MzWLbWRiwq83Wb6DwDWrKFijkkqE0lD+DHWA8nejjMwDO92SzaljLg9wrcUl8MwNUAcgyLujhfzx5mvItB2qtPL7lv+N+ums976x0M4LzSsFb3Gcgpe6qeFRNP5uSUMqoMVUUA/Hs11pCNP3uA5mRdsi5nuOAf6oo6Vejcq3wPyavnqo7if4NIr6EJWlXZ1f1Z1+oa1zOAXLkqGUSmzligrD+xHO+5XDdVA6AC0sfu4ZQMBhe/V8/0GKgHTgXIfO8ZkYqZ55YmM690d9Mtzb+83+1HTkyU0TcAWW4py/anjN0+9KcyNs6nx1RTNh74vtcMLMdCZWAqIMr+4XtvgsdjoBqfFTvz/ckKk6lNTVBlWhvYHgOK1tZHV+fbS5Xl4PfWWinsKUZWlZuDrAJXK1kGS6v62oJlMJm1SpXL+hTWO2Vte4yhArYEJOqKwlWB/4r19uI6CRQOzFveVTzWA4nlG1yrAIt8XGcPmEzJNAwQZs52UbO9yTqqdwi4fMpKVmrZAm494LcMetcN4tyf/dcjFZZLBYA505+ko6ebBl3H5QxuWRdff2p6FrC58B6lrtIUU6gGXdXIHuOwUDPeYxCoyuwxyd53/595W7mdL0FpFsOyZSefrcpy6s2mub1PTVXfMRj9gt8E3ArU/LsD6WZaPsjQ556lu5d1tPJb0bnmOvYYXIKKdyf4/mpmuddOUs7+0YZ8d4aTwcHXKxA0wGd8lfscC+PTzDSfcV/xWxq37MfU54pAVEDpZ3xtyh1OOa2r23/qhckASdWAiqJWSpsdNDWoew2rnqk6rGI5VblTQcrsOC9TMIhOWXo/36Pn1GPK5e49m+31TN2UEnkgs+8yGVKWXxkZXG4Hfrn27du34cDHw8PD0boog1XKwu6f+6yKJ/XYro1NdaaaZeD28Hu6h9luX0vGSqqYbzIex9YMTAbdDCtMsdIKoKv6JDsC4Kr8/FnJwSzRDO4xMPQzCaS9NlTpT71+z/TU16nIU8CNaz0hZt4Ve6vq0SuL3w1AVX1aq8HRFtSuCyzMq9t7A81t7Q2iihXABMm/Ny0+1Wa3IcvMzykFTqbq3zO2WLWPNhnMvI6uikH24nZuk2WTbL21NtoeVTGxNFxmL65LyjtlMTUAU+dSfil72lQdF8XzvXPWqjKngDavT6Wp8WY9qORqHTCQpwysp08Ftdb+5CZ4N4DCp+hib4C0VguzsoLVupjqe688MwMrYwKmf7OblFbE+wmT0VR1mqpf1aEVgFVuTLWkI9vxWP9wD3Vgr2XF1rLevsczke4v2NHu7u6DtvWYrVlv1q/SCUAx3dfewuVe26cGZAXa67CJrIPrZdCuAM9yae3hcpHKoFYs18bZ+bmtVd/3iMlj8vBme4ckXEbG23iukvdj6dkxtinLlHS0GnT5bA7gx6x9r0zusQuVywiqss3AstPJLxWkV6cKBKZmBrmvYoq9fEk9g9BjvxVbq+53Ox2H6s3mZd08MKy4eSClZZ5r2HyoYlpvDxTPXGY/9eJb2c4qcJ2pN3Cn+smDNfUvDVhuR3R9nI8Zf26/SmDueQeZUo96a+gScLOuPT3IkA11d3w0x53bU5U7lZ79zoOsDN+rxrHQ77GKVbM3LpO8qwFVUdYpgM28fQrrbDYbrT4nj0rxe8BSMY+U2VOAMTu6l6p2V/klu3mKsmB08gSQLL/K04ykmq2cz+cPFr221h6AGsknFHvrk8ugnTaW1SGRCcjJojIlmKVeVbJJJtJja7S1GtiVC7pOvbPdUzqVY6ua/TQApQ4jc7/GD93xX+IGLmmmdVja6Ll1UHA2m31orf1/zyrpJb2kl/SS/nz6P6vV6sfHbloL2F7SS3pJL+n/hfT0zVcv6SW9pJf0/0h6AbaX9JJe0j9degG2l/SSXtI/XXoBtpf0kl7SP116AbaX9JJe0j9degG2l/SSXtI/XXoBtpf0kl7SP116BOkq/wAAABpJREFUAbaX9JJe0j9degG2l/SSXtI/Xfr/Ac9JPkxwhalEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "breed_detector(\"hund.jpg\")\n",
    "## This is better then I had expected, it detekt that there are a human in the image and it classifies the dog correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
